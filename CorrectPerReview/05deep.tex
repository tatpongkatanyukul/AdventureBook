\chapter{การเรียนรู้เชิงลึก}
\label{chapter: Deep Learning}
\index{english}{deep learning}
\index{thai}{การเรียนรู้เชิงลึก}

\begin{Parallel}[c]{0.52\textwidth}{0.35\textwidth}
\selectlanguage{english}
\ParallelLText{
``What you get by achieving your goals is not as important as
what you become by achieving your goals.''
\begin{flushright}
---Johann Wolfgang von Goethe
\end{flushright}
}
\selectlanguage{thai}
\ParallelRText{
		``สิ่งที่คุณได้จากการบรรลุจุดมุ่งหมายไม่สำคัญเท่า
		สิ่งที่คุณเป็นจากการบรรลุจุดหมาย.''
\begin{flushright}
		---โยฮันน์ โวล์ฟกัง วอน เกอเธ่
\end{flushright}
}
\end{Parallel}
\index{english}{words of wisdom!Johann Wolfgang von Goethe}
\index{english}{quote!achieving the goal}

\vspace{1cm}


%"Reality cannot be realized through conceptual constructions,
%since concepts are contained inside reality, not vice versa."
%--- Nagarjuna
%"สัจธรรม มิอาจแจ้งผ่านการครุ่นคิด 
%เพราะความคิดเป็นส่วนอยู่ในความจริง หาใช่กลับกันไม่"
%---นาคารชุนะ

%\begin{verse}
%``What you get by achieving your goals is not as important as what you become by achieving your goals.'' \\
%---Johann Wolfgang von Goethe
%\end{verse}
%
%
%\begin{verse}
%``สิ่งที่คุณได้จากการบรรลุจุดมุ่งหมายไม่สำคัญเท่าสิ่งที่คุณเป็นจากการบรรลุจุดหมาย''\\
%---โยฮันน์ โวล์ฟกัง วอน เกอเธ่
%\end{verse}
%\index{words of wisdom}

โครงข่ายประสาทเทียมความลึกสองชั้นนั้น แม้จะสามารถทำงานหลาย ๆ อย่างได้ดี ดังที่แสดงในตัวอย่างบท~\ref{chapter: ANN}
และในทางทฤษฎีนั้น โครงข่ายประสาทเทียมความลึกสองชั้นนั้น จะสามารถฝึกให้ประมาณฟังก์ชันอะไรก็ได้\cite{Cybenko1989a, Hornik1991a}
แต่ในทางปฏิบัติแล้ว สำหรับงานที่ซับซ้อนมาก ๆ ทั้งจำนวนหน่วยซ่อนที่ต้องเพิ่มจำนวนมหาศาล
และการฝึกที่ใช้ทรัพยากรการคำนวณมาก รวมถึงข้อมูลพร้อมฉลากที่ต้องมีจำนวนมากพอ
ทำให้ การประยุกต์กับงาน\textit{การรู้จำรูปแบบ}ของโครงข่ายประสาทเทียมสองชั้น จำกัดอยู่มาก 
โดยเฉพาะกับงานที่ซับซ้อนมาก ๆ เช่น งานรู้จำภาพ เสียงพูด หรือภาษาธรรมชาติ.

จนกระทั่ง ความก้าวหน้าล่าสุด
%ที่สร้างความตื่นเต้นกับศาสตร์การเรียนรู้ของเครื่อง 
%วงการปัญญาประดิษฐ์และคอมพิวเตอร์โดยรวม ไปจนถึงผู้สนใจทั่วไป 
ก็คือ แนวทางของ\textbf{การเรียนรู้เชิงลึก} (Deep Learning)
ที่ได้ขยายความสามารถของการประยุกต์ใช้โครงข่ายประสาทเทียมไปแบบก้าวกระโดด
\textit{การเรียนรู้เชิงลึก} แม้จะมีโครงสร้างพื้นฐานเป็นโครงข่ายประสาทเทียม
แต่มีกลไกสำคัญหลายอย่างที่ช่วยขยายความสามารถ
ซึ่งหนึ่งในนั้นคือ การใช้โครงสร้างเชิงลึก หรือการใช้โครงข่ายประสาทเทียมที่มีจำนวนชั้นคำนวณมาก%
\footnote{%
การนับจำนวนชั้นของโครงข่าย ไม่ได้มีข้อตกลงสากล และอาจมีวิธีนับที่แตกต่างกันไป.
ตัวอย่าง เช่น
โครงข่ายสองชั้น $\bm{y} =h^{(2)}(\bm{W}^{(2)} \bm{z}^{(1)} + \bm{b}^{(2)})$
โดย $\bm{z}^{(1)} =h^{(1)}(\bm{W}^{(1)} \bm{x} + \bm{b}^{(1)})$
เมื่อ $\bm{y}$ คือเอาต์พุตที่ทำนาย
และ $\bm{x}$ คืออินพุตที่ถาม.
โครงข่ายลักษณะนี้ อาจนับเป็นสองชั้น ตามจำนวนชุดของค่าน้ำหนัก $\bm{W}^{(2)}$
และ $\bm{W}^{(1)}$
ซึ่ง ณ ที่นี้ ใช้แบบแผนนี้ในการนับ.
แต่ผู้อ่านอาจพบ บางแห่งนับเป็นสามชั้น โดยนับชุดค่าของหน่วยย่อย ได้แก่ $\bm{y}$, $\bm{z}^{(1)}$, และ $\bm{x}$ โดยมองเสมือนว่า อินพุตก็เป็นชุดค่าของหน่วยย่อย.
บางแห่ง อาจเลือกอ้างถึงโครงข่ายนี้
ว่าเป็นโครงข่ายจำนวนหนึ่งชั้นซ่อน ได้แก่ ชั้นซ่อน $\bm{z}^{(1)}$
เนื่องจากหากมองอินพุตกับเอาต์พุตเป็นชั้นของโครงข่าย อินพุตกับเอาต์พุตก็จะเป็นชั้นที่มีกันทุก ๆ โครงข่าย ฉะนั้น รายงานเฉพาะส่วนที่ต่าง  ซึ่งก็คือจำนวนชั้นซ่อนก็พอ.
วิธีการนับจำนวนชั้นของโครงข่าย
จะมีความสำคัญน้อยลง เมื่อพิจารณาโครงข่ายที่มีความซับซ้อนมากขึ้น
เช่น โครงข่าย\textit{อเล็กซ์เน็ต} (หัวข้อ~\ref{sec: AlexNet}) ที่มีการแยกเส้นทางการคำนวณ
และนำผลการคำนวณจากแต่ละเส้นทางกลับมารวมกันภายหลัง. 
}.
\index{english}{network depth}
\index{thai}{ความลึกของโครงข่าย}
%ความลึกของโครงข่ายประสาทเทียม
%หมายถึงจำนวนชั้นคำนวณของโครงข่าย.

%แต่ในที่นี้ เราจะใช้แบบแผนการนับตามจำนวนชุดของค่าน้ำหนัก. 
%ตัวอย่างเช่น โครงข่ายประสาทเทียมความลึกสิบชั้น
%หมายถึง โครงข่ายประสาทเทียมที่มีชั้นคำนวณสิบชั้น 
%
%เรียกว่ามี.

หากจะเริ่มต้นกล่าวถึง\textit{การเรียนรู้แบบลึก} แม้แนวคิดจะมีมานานมากพอ ๆ กับจุดกำเนิดของโครงข่ายประสาทเทียมเอง 
และ\textit{วิธีการแพร่กระจายย้อนกลับ} (ฺหัวข้อ~\ref{sec: ann training}) ก็มี\textit{ความทั่วไป}มากพอ ที่จะใช้ในกระบวนการฝึกของโครงข่ายประสาทเทียมกี่ชั้นก็ได้. %ไม่ได้จำกัดเฉพาะกับโครงข่าย $2$ ชั้น.
แต่การประยุกต์ใช้\textit{โครงข่ายประสาทเทียมแบบลึก}ในช่วงก่อนศตวรรษที่ยี่สิบเอ็ดนั้นจำกัดอยู่มาก.
อุปสรรคที่ที่สำคัญสำหรับการประยุกต์ใช้\textit{โครงข่ายประสาทเทียมแบบลึก} ก็คือ \textit{ปัญหาการเลือนหายของเกรเดียนต์}.

จนกระทั่งงานศึกษาที่สำคัญของ\textit{ฮินตันและซาลาคูทดินอฟ}\cite{HintonSalakhutdinov2006a} 
ที่พบวิธีการฝึก\textit{โครงข่ายประสาทเทียมแบบลึก}ได้อย่างมีประสิทธิภาพ.
หลังจากนั้น ก็มีการศึกษา\textit{การเรียนรู้เชิงลึก}อย่างกว้างขว้าง 
และการเรียนรู้เชิงลึกก็กลายเป็นศาสตร์และศิลป์ที่สำคัญสำหรับศาสตร์หลาย ๆ แขนง \cite{LeCunEtAl2015a, Jones2014a, Schmidhuber2015a,MNIST20150311,DahlSainathHinton2013a,DengEtAl2013a, 
ErhanEtAl2010a,CaetanoDosSantoEtAl2015,RamsundarEtAl2015,
ChenEtAl2014a,MaEtAl2015a,XiongEtAl2015a} เช่น \textit{คอมพิวเตอร์วิทัศน์} (Computer Vision), \textit{การรู้จำคำพูด} (Speech Recognition), \textit{การประมวลผลภาษาธรรมชาติ} (Natural Language Processing), \textit{การค้นหายา} (Drug Discovery), และ \textit{จีโนมิกส์} (Genomics).
ความสนใจในการเรียนรู้เชิงลึกและโครงข่ายประสาทเทียมมีสูงมาก จนทำให้เกิดการศึกษาและพัฒนาอุปกรณ์คำนวณ สำหรับโครงข่ายประสาทเทียมโดยเฉพาะ ได้แก่
%MIT Technology Review 2013 ได้ยกให้ การเรียนรู้แบบลึก (deep learning) เป็น 1 ใน 10 breakthroughs ของปี 2013.
%ในขณะที่ รายการของปี 2014 ได้ยกให้ Neuromorphic Processing Unit เป็น 1 ใน breakthorughs ของปี.
\textit{หน่วยประมวลผลเชิงประสาทแปลง} (Neuromorphic Processing Unit\cite{MerollaEtAl2014a} คำย่อ NPU).
% ซึ่งคือ ฮาร์ดแวร์ที่โครงสร้างออกแบบเฉพาะ สำหรับ การใช้งานโครงข่ายประสาทเทียมหลากหลายรูปแบบ (รวมถึง โครงสร้างแบบลึก).
%สถาปัตยกรรมนี้ ต่างจาก von Neumann machine ที่เป็นแนวคิดหลักของคอมพิวเตอร์ในปัจจุบัน อย่างสิ้นเชิง.
%ผู้อ่านที่สนใจ เรื่อง NPU สามารถศึกษาได้ จาก งานที่สำคัญของ เมอโรลาและคณะ จาก IBM \cite{MerollaEtAl2014a}.


%\section{ปัญหาการเลือนหายของเกรเดียนต์ในโครงข่ายประสาทเทียมเชิงลึก}
%\section{ความท้าทายของโครงข่ายเชิงลึก}
%\paragraph{อุปสรรคสำคัญและปัจจัยของความสำเร็จ.}

%\paragraph{ทำไมการเรียนรู้เชิงลึกจึงทำงานได้ดี}
%%เควิน คลาร์ค Kevin Clark cs224N (Stanford 2017?)
%* better tricks, e.g., CNN, RNN, ReLu, dropout, better optimizers (e.g., Adam),
%batch norm, attention, pre-training
%* better H/W allows larger models
%* larger datasets


ปัจจัยของความสำเร็จของ\textit{การเรียนรู้เชิงลึก}
หรือ\textit{โครงข่ายประสาทเทียมแบบลึก}
%ที่มีส่วนช่วยการฝึกโครงข่ายประสาทเทียมแบบลึก
มีอยู่หลายประการ
ตั้งแต่ ฮาร์ดแวร์ที่เร็วขึ้น,
ข้อมูลที่มากและหลากหลายขึ้น, 
วิธีการเตรียมข้อมูลที่ดีขึ้น รวมถึงการทำ\textit{การฝึกก่อน}\cite{ErhanEtAl2010},
\index{english}{pre-training}
\index{thai}{การฝึกก่อน}
ขั้นตอนวิธีการหาค่าดีที่สุดที่มีประสิทธิภาพมากขึ้น\cite{IoffeSzegedy2015a, KingmaBa2015}, 
\index{english}{batch normalization}
\index{english}{adam}
\textit{การฝึกทีละหมู่เล็ก}\cite{Bengio2012a},\index{english}{minibatch}
การใช้แบบจำลองที่จับลักษณะสำคัญของข้อมูลได้ดีขึ้น เช่น \textit{โครงข่ายคอนโวลูชั่น}\cite{LeCunEtAl2015a}\index{english}{CNN} และ\textit{โครงข่ายประสาทเวียนกลับ}\index{english}{RNN}\cite{HochreiterEtAl2001a, HochreiterSchmidhuber1997, SchusterPaliwal1997},
การใช้กลไก\textit{การตกออก}\index{english}{dropout}\cite{srivastavaEtAl2014a},  การใช้กลไก\textit{ความใส่ใจ}\index{english}{attention}\cite{Devlin2019BERTPO, VaswaniEtAl2017},
การใช้กลไก\textit{โครงข่ายปรปักษ์เชิงสร้าง}\index{english}{Generative Adversarial Network}\index{english}{GAN}%
\cite{GoodfellowEtAl2014a, PalssonEtAl2018}
ไปจนถึงการเปลี่ยน\textit{ฟังก์ชันกระตุ้น}จาก\textit{ซิกมอยด์}ไปเป็น\textit{ฟังก์ชันกระตุ้น}ที่ลดช่วงค่าอิ่มตัว เช่น \textit{เรลู}\cite{HintonSalakhutdinov2006a}.

บทที่~\ref{chapter: Deep Learning} นี้ อภิปรายการแก้ปัญหาการเลือนหายของเกรเดียนต์
ด้วยฟังก์ชันกระตุ้นเรลู (หัวข้อ~\ref{sec: relu}),
เทคนิคการจัดการฝึกกับข้อมูลขนาดใหญ่ด้วยการฝึกทีละหมู่เล็ก (หัวข้อ~\ref{sec: minibatch}),
เทคนิคการตกออก
(หัวข้อ~\ref{sec: dropout}),
วิธีการกำหนดค่าน้ำหนักเริ่มต้น
(หัวข้อ~\ref{sec: weight init}),
และขั้นตอนวิธีการฝึกที่มีประสิทธิภาพมากขึ้น
(หัวข้อ~\ref{sec: adv training opt}).
นอกจากนั้น ปัจจัยหนึ่งที่มีส่วนอย่างมาก
ในพัฒนาการ และความสนใจ  ไปจนถึงการประยุกต์ใช้ที่กว้างขวาง
ก็คือเครื่องมือที่ช่วยให้การใช้งานเทคนิคต่าง ๆ เหล่านี้ทำได้สะดวกมากขึ้น
หัวข้อ~\ref{section: deep exercises}
อภิปรายตัวอย่างเครื่องมือที่ได้รับความนิยมอย่างสูง สำหรับการประยุกต์ใช้การเรียนรู้เชิงลึก.

การใช้\textit{โครงข่ายคอนโวลูชั่น} ที่เหมาะกับข้อมูลที่มีลักษณะเชิงท้องถิ่นสูง เช่น ข้อมูลภาพ
% (หัวข้อ~\ref{sec: CNN}).
รวมไปจนถึงเทคนิคการฝึกก่อน
%(หัวข้อ~\ref{sec: pre-train})
และตัวอย่างโครงสร้างของ\textit{โครงข่ายคอนโวลูชั่น}ที่รู้จักกันอย่างกว้างขวาง
อภิปรายในบทที่~\ref{chapter: Convolution}. 
การนำ\textit{โครงข่ายคอนโวลูชั่น}
ไปประยุกต์ใช้กับงานการรู้จำทัศนรูปแบบ เป็นเนื้อหาหลักที่อภิปรายในบทที่~\ref{chapter: Convolution Applications}.

บทที่~\ref{chapter: RNN} อภิปรายโครงข่ายประสาทเวียนกลับ
ที่เหมาะกับข้อมูลที่มีลักษณะเชิงลำดับ.
ตัวอย่างการประยุกต์ใช้โครงข่ายประสาทเวียนกลับ
กับงานการรู้จำรูปแบบเชิงลำดับ
เช่น การประมวลผลภาษาธรรมชาติ
รวมไปถึง\textit{กลไกความใส่ใจ}
%(หัวข้อ~\ref{sec: attention})
เป็นเนื้อหาหลักที่อภิปรายในบทที่~\ref{chapter: NLP}.

\section{ปัญหาการเลือนหายของเกรเดียนต์}
\label{sec: relu}
\index{thai}{ปัญหาการเลือนหายของเกรเดียนต์}
\index{english}{vanishing gradient problem}

ปัญหาสำคัญที่ทำให้โครงข่ายลึกไม่ได้รับความนิยมในยุคต้นของพัฒนาการ
คือ
%
%ที่พบ เมื่อต้องการขยายความลึก จากโครงข่ายประสาทเทียมขนาดสองชั้น ไปสู่โครงข่ายประสาทเทียมที่มีความลึกมากขึ้น
%ก็คือ 
การฝึกโครงข่ายประสาทเทียมที่ลึกมากนั้นทำได้ยากมาก.

รูป~\ref{fig: deep vanishing gradient training losses}
แสดงความก้าวหน้าของการฝึก เมื่อใช้ความลึกต่าง ๆ.
แนวโน้มรวม ก็คือ ยิ่งความลึกมาก ดูเหมือนจะต้องการจำนวนสมัยฝึกที่มากขึ้น
ค่าฟังก์ชันสูญเสียต่อสมัยของความลึกที่มากขึ้น ลู่ลงที่จำนวนสมัยมากขึ้น.
.
สังเกต ความก้าวหน้าของการฝึกเมื่อใช้ความลึกสิบชั้น (เส้นทึบหนาสีดำ) ซึ่งมีลักษณะลู่ลงราบเรียบร้อย แสดงถึงการฝึกที่สมบูรณ์
แต่ผลการฝึกได้คุณภาพการทำนายแย่มาก.
นั่นคือ การฝึกเสร็จสิ้น แต่ฝึกไม่สำเร็จ 
ซึ่งยืนยันอย่างชัดเจนจากรูป~\ref{fig: deep vanishing gradient final training losses}
ที่สรุปค่าฟังก์ชันสูญเสียของการฝึก เมื่อใช้ความลึกต่าง ๆ.

ปัญหาของการฝึกโครงข่ายลึกแบบนี้ ภายหลังพบว่า
สาเหตุคือ ขนาดเกรเดียนต์ในชั้นต้น ๆ ของโครงข่ายที่เล็กมากเกินกว่าจะที่ปรับค่าน้ำหนักและไบอัสได้อย่างมีประสิทธิภาพ.
รูป~\ref{fig: deep vanishing gradient sigmoid} แสดงตัวอย่างขนาดเฉลี่ยของเกรเดียนต์ที่ชั้นต่าง ๆ
ของโครงข่ายประสาทเทียมสิบชั้น.
รูป~\ref{fig: deep vanishing gradient sigmoid definitive} สรุปค่าใหญ่ที่สุดของขนาดเฉลี่ยเกรเดียนต์ในชั้นต่าง ๆ.
สังเกตว่า ขนาดเฉลี่ยเกรเดียนต์แต่ละชั้นแตกต่างกันอย่างมาก.
(แบบฝึกหัด~\ref{ex: vanishing gradient}).

การฝึกโครงข่ายประสาทเทียม ก็คือการหาค่าน้ำหนักที่เหมาะสม 
ซึ่งแนวทางที่ใช้ก็คือใช้เกรเดียนต์ของ\textit{ฟังก์ชันจุดประสงค์}ต่อ\textit{ค่าน้ำหนัก}.
แต่หากเกรเดียนต์มีขนาดเล็กมาก การหาค่าน้ำหนักที่เหมาะสมก็ทำได้ยาก และในหลายสถานการณ์ก็คือความล้มเหลวของการฝึกโครงข่าย.
ปัญหาการฝึกโครงข่ายลึก ดังที่อภิปรายนี้ รู้จักกันในชื่อ\textbf{ปัญหาการเลือนหายของเกรเดียนต์} (vanishing gradient problem). 
%\cite{HochreiterEtAl2001a}). 
\index{english}{vanishing gradient problem} \index{thai}{ปัญหาการเลือนหายของเกรเดียนต์}
นั่นคือ ค่าเกรเดียนต์ที่คำนวณจาก\textit{วิธีแพร่กระจายย้อยกลับ}สำหรับค่าน้ำหนักในชั้นต้น ๆ (ใกล้อินพุต) จะมีขนาดเล็กลงมาก จนแทบไม่สามารถปรับค่าน้ำหนักได้
ซึ่งส่งผลให้ การฝึกโครงข่ายลึกล้มเหลว.

%
\begin{figure}
	\begin{center}
%		\begin{tabular}{cc}
		\includegraphics[width=0.9\textwidth]{05Deep/relu/traininglossesEmph10.png}	
%		\end{tabular}		
		\caption[ค่าฟังก์ชันสูญเสียต่อสมัยฝึกที่ความลึกต่าง ๆ]{
			ค่าฟังก์ชันสูญเสียต่อสมัยฝึก เมื่อใช้โครงข่ายประสาทเทียมที่ความลึกต่าง ๆ
			โดย ความลึกสองชั้นถึงเจ็ดชั้น โครงข่ายสามารถถูกฝึกได้ภายใน $5000$ สมัย
			แต่ความลึกแปดชั้นและเก้าชั้น ต้องทำการฝึกถึง $10000$ สมัย
			และที่ความลึกสิบชั้น (เส้นทึบหนาสีดำ) แม้ทำการฝึกไป $10000$ สมัยแล้ว ซึ่งความก้าวหน้าของการฝึกก็ดูคล้ายการฝึกสมบูรณ์ 
			แต่ได้ผลการฝึกที่แย่มาก.}
		\label{fig: deep vanishing gradient training losses}
	\end{center}
\end{figure}
%

%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[height=1.8in]{05Deep/relu/TrainLosses.png}
		\end{tabular}		
		\caption[การฝึกโครงข่ายลึกล้มเหลว]{การฝึกโครงข่ายลึกล้มเหลว.
			ภาพ แสดงการสรุปค่าฟังก์ชันสูญเสียสำหรับข้อมูลฝึกที่ความลึกต่าง ๆ.
			สรุปค่าฟังก์ชันสูญเสียสำหรับข้อมูลฝึก หลังจากฝึกเสร็จ ($5000$ สมัยสำหรับโครงข่าย $2$ ถึง $7$ ชั้น และ $10000$ สมัยสำหรับโครงข่าย $8$ ถึง $10$ ชั้น).
			ในขณะที่โครงข่ายที่ตื้นกว่าสามารถฝึกได้ดี แต่การฝึกโครงข่ายลึกกลับล้มเหลว.
		}
		\label{fig: deep vanishing gradient final training losses}
	\end{center}
\end{figure}
%			&
	
%
\begin{figure}
	\begin{center}
		\begin{tabular}{c}
			\includegraphics[width=\textwidth]{05Deep/relu/sigmoid10_dEw.png}	
		\end{tabular}		
		\caption[ปัญหาการเลือนหายของเกรเดียนต์]{ปัญหาการเลือนหายของเกรเดียนต์.
			ตัวอย่างความก้าวหน้าของการฝึกโครงข่ายประสาทเทียมสิบชั้น.
			แต่ละภาพแสดงค่าเฉลี่ยเกรเดียนต์ของค่าน้ำหนัก (เส้นทึบสีฟ้า) และไบอัส (เส้นประสีแดง) ของแต่ละชั้นคำนวณ (ระบุเหนือภาพ)
			โดยแกนนอนเป็นสมัยฝึก.
			%ภาพบนสุดซ้าย แสดงค่าฟังก์ชันสูญเสียต่อสมัยฝึก.
			%ภาพบนสุดขวา แสดงขนาดเฉลี่ยของเกรเดียนต์ของค่าน้ำหนักและของไบอัสในชั้นคำนวณต่าง ๆ.
			%ภาพอื่น ๆ แยกแสดงขนาดเฉลี่ยของเกรเดียนต์ดังระบุไว้ในชื่อเหนือภาพ.
		}
		\label{fig: deep vanishing gradient sigmoid}
	\end{center}
\end{figure}
%

%
\begin{figure}
	\begin{center}
		\begin{tabular}{c}
			\includegraphics[width=0.4\textwidth]{05Deep/relu/vanishing_gradients.png}	
		\end{tabular}		
		\caption[การเลือนหายของเกรเดียนต์เป็นสาเหตุของการฝึกโครงข่ายลึกล้มเหลว]{การเลือนหายของเกรเดียนต์ในโครงข่ายสิบชั้น.
		ภาพแสดงขนาดเฉลี่ยเกรเดียนต์ของแต่ละชั้นเปรียบเทียบกัน 
		โดย
		แกนตั้ง แสดงค่าใหญ่ที่สุดของขนาดเฉลี่ยเกรเดียนต์ของแต่ละชั้น
		และแกนนอนระบุชั้นคำนวณ.
		เกรเดียนต์ของค่าน้ำหนัก แสดงด้วยเส้นทึบสีฟ้า.
		เกรเดียนต์ของค่าไบอัส แสดงด้วยเส้นประสีแดง.
		เส้นไข่ปลาสีเทา แสดงแนวค่าศูนย์.
		ขนาดเกรเดียนต์ตั้งแต่ชั้นที่หกย้อนไปถึงชั้นที่หนึ่งมีค่าน้อยมาก ๆ (ใกล้ศูนย์)
		ซึ่งเป็นสาเหตุทำให้การฝึกล้มเหลว.
		}
		\label{fig: deep vanishing gradient sigmoid definitive}
	\end{center}
\end{figure}
%


การเลือนหายของเกรเดียนต์เอง ก็พบว่าสาเหตุหลักมาจากการใช้ฟังก์ชันกระตุ้นซิกมอยด์ ที่มีช่วงพลวัตรแคบ.
เพื่อแก้ปัญหาช่วงพลวัตรของฟังก์ชันกระตุ้นซิกมอยด์
\textit{ฟังก์ชันกระตุ้นเรคติไฟด์ลิเนียร์} (rectified linear ซึ่งมักย่อว่า เรลู relu สำหรับ rectified linear unit\cite{HintonSalakhutdinov2006a}) ถูกเสนอขึ้นมา.
การเปลี่ยนไปใช้ฟังก์ชันกระตุ้น ที่เป็น\textit{ฟังก์ชันเรคติไฟด์ลิเนียร์} หรือ\textit{เรลู}
เป็นปัจจัยที่สำคัญ ที่ช่วยให้การฝึกโครงข่ายประสาทเทียมเชิงลึกทำได้ง่ายขึ้น และช่วยลดปัญหาการเลือนหายของเกรเดียนต์.
สมการ~\ref{eq: deep rectified linear} แสดงการคำนวณของฟังก์ชันกระตุ้น\textit{เรลู}
\index{thai}{เรคติไฟด์ลิเนียร์}
\index{english}{rectified linear}
\index{english}{relu}
\index{english}{activation!relu}
\index{thai}{เรลู}
\begin{equation}
   \mathrm{relu}(a) = \left\{
     \begin{array}{l l}
        a & \quad \mbox{เมื่อ } a \geq 0, \\
        0 & \quad \mbox{เมื่อ } a < 0. 
     \end{array} \right.
\label{eq: deep rectified linear}
\end{equation}
\index{english}{relu}\index{thai}{เรลู}
ซึ่งอนุพันธ์สามารถคำนวณได้จาก
\begin{equation}
   \frac{d \mathrm{relu}}{da} = \left\{
     \begin{array}{l l}
        1 & \quad \mbox{เมื่อ } a \geq 0, \\
        0 & \quad \mbox{เมื่อ } a < 0.
     \end{array} \right.
\label{eq: deep rectified linear derivative}
\end{equation}

รูป~\ref{fig: deep activations and gradients} ภาพบนซ้ายแสดงการกระตุ้นของฟังก์ชันซิกมอยด์ เมื่อเปรียบเทียบกับการกระตุ้นของฟังก์ชันเรลู (ภาพบนขวา)
และค่าอนุพันธ์ของฟังก์ชันซิกมอยด์ (ภาพล่างซ้าย) และค่าอนุพันธ์ของฟังก์ชันเรลู (ภาพล่างขวา).
สังเกตว่า ค่าอนุพันธ์ของฟังก์ชันซิกมอยด์ จะมีช่วงพลวัตรอยู่ในบริเวณแคบ ๆ ใกล้ ๆ ศูนย์ 
ในขณะที่ ค่าอนุพันธ์ของฟังก์ชันเรลู
จะมีช่วงพลวัตรครอบคลุมบริเวณที่มีค่าเป็นบวกทั้งหมด.
การที่ค่าอนุพันธ์ของฟังก์ชันซิกมอยด์มีช่วงพลวัตรแคบ 
ทำให้การฝึกโครงข่ายประสาทเทียมที่ใช้ฟังก์ชันกระตุ้นเป็นซิกมอยด์ทำได้ยาก.
ในที่นี้ ช่วงพลวัตร หมายถึง ช่วงบริเวณของอินพุตที่ค่าอนุพันธ์มีขนาดใหญ่.
จากรูป~\ref{fig: deep activations and gradients} 
เราจะเห็นว่าค่าอนุพันธ์ของฟังก์ชันเรลู มีขนาดเป็นหนึ่ง ตลอดช่วงอินพุตที่มีค่าเป็นบวก 
เปรียบเทียบกับอนุพันธ์ของฟังก์ชันซิกมอยด์
ที่มีค่ามากกว่าศูนย์อย่างชัดเจน อยู่แค่บริเวณที่อินพุตมีค่าใกล้ ๆ ศูนย์เท่านั้น.

%
\begin{figure}
\begin{center}
\includegraphics[width=4in]{05Deep/vgradDemo04dActFn.png}
\end{center}
\caption[ฟังก์ชันกระตุ้นต่าง ๆ]{ฟังก์ชันกระตุ้นซิกมอยด์ (ภาพบนขวา) ฟังก์ชันกระตุ้นเรลู (ภาพบนซ้าย) เกรเดียนต์ของฟังก์ชันซิกมอยด์ (ภาพล่างขวา) เกรเดียนต์ของฟังก์ชันเรลู (ภาพล่างซ้าย).}
\label{fig: deep activations and gradients}
\end{figure}
%

รูป~\ref{fig: deep vanishing gradient mitigated with relu}
และ~\ref{fig: deep vanishing gradient sigmoid vs relu definitive}
แสดงให้เห็นว่า
การเปลี่ยนฟังก์ชันกระตุ้นจากซิกมอยด์มาเป็นเรลู ช่วยแก้ปัญหาการเลื่อนหายของเกรเดียนต์ได้อย่างชัดเจน.
%
เปรียบเทียบรูป~\ref{fig: deep vanishing gradient sigmoid definitive} (ใช้ฟังก์ชันกระตุ้นซิกมอยด์)
กับรูป~\ref{fig: deep vanishing gradient sigmoid vs relu definitive} (ใช้ฟังก์ชันกระตุ้นเรลู)
ซึ่งทั้งคู่ เป็นโครงข่ายประสาทเทียมสิบชั้นเหมือนกัน เพียงแต่ใช้ฟังก์ชันกระตุ้นต่างกัน.
จะเห็นว่า
เมื่อใช้ฟังก์ชันกระตุ้นซิกมอยด์
ขนาดของเกรเดียนต์จะลดลงเรื่อย ๆ จากชั้นสุดท้ายไปสู่ชั้นต้น.
นอกจากขนาดเกรเดียนต์ลดลงเรื่อย ๆ แล้ว ขนาดเกรเดียนต์ยังลดลงไปอยู่ในระดับใกล้ศูนย์ด้วย (เส้นประสีเทา แสดงแนวค่าศูนย์).
ขณะที่ เมื่อใช้ฟังก์ชันกระตุ้นเรลู (รูป~\ref{fig: deep vanishing gradient sigmoid vs relu definitive})
นอกจาก จะไม่เห็นแนวโน้มการลดลงของเกรเดียนต์จากชั้นสุดท้ายไปชั้นต้นแล้ว
(1) ขนาดเกรเดียนต์มีค่าใหญ่ขึ้นมาก
และ (2) ขนาดเกรเดียนต์มีค่าอยู่ในระดับมากกว่าศูนย์อย่างเห็นได้ชัด (อยู่เหนือเส้นประสีเทา).

รูป~\ref{fig: deep vanishing gradient final training losses compared}
แสดงให้เห็นว่า เมื่อแก้ปัญหาการเลื่อนหายของเกรเดียนต์ได้ 
การฝึกโครงข่ายลึกก็สามารถทำได้ดีขึ้นมาก.
% แสดงผลการใช้ฟังก์ชันกระตุ้นเรลูเปรียบเทียบกับการใช้ฟังก์ชันกระตุ้นซิกมอยด์ ซึ่งแสดงให้เห็นการแก้ปัญหาการฝึกโครงข่ายลึก.

%
\begin{figure}
	\begin{center}
		\begin{tabular}{c}
			\includegraphics[width=\textwidth]{05Deep/relu/relu10_dEw.png}	
		\end{tabular}		
		\caption[ความก้าวหน้าของการฝึกโครงข่ายประสาทเทียมสิบชั้น เมื่อใช้ฟังก์ชันกระตุ้นเรลู]{
			ตัวอย่างความก้าวหน้าของการฝึกโครงข่ายประสาทเทียมสิบชั้น เมื่อใช้ฟังก์ชันกระตุ้นเรลู.
			แต่ละภาพแสดงค่าเฉลี่ยเกรเดียนต์ของค่าน้ำหนัก (เส้นทึบสีฟ้า) และไบอัส (เส้นประสีแดง) ของแต่ละชั้นคำนวณ (ระบุเหนือภาพ)
			โดยแกนนอนเป็นสมัยฝึก.
		}
		\label{fig: deep vanishing gradient mitigated with relu}
	\end{center}
\end{figure}
%

%
\begin{figure}
	\begin{center}
		\begin{tabular}{c}
			\includegraphics[width=0.4\textwidth]{05Deep/relu/vanishing_gradients_mitigated.png}	
		\end{tabular}		
		\caption[การเลือนหายของเกรเดียนต์บรรเทาลงด้วยการเปลี่ยนฟังก์ชันกระตุ้น]{
			ขนาดเฉลี่ยเกรเดียนต์ของแต่ละชั้นเปรียบเทียบกัน 
			โดย
			แกนตั้ง แสดงค่าใหญ่ที่สุดของขนาดเฉลี่ยเกรเดียนต์ของแต่ละชั้น
			และแกนนอนระบุชั้นคำนวณ.
			เกรเดียนต์ของค่าน้ำหนัก แสดงด้วยเส้นทึบสีฟ้า.
			เกรเดียนต์ของค่าไบอัส แสดงด้วยเส้นประสีแดง.
			การใช้ฟังก์ชันกระตุ้นเรลู ช่วยให้ค่าเกรเดียนต์ของทั้งสิบชั้นมีขนาดค่าใหญ่พอสำหรับการปรับค่าน้ำหนักและไบอัสอย่างมีประสิทธิภาพ.
		}
		\label{fig: deep vanishing gradient sigmoid vs relu definitive}
	\end{center}
\end{figure}
%

%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[height=1.8in]{05Deep/relu/trainlosses_relu_sigmoid.png}
		\end{tabular}		
		\caption[ฟังก์ชันกระตุ้นเรลูช่วยให้การฝึกโครงข่ายลึกดีขึ้น]{ค่าฟังก์ชันสูญเสียสำหรับข้อมูลฝึกที่ความลึกต่าง ๆ เปรียบเท่ียบเมื่อใช้ฟังก์ชันกระตุ้นซิกมอยด์กับเมื่อใช้ฟังก์ชันกระตุ้นเรลู.
			ฟังก์ชันกระตุ้นเรลูช่วยให้การฝึกโครงข่ายลึกทำได้ดีขึ้นมาก.
		}
		\label{fig: deep vanishing gradient final training losses compared}
	\end{center}
\end{figure}

\section{การฝึกทีละหมู่เล็ก}
\label{sec: minibatch}
\index{english}{minibatch}
\index{thai}{หมู่เล็ก}

การมีข้อมูลจำนวนมาก แม้เป็นโอกาสที่ดี แต่ก็ต้องการกลไกที่ช่วยในการจัดการข้อมูลมหาศาล
เพื่อการใช้งานหน่วยประมวลผลและหน่วยความจำได้อย่างมีประสิทธิภาพ.
หนึ่งในกลไกที่สำคัญนั้น คือ \textit{การฝึกทีละหมู่เล็ก} (minibatch training).

บท~\ref{chapter: ANN} ได้อภิปรายถึง ทางเลือกในการฝึก 
โดย
การฝึกที่ใช้ข้อมูลทั้งหมดในการปรับปรุงค่าน้ำหนักพร้อม ๆ กันทีเดียว ซึ่งเรียกว่า
\textit{การฝึกแบบออฟไลน์} (offline training) หรือ \textit{การฝึกแบบหมู่}
และ
การฝึกที่ใช้ข้อมูลทีละจุดข้อมูลและปรับปรุงค่าน้ำหนักทีละครั้งสำหรับแต่ละจุดข้อมูล ซึ่งเรียกว่า
\textit{การฝึกแบบออนไลน์} (online training) หรือ \textit{การฝึกแบบส่วนเพิ่ม} (incremental mode).
นอกจากนั้น ยังได้อภิปรายถึงข้อดีและข้อเสียต่าง ๆ ของทางเลือกทั้งสองนี้
นั่นคือ \textit{การฝึกแบบออฟไลน์} สามารถทำการคำนวณได้อย่างรวดเร็ว และยังสามารถใช้การประมวลผลแบบขนาน (parallel processing) มาช่วยทำให้การคำนวณมีประสิทธิภาพมากขึ้นได้ แต่ข้อเสียคือ ต้องการหน่วยความจำมาก.
ในขณะที่ ข้อดีของการฝึกแบบออนไลน์ นอกจากต้องการใช้หน่วยความจำปริมาณน้อยกว่า คือ เมื่อทำร่วมกับการสลับลำดับของข้อมูลแต่ละสมัยฝึก จะช่วยลดความเสี่ยงในการเข้าไปติดอยู่ใน\textit{ค่าตัวทำต่ำสุดท้องถิ่น}ได้ หรือ กล่าวอีกอย่างอาจช่วยปรับปรุงคุณภาพของการฝึกได้.
ประเด็นเรื่องการฝึกแบบออนไลน์ช่วยคุณภาพของการฝึกนี้ 
กูดเฟโลและคณะ\cite{GoodfellowEtAl2016} เสริมว่า
การฝึกโดยใช้ข้อมูลทีละน้อย ๆ อาจจะช่วยให้ผลคล้ายการทำ\textit{เรกูลาไรซ์}\cite{WilsonMartinez2003}
ซึ่งอาจจะเพราะสัญญาณรบกวนที่ปนเข้ามาในกระบวนการฝึก.

การฝึกทีละ\textbf{หมู่เล็ก} (minibatch) เป็นเทคนิคที่ประณีประนอม ระหว่างแนวทาง\textit{การฝึกแบบหมู่} และ\textit{การฝึกแบบออนไลน์} เพื่อใช้เวลาในการฝึกไม่นานเกินไป ใช้หน่วยความจำไม่มากเกินไป และได้คุณภาพการฝึกที่ดี.
นั่นคือ การฝึกทีละหมู่เล็ก จะแบ่งข้อมูลออกเป็นกลุ่มเล็ก ๆ 
โดยที่ แต่ละกลุ่มมีจำนวนข้อมูลมากกว่าหนึ่งจุดข้อมูล แต่น้อยกว่าจำนวนข้อมูลฝึกทั้งหมด. 
การฝึกแต่ละ\textit{สมัย} จะปรับค่าน้ำหนักและไบอัสสำหรับการคำนวณกับแต่ละหมู่เล็กนี้ จนครบทุกหมู่.
สมการ~\ref{eq: deep minibatch} แสดงการคำนวณของค่าน้ำหนักสำหรับแต่ละหมู่เล็ก (การคำนวณไบอัสก็ทำได้ในลักษณะเดียวกัน)

\begin{eqnarray}
%w^{(l)}_{ji} \leftarrow w^{(l)}_{ji} - \alpha \sum_{n= (m-1) \cdot B + 1}^{m \cdot B} \frac{\partial E_n}{\partial w^{(l)}_{ji}}, 
w^{(l)}_{ji} \leftarrow w^{(l)}_{ji} - \alpha \frac{1}{|B_m|} \sum_{n \in B_m} \frac{\partial E_n}{\partial w^{(l)}_{ji}}, 
\label{eq: deep minibatch}
\end{eqnarray}
สำหรับ $m = 1, 2, \ldots, \left\lceil\frac{N}{|B|}\right\rceil$ 
เมื่อ 
$w^{(l)}_{ji}$ เป็นค่าน้ำหนักระหว่างหน่วย $j$ ของชั้น $l$ และหน่วย $i$  ของชั้นก่อนหน้า.
พจน์ $\frac{\partial E_n}{\partial w^{(l)}_{ji}}$ คือ ค่าอนุพันธ์ของฟังก์ชันจุดประสงค์ คำนวณจากจุดข้อมูลที่ $n^{th}$ จากจำนวนทั้งหมด $N$ จุดข้อมูล.
ตัวแปร $m$ แทนดัชนีของหมู่เล็ก.
สัญกรณ์ $|B_m|$ และ $|B|$ แทน \textit{ขนาดของหมู่เล็ก}ที่ $m^{th}$ และ\textit{ขนาดของหมู่เล็ก}ส่วนใหญ่ ตามลำดับ.
\textbf{ขนาดของหมู่เล็ก} (batch size) คือ จำนวนจุดข้อมูลในหมู่เล็ก.
เซต $B_m$ เป็นเซตของดัชนีของจุดข้อมูลที่ถูกจัดอยู่ในหมู่เล็กที่ $m^{th}$
โดยดัชนีของจุดข้อมูล จะถูกสุุ่มจัดเข้าหมู่เล็ก 
และหากจำนวนข้อมูลทั้งหมดไม่อาจแบ่งได้เท่า ๆ กันทุกหมู่เล็ก
จะมีหมู่เล็กหนึ่งหมู่ที่มีจำนวนต่างจากหมู่อื่น ๆ (หรือหมู่เศษนี้อาจถูกตัดทิ้ง เพื่อประสิทธิภาพของการคำนวณ).
\index{english}{minibatch}
\index{english}{minibatch!batch size}
\index{thai}{ขนาดของหมู่เล็ก}
\index{thai}{การฝึกทีละหมู่เล็ก!ขนาดของหมู่เล็ก}

การทำลักษณะเช่นนี้ จะคล้ายกับการฝึกแบบหมู่ ในแง่ที่ว่า แต่ละการคำนวณกับหมู่เล็กจะเป็นการคำนวณกับจุดข้อมูลหลาย ๆ จุดพร้อม ๆ กัน 
และก็จะคล้ายกับการฝึกแบบออนไลน์ ในแง่ที่ว่า ค่าน้ำหนักจะถูกปรับหลาย ๆ ครั้งในหนึ่งสมัย (แต่ละครั้ง ปรับสำหรับแต่ละหมู่เล็ก).
%
%หัวข้อ~\ref{sec: mini-batch example} แสดงตัวอย่างของการฝึกทีละหมู่

กูดเฟโลและคณะ\cite{GoodfellowEtAl2016}
อภิปรายว่า 
ขนาดของหมู่เล็กที่เหมาะสม ขึ้นกับทั้งฮาร์ดแวร์และ\textit{ขั้นตอนวิธีการหาค่าดีที่สุด}ที่เลือกใช้
เช่น หากใช้การคำนวณด้วย\textit{หน่วยประมวลผลกราฟิกส์} (Graphics Processing Unit หรือ GPU)
การเลือกขนาดหมู่เล็กเป็นจำนวนของสองยกกำลัง เช่น $16, 32, 64, 128, 256$ จะช่วยให้ได้เวลาประมวลผลที่เร็ว.
\textit{ขั้นตอนวิธีการหาค่าดีที่สุด}ที่ใช้แค่ค่าเกรเดียนต์  เช่น \textit{วิธีลงเกรเดียนต์} 
โดยทั่วไปแล้ว มักจะทนทานและสามารถได้งานได้ดีกับขนาดต่าง ๆ ของหมู่เล็กได้.

หมายเหตุ แต่\textit{ขั้นตอนวิธีการหาค่าดีที่สุด}ที่ใช้ทั้งค่าเกรเดียนต์และ\textit{เฮเชี่ยน}%
\footnote{%
\textit{เฮเชี่ยน} (Hessian) คือ เมทริกซ์ของอนุพันธ์อันดับที่สอง.
ขั้นตอนวิธีการหาค่าดีที่สุดหลายอย่าง อาศัย\textit{เฮเชี่ยน} เพื่อเพิ่มประสิทธิภาพในการทำงาน.
ตัวอย่างเช่น วิธีนิวตัน (Newton method) และ วิธีบีเอฟจีเอส (BFGS method)
ที่สามารถทำงานได้อย่างรวดเร็ว แต่ต้องการ\textit{เฮเชี่ยน}.
ดู ชองและเซค\cite{ChongZak2ndEd} เพิ่มเติมสำหรับรายละเอียดของขั้นตอนวิธีการหาค่าดีที่สุดที่อาศัย\textit{เฮเชี่ยน}.
}
ต้องการขนาดหมู่เล็กที่ใหญ่พอที่จะสามารถประมาณค่า\textit{เฮเชี่ยน}ได้อย่างมีประสิทธิภาพ เช่น\cite{GoodfellowEtAl2016} $10000$.

รูป~\ref{fig: deep minibatch batch sizes} 
แสดงเวลาที่ใช้ในการฝึก เมื่อใช้ขนาดหมู่เล็กต่าง ๆ.
หมายเหตุ การใช้ขนาดหมู่เล็กเป็นหนึ่ง เทียบเท่า\textit{การฝึกแบบออนไลน์}
และการใช้ขนาดหมู่เล็กเท่ากับหรือมากกว่าจำนวนข้อมูล (ซึ่งในตัวอย่างแสดงในรูป คือ $150$) เทียบเท่า\textit{การฝึกแบบหมู่}.
สังเกตว่า ขนาดหมู่ที่เล็กลง จะใช้เวลาในการฝึกนานขึ้น.
แม้ว่า การใช้ขนาดหมู่ที่เล็กลง จะทำให้เวลาในการฝึกนานขึ้น 
แต่ขนาดหมู่ที่เล็กลง ทำให้ในการคำนวณทำงานกับเมทริกซ์ขนาดเล็กลงด้วย.
การเลือกขนาดหมู่ เป็นเสมือนการหาสมดุลระหว่างเวลาฝึกและขนาดหน่วยความจำ.

นอกจากนั้น ผลทางอ้อมของการฝึกหมู่เล็ก ยังช่วยให้คุณภาพการฝึกดีขึ้นได้ด้วย (ถ้าทำอย่างเหมาะสม)
ดังแสดงในรูป~\ref{fig: deep minibatch batch sizes quality}.
กูดเฟโลและคณะ\cite{GoodfellowEtAl2016} อภิปรายว่า
ขนาดหมู่เล็ก อาจจะช่วยให้ผลในเชิงการ\textit{เรกูลาไรซ์} หรือคุมความซับซ้อนของแบบจำลอง
ช่วยลดโอกาสการ\textit{โอเวอร์ฟิต}ข้อมูลลง
ซึ่งอาจจะเป็น เพราะผลจากสัญญาณรบกวนจากการฝึกหมู่เล็ก ในกระบวนการหาค่าดีที่สุด.
ในทางปฏิบัติ การใช้ขนาดหมู่ที่เล็กลง มักจะทำให้ต้องการจำนวนสมัยฝึกน้อยลง
(ถึงแม้แต่ละสมัย อาจจะใช้เวลาฝึกนานขึ้น).
%ดูแบบฝึกหัด~\ref{ex: torch mnist different Ms}).

%
\begin{figure}
	\begin{center}
		\begin{tabular}{ccc}
			\includegraphics[height=1.8in]{05Deep/minibatch/norm_ttime_per_batchsize_fixed.png}
%			&
%			\includegraphics[height=1.8in]{05Deep/minibatch/memusage_batchsize.png}
		\end{tabular}		
		\caption[ตัวอย่างเวลาการฝึก เมื่อใช้ขนาดหมู่เล็กต่าง ๆ]{ตัวอย่างเวลาการฝึก เมื่อใช้ขนาดหมู่เล็กต่าง ๆ.
		โครงข่ายขนาดสองชั้น สี่ชั้น และสิบชั้น แสดงด้วยสัญลักษณ์ ดังระบุในภาพ.
		พื้นที่สีเขียวอ่อน ฟ้าอ่อน และชมพู แสดงช่วงระหว่างค่ามากที่สุดและน้อยที่สุดของเวลาฝึกโครงข่ายสองชั้น สี่ชั้น และสิบชั้น ตามลำดับ.
		หมายเหตุ พื้นที่สีเขียวอ่อน อาจสังเกตได้ยากในภาพ เนื่องจากเวลาที่ทดสอบพบว่ามีความผันผวนต่ำ.
		}
		\label{fig: deep minibatch batch sizes}
	\end{center}
\end{figure}

%
\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[height=1.8in]{05Deep/minibatch/test_acc_per_batchsize_fixed.png}	
\end{tabular}		
\caption[ตัวอย่างคุณภาพการทำนายเมื่อใช้ขนาดหมู่เล็กต่าง ๆ]{ตัวอย่างคุณภาพการทำนายเมื่อใช้ขนาดหมู่เล็กต่าง ๆ.
ในด้านคุณภาพการทำนาย ผลอาจจะไม่ได้ชี้ไปในแนวทางใดอย่างชัดเจนมาก 
นอกจากการใช้ขนาดหมู่ที่เล็กลง โดยรวมแล้ว ช่วยเพิ่มคุณภาพการทำนายจากการฝึกแบบหมู่ (\texttt{Batch size 150} ในภาพ).
โครงข่ายขนาดสองชั้น สี่ชั้น และสิบชั้น แสดงด้วยสัญลักษณ์ ดังระบุในภาพ.
}
\label{fig: deep minibatch batch sizes quality}
\end{center}
\end{figure}

กลไกที่สำคัญในการฝึกหมู่เล็ก คือการสุ่มลำดับของข้อมูล.
การสุ่มนี้อาจจะสุ่มครั้งเดียว และใช้ลำดับนั้นตลอด
หรือจะสุ่มทุกสมัยฝึกก็ได้.
ในทางปฏิบัติ 
กูดเฟโลและคณะ\cite{GoodfellowEtAl2016} อภิปรายว่า
ผลจากการสุ่มแค่ครั้งเดียว กับการสุ่มทุกสมัยฝึก ไม่ได้ต่างกันมาก
แต่สำคัญมาก ๆ ที่ต้องทำการสุ่ม.
รูป~\ref{fig: deep minibatch shuffle}
ภาพซ้าย แสดงเวลาในการฝึก เมื่อฝึกหมู่เล็กด้วยลำดับข้อมูลแบบต่าง ๆ ได้แก่ ไม่มีการสุ่มลำดับ, สุ่มลำดับครั้งเดียว, และสุ่มลำดับทุกสมัยฝึก.
ภาพขวา แสดงผลความแม่นยำของการทำนาย.
จากภาพ เวลาในการฝึกเมื่อทำการสุ่มครั้งเดียว ไม่ได้ต่างจากการไม่สุ่มมาก แต่การสุ่มทุกรอบฝึกมีผลในการเพิ่มเวลาฝึกอย่างชัดเจน
ในขณะที่ คุณภาพของการฝึก (ความแม่นยำในการทำนาย)
การสุ่มครั้งเดียว และการสุ่มทุกสมัย ไม่ได้ต่างกันมาก แต่มีผลดีกว่าการไม่สุ่มอย่างชัดเจน.
%0.84133333,  0.91666667,  0.92666667

%
\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics[height=1.8in]{05Deep/minibatch/normalized_training_time_per_shuffle.png}
&
\includegraphics[height=1.8in]{05Deep/minibatch/test_accuracy_shuffle.png}	
\end{tabular}		
\caption[ตัวอย่างผลจากการใช้วิธีจัดหมู่เล็กแบบต่าง ๆ]{ตัวอย่างผลจากการใช้วิธีจัดหมู่เล็กแบบต่าง ๆ.
ภาพซ้าย แผนภูมิกล่องแสดงเวลาที่ใช้.
ภาพขวา แผนภูมิกล่องแสดงผลการทดสอบของแบบจำลองที่ฝึกแบบหมู่เล็ก โดยจัดหมู่เล็ก 
(1) ตามลำดับข้อมูล (ไม่มีการเปลี่ยนลำดับ \texttt{none})
(2) ตามการสุ่มลำดับโดยสุ่มครั้งเดียวและใช้ลำดับที่สุ่มน้ำตลอดทุกสมัย (\texttt{once})
และ (3) ตามการสุ่มลำดับโดยสุ่มใหม่สำหรับแต่ละสมัย (\texttt{often}).
จุดสีน้ำเงิน แสดงค่าเฉลี่ยความแม่นยำ.
}
\label{fig: deep minibatch shuffle}
\end{center}
\end{figure}




%\paragraph{การสลับลำดับ}


%\begin{lstlisting}[language=python]
%http://keras.io/optimizers/

%## Learning Rate Decay

%lr = self.lr * (1. / (1. + self.decay * self.iterations))


%## nesterov

%v = self.momentum * m - lr * g  # velocity

%if self.nesterov:
%    new_p = p + self.momentum * v - lr * g
%else:
%    new_p = p + v
%\end{lstlisting}

%\subsection{ตัวอย่างเปรียบเทียบประโยชน์ของโครงข่ายลึก เมื่อเทียบกับโครงข่ายตื้น}

%ลอง MNIST ดีกว่า (แต่ต้องหลังจาก min batch ก่อน เพราะถ้าไม่ทำ minit batch run นานมาก 
%3 layers (8, 8, 8) => train 1.5 hrs on keras/tensorflow, test acc 0.91)
% ไม่สำเร็จ ลอง 1 layer (8) => ได้ test acc = (8, 8, 8)

%LATER
%{\small
%	\begin{shaded}
%		\paragraph{\small เกร็ดความรู้ รูปแบบความเชื่อมโยงของชีวิต}
%		\index{trophic cascades}
%		\index{ความเชื่อมโยงทางโภชนาการ}
%		\index{side story}
%		\index{side story!Life and Trophic Cascades}
%		\index{เกร็ดความรู้}
%		\index{เกร็ด!รูปแบบของชีวิตและความเชื่อมโยง}
%		
%		%LATER 
%		BREAK HERE!
%		\begin{center}
%			\begin{tabular}{ >{\arraybackslash}m{3.2in}  >{\arraybackslash}m{2.4in} }
%				``dummy ...''
%				&
%				``...''
%				\\
%				---dummy credit
%				&
%				---ใคร
%			\end{tabular} 
%		\end{center}
%		\index{words of wisdom}
%		\index{words of wisdom!who?}
%		\index{quote!what?}
%		
%		% Serengeti rule 1
%		(Rule 1) Keystones: Not all species are equal.
%		Some species exert effects on the stability and diversity of their communities that are disproportionate to their numbers or biomass.
%		The importance of keystone species is the magnitude of their influence, not their rung in the food chain.
%		
%		(Rule 2)
%		Some species mediate strong indirect effects through trophic cascades.
%		
%		Kick and see method.
%		
%		(Rule 3)
%		Competition: Some species compete for common resources.
%		Species that compete for space, food, or habitat can regulate the abundance of other species.
%		
%		(Rule 4)
%		Body size affects the mode of regulation.
%		Animal body size is an important determinant of the mechanism of population regulation in food webs, with smaller animals regulated by predators (top-down regulation)
%		and larger animals by food supply (bottom-up regulation)
%		
%		(Rule 5)
%		Density: The regulation of some species depends on their density.
%		Some animal populations are regulated by density-dependent factors that tend to stabilize population size.
%		
%		(Rule 6)
%		Migration increases animal numbers by increasing access to food (reducing bottom-up regulation) and decreasing susceptibility to predation (reducing top-down regulation).
%		
%		Tony Sinclair.
%		
%		\hspace{-0.5in}
%		\begin{center}
%			\begin{tabular}{ >{\arraybackslash}m{3.4in}  >{\arraybackslash}m{2.5in} }
%				``...''
%				&
%				``...''
%				\\
%				---?
%				&
%				---?
%			\end{tabular} 
%		\end{center}
%		\index{words of wisdom}
%		
%	\end{shaded}
%}%small

\section{เทคนิคการตกออก}
\label{sec: dropout}

เทคนิค\textbf{การตกออก} (drop out\cite{srivastavaEtAl2014a})
เป็นกลไกสำหรับการทำ\textit{เรกูลาไรซ์}สำหรับโครงข่ายประสาทเทียม
โดยได้รับแรงบันดาลใจ
จากการทำงานของโครงข่ายประสาททางชีววิทยา
ที่ผลการทำงานเชื่อถือได้สูง
ในขณะที่
เซลล์ประสาทต่าง ๆ ที่เป็นส่วนประกอบของโครงข่าย
แต่ละเซลล์มีการทำงานที่เชื่อถือไม่ค่อยได้.
นั่นคือ ในขณะที่ แต่ละเซลล์ บางครั้งอาจจะทำงาน บางครั้งอาจจะไม่ทำงาน
แต่ด้วยการที่โครงข่ายมีเซลล์จำนวนมาก
และการเชื่อมต่อได้เตรียมสำหรับความไม่แน่นอนนี้ไว้
ทำให้ผลโดยรวม ยังคงรักษาการทำงานที่เชื่อถือได้สูง.

สำหรับโครงข่ายประสาทเทียม
\textit{การตกออก} สามารถทำได้โดยการสุ่มเลือก\textit{หน่วยคำนวณ} ที่จะปิดการทำงาน
ซึ่ง
ทางการคำนวณ สามารถทำได้ง่ายๆ โดยคูณด้วยค่าศูนย์.
ดังนั้น อาจมองได้ว่า
\textit{การตกออก}
เป็นเสมือน
การใช้\textit{หน้ากาก} หรือค่าสัมประสิทธิ์ของการตกออก
$m$ ไปคูณกับค่า\textit{หน่วยคำนวณ} $z$
โดย ค่าของ $m$ สุ่มมาจากค่าศูนย์หรือหนึ่ง.

นั่นคือ
ค่า\textit{หน่วยคำนวณ}หลังทำ\textit{การตกออก} $\tilde{z}$
คำนวณได้จาก
$\tilde{z} = m \cdot z$
เมื่อ $m \sim \mathrm{Bernoulli}(p)$
โดย $\mathrm{Bernoulli}(p)$
หมายถึง
การแจกแจงแบบ\textit{แบร์นูลลี่} (Bernoulli distribution)
ที่โอกาสที่ค่า $m=1$ คือ $p$
นอกนั้น (โอกาส $1-p$)
$m = 0$.
%
การคำนวณการตกออก
เขียนในรูปเวกเตอร์ได้เป็น
\begin{eqnarray}
\tilde{\bm{z}}
&=& \bm{m} \odot \bm{z}
\label{eq: drop out} 
\end{eqnarray}
เมื่อ หน้ากาก $\bm{m}$ มีส่วนประกอบแต่ละตัวเป็นค่าที่สุ่มมาจากหนึ่งหรือศูนย์ (การแจกแจงแบบ\textit{แบร์นูลลี่}).

\textit{การตกออก}
จะให้ผลในลักษณะคล้ายกับการทำ\textit{เรกูลาไรซ์}
นั่นคือ ช่วย\textit{คุณสมบัติความทั่วไป}ของแบบจำลอง.
นอกจากนั้น
ยังเชื่อว่า
\textit{การตกออก}
ยังช่วยเพิ่มความยืดหยุ่น ความทนทานในการเชื่อมต่อ
ในลักษณะที่ช่วยลดการพึ่งพา\textit{คุณลักษณะ}ที่สำคัญไม่กี่อย่างลง
และเพิ่มโอกาสที่ทำให้แบบจำลองได้เรียนรู้\textit{คุณลักษณะ}ที่สำคัญต่าง ๆ ของรูปแบบได้ครบถ้วนมากขึ้น.
การสุ่มปิดการทำงานของ\textit{หน่วยคำนวณ}
เชื่อว่า น่าจะช่วย\textit{หยุดการปรับตัวร่วมกัน}
(break co-adaptation)
%break co-adaptation
และน่าจะส่งผลให้เกิดความหลากหลายในการเชื่อมต่อมากขึ้น.
และเพราะความหลากหลายในการเชื่อมต่อ 
สัมพันธ์โดยตรงกับความยืดหยุ่นกับความทนทานของระบบโดยรวม
จึงเชื่อว่า \textit{การตกออก} จะช่วยให้แบบจำลองมีความยืดหยุ่น
และทนทาน(ต่ออินพุตที่หลากหลาย)ได้ดีขึ้น.
\index{english}{drop out!break co-adaptation}
\index{english}{drop out!robustness}
\index{english}{drop out!learn features more thoroughly}

\paragraph{การใช้งาน\textit{การตกออก}.}
การสุ่มปิดการทำงาน มักจะใช้เฉพาะตอนฝึกเท่านั้น.
ตอนใช้งานอนุมาน จะเปิดการทำงานของทุกส่วน.
เนื่องจาก หากฝึกได้ดีพอ ส่วนย่อยต่าง ๆ ในโครงข่ายจะทำงานได้ดีพอสมควร
ดังนั้น 
หากเปิดทุกส่วนหมดพร้อมกัน
จะต้องทำการชดเชย เพื่อไม่ให้เอาต์พุตที่ได้มีค่ามากเกินไป.
การชดเชยนี้ มักถูกเรียกว่า \textbf{การปรับส่วนค่าน้ำหนัก} (weight scaling).
\index{thai}{การปรับส่วนค่าน้ำหนัก}
\index{english}{weight scaling}
นั่นคือ หากสุ่มด้วยความน่าจะเป็น $0.5$ 
หมายถึง โดยประมาณ \textit{หน่วยคำนวณ}ต่าง ๆ ในโครงข่ายจะทำงานแค่ครึ่งเดียว.
ถ้าหากเปิดทุก\textit{หน่วยคำนวณ}พร้อม ๆ กัน
จะต้องชดเชยด้วยการลดความแรงของค่า\textit{หน่วยคำนวณ}ลงครึ่งหนึ่ง.
และในกรณีทั่ว ๆ ไป สำหรับความน่าจะเป็นของการคงอยู่ $p$ แล้ว
ค่า\textit{หน่วยคำนวณ}
\begin{eqnarray}
\bm{z}' &=& p \cdot \bm{z}
\label{eq: dropout eval1}
\end{eqnarray}
เมื่อ $\bm{z}'$ คือค่า\textit{หน่วยคำนวณ}หลัง\textit{การปรับส่วนค่าน้ำหนัก}
และ $p$ เป็นค่าความน่าจะเป็นของการคงอยู่
และ $\bm{z}$ คือค่า\textit{หน่วยคำนวณ} (ก่อน\textit{การปรับส่วนค่าน้ำหนัก}).

%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[width=\textwidth]{05Deep/dropout/dropout_illustrated.png}	
		\end{tabular}		
		\caption[ภาพประกอบแสดงแนวคิดของการตกออก]{ภาพประกอบแสดงแนวคิดของการตกออก.
		ภาพแสดงโครงข่ายประสาทเทียมสามชั้น อินพุต (ห้ามิติ) อยู่ด้านบน และเอาต์พุต (สี่มิติ) อยู่ด้านล่าง ชั้นซ่อนทั้งสองชั้น แต่ละชั้นมีจำนวนหน่วยซ่อนแปดหน่วย.
		การฝึกโครงข่าย ใช้การตกออกโดย ความน่าจะเป็นของการคงอยู่เป็น $0.8$ สำหรับชั้นอินพุต และ $0.5$ สำหรับชั้นซ่อนทั้งสอง.
สี่โครงข่ายทางซ้าย แสดงตัวอย่างของการทำงานของโครงข่าย ขณะฝึก
ซึ่งแต่ละโครงข่าย สำหรับการคำนวณเกรเดียนต์แต่ละครั้ง.
แต่ละครั้ง การตกออกจะสุ่ม และให้ผลเป็นหน่วยคำนวณที่คงอยู่ต่าง ๆ  กันไป.
วงกลมสีเขียว แสดงหน่วยที่ทำงาน และวงกลมสีขาว แสดงหน่วยที่ถูกปิด.
โครงข่ายทางขวาสุด แสดงการทำงานของโครงข่าย ขณะใช้งานอนุมาน.
เมื่อใช้งาน ทุกหน่วยคำนวณจะทำงานพร้อม ๆ กันทั้งหมด แต่ค่าของหน่วยคำนวณจะถูกลดความแรงลง (ทำ\textit{การปรับส่วนค่าน้ำหนัก}) เพื่อไม่ให้เอาต์พุตมีค่ามากเกินไป.
สีของวงกลมที่ต่างไป สะท้อนการปรับค่าลงของหน่วยคำนวณ โดยชั้นอินพุตปรับลงเป็น $0.8$ เท่า และชั้นซ้อนปรับลงเป็น $0.5$ เท่า.}
		\label{fig: dropout}
	\end{center}
\end{figure}
%

รูป~\ref{fig: dropout} 
แสดงภาพประกอบแนวคิดของการตกออก.
ขณะฝึก จะมีหน่วยคำนวณบางส่วนถูกปิดไป และส่งผลเสมือนว่า 
กำลังใช้งานโครงข่ายย่อยอยู่
โดยที่โครงข่ายย่อยจะเปลี่ยนไปแบบสุ่มในการคำนวณเกรเดียนต์แต่ละครั้ง.
ขณะใช้งาน ทุกหน่วยคำนวณจะทำงานพร้อมกัน ดังนั้น เพื่อไม่ให้เอาต์พุตมีค่ามากเกินไป
ค่าหน่วยคำนวณจะถูกปรับขนาดลงอย่างเหมาะสม.
สังเกตว่า  การตกออก จะไม่ทำกับชั้นเอาต์พุต.

ในทางปฏิบัติ
การคำนวณค่าอนุมาน โดยชดเชยการตกออกที่ทำในขณะฝึก 
ค่อนข้างเทอะทะ และทำให้การใช้งานแบบจำลองต้องระมัดระวังมากในการนำค่าน้ำหนักที่ฝึกแล้วมาใช้.
นั่นคือ หากระหว่างการฝึก ไม่ได้ใช้การตกออก ก็ต้องไม่ทำ\textit{การปรับส่วนค่าน้ำหนัก}
แต่หากระหว่างการฝึก ทำการตกออก ก็ต้องทำ\textit{การปรับส่วนค่าน้ำหนัก}และปรับส่วนค่าน้ำหนักด้วยค่า $p$ ที่ใช้ (และแต่ละชั้นคำนวณสามารถทำการตกออก ด้วยค่า $p$ ที่ต่างกันได้).
ดังนั้น การนำค่าน้ำหนักที่ฝึกจากหลาย ๆ วิธีมาใช้ จะค่อนข้างยุ่งยากและมีความเสี่ยงมาก
รวมถึงยังจำกัดการฝึกด้วยว่า หากฝึกด้วยการตกออก แล้วต้องทำตลอดทุกสมัยฝึก และใช้ค่า $p$ เท่าเดิมตลอด.

เพื่อลดปัญหาดังกล่าว
การทำ\textit{การตกออก} จึงอาจเลือกทำ
\begin{eqnarray}
\tilde{\bm{z}}
&=& \frac{1}{p} \cdot \bm{m} \odot \bm{z}
\label{eq: drop out 2} 
\end{eqnarray}
ขณะฝึก
และไม่ต้องทำ\textit{การปรับส่วนค่าน้ำหนัก} ขณะใช้งานอนุมาน (นั่นคือ $\bm{z}' = \bm{z}$).
แนวทางนี้ ยืดหยุ่น สะดวก และลดความเสี่ยงของการชดเชยผิดลง.
การใช้ $\frac{1}{p}$ (ซึ่งมากกว่าหรือเท่ากับหนึ่ง)
เทียบเท่าการขยายขนาดของหน่วยคำนวณ ขณะทำการตกออก
ซึ่งจะบังคับให้แบบจำลองเรียนรู้ค่าน้ำหนัก ที่จะไม่ทำให้เอาต์พุตมีค่ามากเกินไป
ในอัตราส่วนที่สัมพันธ์กับโอกาสการคงอยู่.
ดังนั้น ค่าน้ำหนักที่ได้จึงสามารถนำไปใช้งานได้เลย โดยไม่ต้องทำ\textit{การปรับส่วนค่าน้ำหนัก}อีก.
%เมื่อไม่ได้ทำการตกออก 
%จึงไม่จำเป็นต้องชดเชย และทำให้การอนุมานของแบบจำลองสามารถคำนวณได้เช่นเดิม.

ประโยชน์ของ\textit{การตกออก}
ยังถูกมองว่า เป็นเพิ่มความน่าเชื่อถือได้ของการอนุมาน
ในลักษณะคล้ายแนวทาง\textbf{การจัดถุง} (bagging).
แนวทาง\textit{การจัดถุง}
เป็นหนึ่งในแนวทางหลัก ของ\textbf{การประสานการเรียนรู้}
(ensemble learning).
\index{thai}{การจัดถุง}
\index{english}{bagging}
\index{thai}{การประสานการเรียนรู้}
\index{english}{ensemble learning}
\textit{การประสานการเรียนรู้}
เป็นเทคนิคของ\textit{การเรียนรู้ของเครื่อง}
เพื่อปรับปรุงคุณภาพการอนุมาน
โดยใช้ค่าทำนายจากหลาย ๆ แบบจำลอง และนำค่าทำนายต่าง ๆ เหล่านั้นมาสรุปรวมเป็น ค่าทำนายของการประสานการเรียนรู้.
วิธีการสรุปอาจทำได้หลายแบบ ขึ้นกับภารกิจการทำนาย
เช่น หากเป็นการทำนายค่าถดถอย (เอาต์พุต $y \in \mathbb{R}$)
อาจใช้ค่าเฉลี่ยจากค่าทำนายของแบบจำลองต่าง ๆ.
แต่หากเป็นการจำแนกกลุ่ม (เอาต์พุต $y \in \{1, \ldots, K\}$ เมื่อ $K$ เป็นจำนวนกลุ่ม)
อาจสรุปโดยการ\textit{ลงคะแนนเสียง} (vote) นั่นคือ การใช้ค่าฐานนิยม หรือสรุปเป็นกลุ่มที่ถูกจำแนกมากที่สุด (ซึ่งอาจต้องการกลยุทธ์ในการจัดการกับกรณีเสมอกัน).

ในขณะที่ \textit{การประสานการเรียนรู้} เป็นเทคนิคแนวทางกว้างๆ ที่เน้นการนำผลทำนายจากหลาย ๆ แบบจำลอง มาสรุปร่วมกัน.
แนวทาง\textit{การจัดถุง} เป็นแนวทางการเตรียมแบบจำลองต่างๆ สำหรับใช้ใน\textit{การประสานการเรียนรู้}.

แบบจำลองต่างๆ ที่กล่าวถึงใน\textit{การประสานการเรียนรู้}
หมายถึง ฟังก์ชันการทำนายใดๆ ที่สร้างมาต่างกัน อาจจะโดยมีโครงสร้างทางคณิตศาสตร์ที่ต่างกัน (เช่น โครงข่ายประสาทเทียมสามชั้น กับโครงข่ายประสาทเทียมห้าชั้น หรือโครงข่ายประสาทเทียม กับซัพพอร์ตเวกเตอร์แมชชีน)
หรืออาจจะโดยมีโครงสร้างทางคณิตศาสตร์ที่เหมือนกัน แต่ผ่านกระบวนการฝึกที่ต่างกัน
เช่น ใช้ข้อมูลในการฝึกที่ต่างกัน
หรือต่างกันทั้งโครงสร้างทางคณิตศาสตร์และกระบวนการฝึก.

แนวทาง\textit{การจัดถุง}
เน้นการเตรียมแบบจำลองที่ต่างกัน ด้วยการใช้ข้อมูลฝึกที่ต่างกัน
นั่นคือ 
หากต้องการเตรียม $M$ แบบจำลองสำหรับใช้ใน\textit{การประสานการเรียนรู้}
จากชุดข้อมูลฝึกที่มีจำนวนจุดข้อมูลเป็น $N$
แนวทาง\textit{การจัดถุง} จะสร้างข้อมูลสำหรับฝึกขึ้นมา $M$ ชุด 
โดยแต่ละชุด จะสุ่มจุดข้อมูลจากชุดฝึกมาแบบหยิบคืน (sample with replacement) โดยจำนวนข้อมูลในแต่ละชุด $N'$ เป็นอภิมานพารามิเตอร์ของ\textit{การจัดถุง}.
จากนั้น แบบจำลองแต่ละตัว จะถูกฝึกกับข้อมูลที่สร้างขึ้นแต่ละชุด
และแบบจำลองทั้งหมดที่ฝึกเสร็จ ก็จะสามารถนำไปใช้ใน\textit{การประสานการเรียนรู้}ได้.

\textit{การตกออก} ถูกมองว่า เป็นกลไกในลักษณะคล้ายแนวทาง\textbf{การจัดถุง}
จากการที่ ขณะฝึก
การปรับค่าน้ำหนักแต่ละครั้ง จะมีเฉพาะบางส่วนของโครงข่ายเท่านั้นที่จะถูกปรับค่า
ส่วนที่ถูกปิดการทำงาน จะไม่ได้ถูกปรับค่าน้ำหนัก.
ดังนั้น เมื่อใช้งาน และเปิดการทำงานของทุกส่วน
จึงคล้ายการประสาทการเรียนรู้ของส่วนย่อยต่าง ๆ ภายในโครงข่าย.
แต่\textit{การตกออก} ก็ไม่ได้ทำให้การใช้งานโครงข่ายประสาทเทียมเหมือนการประสานการเรียนรู้แบบดั้งเดิม.
เพราะว่า
โดยทั่วไปแล้ว ส่วนต่าง ๆ ของโครงข่ายที่เปิดและปิดขณะฝึกจากกลไกของ\textit{การตกออก} จะมีการซ้อนทับกันอยู่มาก
ซึ่งต่างจาก การประสานการเรียนรู้แบบดั้งเดิม ที่แต่ละแบบจำลองมีความเป็นอิสระต่อกันสูงกว่ามาก.
อย่างไรก็ตาม ด้วยเหตุผลดังอภิปรายนี้ 
ในมุมมองหนึ่ง
\textit{การตกออก} ถูกตีความว่า น่าจะช่วยปรับปรุงคุณภาพของแบบจำลอง ได้จากกลไกที่ให้ผลในลักษณะของการประสานการเรียนรู้.

โดยทั่วไป \textit{การตกออก}
นิยมใช้ความน่าจะเป็นของการคงอยู่ $p$ เป็น $0.8$ สำหรับชั้นอินพุต และ $0.5$ สำหรับชั้นซ่อน
และ\textit{การตกออก} จะใช้งานได้ดีกับแบบจำลองที่มีขนาดใหญ่พอ (ความซับซ้อนมากเพียงพอ).
แต่\textit{การตกออก} อาจทำให้การฝึกทำได้ช้าลง.
การศึกษาของศรีวาสทาวาและคณะ\cite{srivastavaEtAl2014a}
รายงานว่า \textit{การตกออก} ให้ผลช่วยการฝึกได้ดีกว่าการทำ\textit{เรกูลาไรซ์}หลาย ๆ วิธี รวมถึง วิธีค่า\textit{น้ำหนักเสื่อม}.
ข้อเสียของการใช้ \textit{การตกออก}ที่สำคัญ
ก็เช่น\cite{GoodfellowEtAl2016} อาจทำให้การฝึกทำได้ช้าลง และอาจทำให้ต้องการแบบจำลองที่ใหญ่ขึ้น.
ลักษณะเดียวกับการทำ\textit{เรกูลาไรซ์}
\textit{การตกออก} อาจไม่ได้ช่วยมาก หากข้อมูลที่ฝึกมีปริมาณมาก
ซึ่งประโยชน์ที่ได้จากการทำ\textit{การตกออก} อาจจะน้อยกว่าข้อเสียที่จะทำให้ต้องการแบบจำลองใหญ่ขึ้น
และทำให้การฝึกช้าลง.

นอกจากเทคนิค\textit{การตกออก}แล้ว
เทคนิค\textit{การตกออก} ยังเป็นแรงบันดาลใจให้เกิดการพัฒนาเทคนิคอื่น ๆ ที่คล้าย ๆ กันจำนวนมาก
เช่น \textit{การตกออกเร็ว} (fast drop out\cite{WangManning2013a}),
\textit{การส่งเสริมการตกออก} (dropout boosting\cite{Warde-FarleyEtAl2014a}),
\textit{การเชื่อมตกออก} (DropConnect\cite{WanEtAl2013a}).
อย่างไรก็ตาม ด้วยผลลัพธ์การทำงาน ประสิทธิภาพ และความสะดวกของการใช้งาน
\textit{การตกออก} เป็นแนวทางที่ได้รับความนิยมสูงกว่าวิธีที่พัฒนาต่อ ๆ ขึ้นมาเหล่านี้.
กูดเฟโลและคณะ\cite{GoodfellowEtAl2016} 
อภิปรายสรุปว่า กลไกสำคัญที่เทคนิค\textit{การตกออก}เป็นตัวแทน
คือ การใส่ความไม่แน่นอนเข้าไปในการฝึกโครงข่าย
และทำการอนุมานโดยสรุปจากผลต่าง ๆ ที่ผ่านความไม่แน่นอน
ซึ่งในผลในลักษณะการ\textit{การจัดถุง} โดยมีการใช้พารามิเตอร์ร่วมกัน.
การใส่ความไม่แน่นอน เข้าไปจะช่วยให้แบบจำลองเรียนรู้ที่จะยืดหยุ่นขึ้น และครบถ้วนขึ้น
ซึ่งอาจจะคล้ายกับคน ที่เรียนรู้ที่จะยืดหยุ่นขึ้นและรอบคอบขึ้น เมื่อคำนึงความไม่แน่นอนที่อาจเกิดขึ้น.
ศรีวาสทาวาและคณะ\cite{srivastavaEtAl2014a}
ได้ทดลองใช้หน้ากากค่าจริง $\bm{m} \sim \mathcal{N}(\bm{1}, \bm{I})$ (การแจกแจงปกติ ที่มีค่าเฉลี่ยและค่าเบี่ยงเบนมาตราฐานเป็นหนึ่ง ไม่มี\textit{สหสัมพันธ์}ระหว่างตัวแปร)
แทนการแจกแจงแบร์นูลลี่ (สมการ~\ref{eq: drop out})
และพบว่า ได้ผลการทำงานที่ดีเช่นกัน
นอกจากนั้น หน้ากากค่าจริงนี้ มี\textit{ค่าคาดหมาย} $E[\bm{m}] = \bm{1}$
จึงไม่ต้องทำ\textit{การปรับส่วนค่าน้ำหนัก}.

\section{การกำหนดค่าน้ำหนักเริ่มต้น}
\label{sec: weight init}
\index{thai}{การกำหนดค่าน้ำหนักเริ่มต้น}
\index{english}{weight initialization}

การฝึกโครงข่ายประสาทเทียมแบบลึก
ใช้ขั้นตอนวิธี\textit{การหาค่าดีที่สุด}ที่อาศัยเกรเดียนต์.
ค่าเริ่มต้นของตัวแปร 
มีผลอย่างมากต่อการทำงาน\textit{การหาค่าดีที่สุด} 
เช่นการเริ่มต้นในตำแหน่งที่ค่าเกรเดียนต์พอดี จะช่วยทำให้การฝึกโครงข่ายทำได้ง่ายและเร็วขึ้น.
ในขณะที่การเริ่มต้นในตำแหน่งที่ค่าเกรเดียนต์เปลี่ยนแปลงอย่างรุนแรง อาจนำไปสู่ปัญหาเสถียรภาพของการฝึก
หรือหากเริ่มต้นในตำแหน่งที่ค่าเกรเดียนต์มีค่าน้อยมาก 
(มักอ้างถึงด้วยคำว่า ``ที่ราบ'' หรือ plateau)
อาจทำให้การฝึกไม่ก้าวหน้า หรือหยุดชะงักได้.

เนื่องจากความเข้าใจในกระบวนการเรียนรู้ของโครงข่ายประสาทเทียมยังไม่กระจ่างชัดสมบูรณ์
ปัจจัยต่าง ๆ ของการฝึก รวมถึงการกำหนดค่าน้ำหนักเริ่มต้น
จึงยังไม่มีข้อสรุปที่แน่ชัด.
นอกจาก
สิ่งหนึ่งที่จำเป็น
คือ ค่าน้ำหนักเริ่มต้น ต้องช่วยลดการปรับตัวไปเหมือน ๆ กัน (มักอ้างถึงเป็น break symmetry).
\index{english}{symmetry breaking}

การปรับตัวไปเหมือน ๆ กัน มาจากการที่โครงข่ายประสาทเทียมมีวิธีการคำนวณแต่ละหน่วยคำนวณเหมือน ๆ กัน.
ดังนั้นการที่แต่ละหน่วยคำนวณเริ่มต้นด้วยค่าเดียวกัน จะทำให้มันมีค่าเกรเดียนต์เท่ากัน และถูกปรับค่าไปเท่า ๆ กัน จนสุดท้าย แต่ละหน่วยคำนวณจะทำงานเหมือนกัน ตอบสนองกับรูปแบบย่อยเดียวกัน 
ไม่ได้แยกกันรับผิดชอบแต่ละรูปแบบย่อย ๆ
ทำให้ความสามารถโดยรวมของโครงข่าย ที่แม้จะมีจำนวนหน่วยคำนวณมาก แต่ให้ประสิทธิผลการทำงานเหมือนโครงข่ายที่มีหน่วยคำนวณน้อย (หรือ ในกรณีสุดโต่ง อาจทำงานเหมือนมีหน่วยคำนวณเดียว).

นอกจากลดการปรับตัวไปเหมือนกัน
อีกปัจจัยหนึ่งที่สำคัญในการกำหนดค่าเริ่มต้น
คือ
ขนาดของค่าน้ำหนักไม่ควรจะมากเกินไป
จนผลต่อเนื่อง
ให้เกรเดียนต์มีค่าน้อย (ฝึกยาก) หรือมากเกินไป (การคำนวณขาดเสถียรภาพ).
%และวิธีการได้มาซึ่งค่าน้ำหนักเริ่มต้น
%ก็ไม่ควรจะต้องทำการคำนวณมากเกินไป.
%ดังนั้น 
แนวปฏิบัติคือ
การใช้การสุ่มค่า เพื่อกำหนดค่าน้ำหนักเริ่มต้น.
การกำหนดค่าน้ำหนักเริ่มต้นด้วยการสุ่มจากการแจกแจงที่มีเอนโทรปีสูง (เช่น \textit{การแจกแจงเอกรูป})
สามารถทำได้ง่ายๆ และน่ามีโอกาสน้อยมาก
ที่ค่าน้ำหนักจะไปเริ่มต้นที่เดียวกัน แม้ว่าจะมีพารามิเตอร์ค่าน้ำหนักจำนวนมาก.

กูดเฟโลและคณะ\cite{GoodfellowEtAl2016}
อภิปรายว่า
แทนที่การสุ่ม
เราอาจจะคำนวณหาชุดค่าน้ำหนัก
ที่แต่ละชุดแตกต่างกันมาก ๆ ได้ 
เช่น ในกรณีที่เหมาะสม อาจใช้ขั้นตอนวิธี\textit{แกรมชมิดต์}%
\footnote{%
ขั้นตอนวิธีแกรมชมิดต์ (Gram-Schmidt algorithm)
เป็นวิธีคำนวณหาเวกเตอร์ที่ตั้งฉากกับเวกเตอร์ที่กำหนด.
ดู \cite{ChongZak2ndEd} เพิ่มเติมสำหรับรายละเอียด.
}
แต่แนวทางนี้ มักจะเพิ่มภาระการคำนวณก่อนการฝึกขึ้นมาก
และภาระการคำนวณก่อนการฝึกที่เพิ่มขึ้นมากนี้ 
อาจไม่คุ้มกับผลประโยชน์ที่ช่วยลดการภาระการคำนวณระหว่างการฝึกลง 
เมื่อเปรียบเทียบกับการใช้แนวทางการสุ่ม.

ค่าพารามิเตอร์น้ำหนัก $\bm{w}$
นิยมกำหนดค่าเริ่มต้นด้วยการสุ่ม
ส่วนค่าพารามิเตอร์ไบอัส $\bm{b}$
อาจกำหนดค่าเริ่มต้นเป็นค่าคงที่
หรืออาจจะสุ่มค่าเช่นเดียวกันก็ได้\cite{GoodfellowEtAl2016}.
การสุ่มค่าน้ำหนัก มักนิยมสุ่มจาก\textit{การแจกแจงเอกรูป}
หรือ\textit{การแจกแจงปกติ}.
ค่าการแจกแจงที่นิยม\cite{GlorotAISTATS2010}
%
%\footnote{
%ใช้เป็นดีฟอล์ต
%ของไพทอร์ช \texttt{nn}
%จาก
%\url{https://pytorch.org/docs/stable/nn.html}
%สืบค้น 21 พ.ค. 2563.
%}
คือ กำหนดค่าเริ่มต้นน้ำหนักจากการแจกแจงเอกรูป $\mathcal{U}\left(-\frac{1}{\sqrt{m_i}},\frac{1}{\sqrt{m_i}} \right)$
เมื่อ $m_i$ เป็นจำนวนอินพุตของชั้นคำนวณ 
(อาจอ้างถึงว่าเป็น ``จำนวนแผ่เข้า'' หรือ a number of fan-in units).
ค่า $\frac{1}{\sqrt{m_i}}$
เพื่อป้องกันไม่ให้ผลคำนวณมีค่าใหญ่เกินไป
จนผลเสียต่อเสถียรภาพของการฝึก
สำหรับโครงข่ายขนาดใหญ่.


ตัวอย่างเช่น
หากชั้นคำนวณ ทำ $\bm{a} = \bm{w}^T \bm{x} + \bm{b}$
กับ $\bm{z} = h(\bm{a})$
เมื่อ $h$ เป็นฟังก์ชันกระตุ้น
และอินพุตของชั้น $\bm{x} \in \mathbb{R}^{m_i}$.
ค่าน้ำหนักของชั้น $\bm{w} \in \mathbb{R}^{m_o \times m_i}$.
แล้ว ค่าเริ่มต้นของ $\bm{w}$ 
กำหนดโดย
\begin{eqnarray}
w_{kj}  &\sim& \mathcal{U}\left(-\frac{1}{\sqrt{m_i}},\frac{1}{\sqrt{m_i}} \right)
\label{eq: standard init}
\end{eqnarray}
เมื่อ $w_{kj}$ คือค่าน้ำหนักแต่ละค่า 
โดย $k = 1, \ldots, m_o$
และ 
$j = 1, \ldots, m_i$.


อย่างไรก็ตาม
เซเวียร์ โกลโรต์ และโยชัว เบนจิโอ\cite{GlorotAISTATS2010}
ศึกษาความยากของการฝึกโครงข่ายประสาทเทียม
ตีความผลที่ได้ และเมื่อประกอบกับผลงานศึกษาของแบรดลีย์\cite{Bradley2009} ที่พบว่า\textit{ความแปรปรวน} (variance) ของค่าเกรเดียนต์ที่แพร่กระจายย้อนกลับ
ลดลงเรื่อย ๆ ตามชั้นที่ย้อนกลับ
ทั้งคู่สันนิษฐานว่า
หาก\textit{ความแปรปรวน}
ของผลการกระตุ้น $\bm{z}$
และ\textit{ความแปรปรวน}ค่าเกรเดียนต์
ของแต่ละชั้นคำนวณมีค่าพอ ๆ กัน
จะช่วยให้สารสนเทศไหลผ่านได้ดีขึ้น และจะช่วยให้การฝึกโครงข่ายทำได้สะดวกขึ้น.
จากข้อสันนิษฐานดังกล่าว
ทั้งคู่วิเคราะห์\textit{ความแปรปรวน}ของของแต่ละชั้นคำนวณโดยประมาณ (อาศัยสมมติฐานหลายอย่าง 
รวมถึงสมมติฐานเชิงเส้น)
และเสนอว่า ควรกำหนดค่าเริ่มต้นสำหรับค่าน้ำหนัก
โดยให้
\begin{eqnarray}
\forall l, \mathrm{var}[\bm{w}^{(l)}] &=& \frac{2}{m_i^{(l)} + m_o^{(l)}}
\label{eq: premise xavier init}
\end{eqnarray}
เมื่อ $\mathrm{var}[\bm{w}^{(l)}]$
คือ\textit{ความแปรปรวน}ของค่าน้ำหนักชั้นคำนวณที่ $l^{th}$
และ $m_i^{(l)}$ คือจำนวนแผ่เข้า และ $m_o^{(l)}$ คือจำนวนหน่วยคำนวณในชั้น (หรือจำนวนเอาต์พุตของชั้นคำนวณ ที่อาจอ้างถึงเป็น ``จำนวนแผ่ออก'' หรือ a number of fan-out units).
เมื่อนำเงื่อนไขนี้ไปใช้กับ\textit{การแจกแจงเอกรูป} จะได้ว่า
\begin{eqnarray}
w_{kj} &\sim& \mathcal{U}\left( -\sqrt{\frac{6}{m_i + m_o}}, \sqrt{\frac{6}{m_i + m_o}}\right)
\label{eq: xavier initialization}.
\end{eqnarray}
%เมื่อ $w_{kj}$ คือค่าน้ำหนักแต่ละค่า.
การกำหนดค่าน้ำหนักด้วยนิพจน์~\ref{eq: xavier initialization}
นิยม เรียกว่า การกำหนดค่าน้ำหนักด้วย\textbf{วิธีเซเวียร์} 
(Xavier weight initialization).
\index{thai}{วิธีเซเวียร์}
\index{thai}{การกำหนดค่าน้ำหนักเริ่มต้น!วิธีเซเวียร์}
\index{english}{Xavier initialization}
\index{english}{weight initialization!Xavier}

การวิเคราะห์ของโกลโรต์และเบนจิโอ\cite{GlorotAISTATS2010}
คิดจากฟังก์ชันกระตุ้น\textit{ไฮเปอร์บอลิกแทนเจนต์} (tanh)
และฟังก์ชันกระตุ้น\textit{เครื่องหมายอ่อน} (softsign, $h(a) = \frac{a}{1+|a|}$).
\index{english}{softsign}
\index{thai}{ฟังก์ชันเครื่องหมายอ่อน}
\index{english}{activation!softsign}
\index{thai}{ฟังก์ชันกระตุ้น!ฟังก์ชันเครื่องหมายอ่อน}
ทั้งคู่เป็นฟังก์ชันที่สมมาตรที่ศูนย์%
\footnote{% 
ในแง่ที่ว่า ค่าห่างจากศูนย์ไปทางซ้ายและขวาเท่า ๆ กัน 
จะหักล้างกันได้.
}
ไคมิง เห้อ และคณะ\cite{He2015DelvingDI}
พบว่า เงื่อนไขที่โกลโรต์และเบนจิโอวิเคราะห์
อาจจะไม่เหมาะ
เมื่อพิจารณาฟังก์ชันกระตุ้นที่ไม่สมมาตรที่ศูนย์
เช่น ฟังก์ชันเรลู ที่นิยมใช้กับโครงข่ายลึก.
ตามแนวทางของโกลโรต์และเบนจิโอ
คณะของไคมิง เห้อ\cite{He2015DelvingDI}
ทำการวิเคราะห์เงื่อนไขของค่าน้ำหนัก โดยพิจารณาฟังก์ชันกระตุ้นเรลู และฟังก์ชันอื่นในลักษณะคล้ายกัน.
ฟังชั่งตระกูลเรลู
ที่คณะของเห้อพิจารณา
อาจเขียนเป็นรูปทั่วไปได้ดังสมการ~\ref{eq: PRelu}.
\begin{eqnarray}
h(a) = \left\{\begin{array}{cc}
a, & \mbox{เมื่อ } a > 0, \\
\alpha \cdot a, & \mbox{เมื่อ } a \leq 0.
\end{array}
\right.
\label{eq: PRelu}
\end{eqnarray}
เมื่อ $\alpha$ คือ พารามิเตอร์ของฟังก์ชัน.
หาก $\alpha = 0$ 
จะทำให้ $h(a)$ เป็นฟังก์ชันเรลู.
หาก $\alpha > 0$ เป็นค่าคงที่ โดยเป็นอภิมานพารามิเตอร์ที่กำหนดโดยผู้ใช้ 
จะทำให้ $h(a)$ เป็นฟังก์ชัน\textbf{เรลูรั่ว} (leaky relu).
นอกจากนั้น
คณะของเห้อ ได้เสนอฟังก์ชันกระตุ้นที่สามารถปรับตัวได้
โดยให้ $\alpha > 0$ เป็นค่าพารามิเตอร์ที่ถูกฝึกไปพร้อม ๆ กับค่าน้ำหนักและไบอัส
และคณะของเห้อ เรียกฟังก์ชันกระตุ้นนี้ว่า 
ฟังก์ชัน\textbf{พีเรลู} (PRelu).
\index{english}{PRelu}
\index{english}{leaky relu}
\index{english}{activation!PRelu}
\index{english}{activation!leaky relu}

เงื่อนไขที่คณะของเห้อเสนอ แสดงในสมการ~\ref{eq: premise kaiming init}.
\begin{eqnarray}
\mathrm{var}[\bm{w}^{(l)}] &=& \frac{2}{(1 + \alpha^2) \cdot m_i^{(l)} }
\label{eq: premise kaiming init}
\end{eqnarray}
เมื่อ $\alpha$ เป็นพารามิเตอร์ของฟังก์ชันตระกูลเรลู.

เมื่อนำเงื่อนไขในสมการ~\ref{eq: premise kaiming init} ไปใช้กับการแจกแจงปกติ (ที่มักนิยมใช้กับเงื่อนไขของคณะของเห้อ)
จะได้ว่า
\begin{eqnarray}
w_{kj} &\sim& \mathcal{N}\left(0, \sqrt{\frac{2}{(1 + \alpha^2) \cdot m_i}}\right)
\label{eq: kaiming init}.
\end{eqnarray}
การกำหนดค่าน้ำหนักเริ่มต้น
ด้วยนิพจน์~\ref{eq: kaiming init}
รู้จักกันทั่วไปในชื่อ การกำหนดค่าน้ำหนักด้วย\textbf{วิธีไคมิง} (Kaiming weight initialization).
สังเกต หากพิจารณากรณีฟังก์ชันกระตุ้นเรลู ($\alpha = 0$)
ความแปรปรวนของค่าน้ำหนัก จากเงื่อนไขไคมิง
คือ $\frac{2}{m_i}$.
ในขณะที่นิพจน์~\ref{eq: standard init}
ส่งผลให้
ความแปรปรวนของค่าน้ำหนัก คือ
$\frac{1}{3 \cdot m_i}$ (โดยนิพจน์~\ref{eq: standard init} ไม่คำนึงถึงฟังก์ชันกระตุ้น).

นอกจาก
การกำหนดค่าน้ำหนักเริ่มต้น
ด้วยการสุ่มดังอภิปรายนี้แล้ว
การทำ\textit{การฝึกก่อน} (หัวข้อ~\ref{sec: deep adv train})
เพื่อได้ค่าน้ำหนักที่ดี ก่อนที่จะทำการฝึกแบบจำลองสำหรับภารกิจที่ต้องการจริงๆ เป็นแนวทางหนึ่งที่ให้ผลดีมากในทางปฏิบัติ.
%หัวข้อ~\ref{sec: pre-train}
%อภิปราย\textit{การฝึกก่อน}.



%LATER/ WAY LATER
%{\small
%\begin{shaded}
%\paragraph{\small เกร็ดความรู้ ควอนตัมฟิสิกส์ และจิตสำนึก}
%\index{side discourse}
%
%%\paragraph{\small ชีวิต}
%%https://en.wikipedia.org/wiki/Meaning_of_life#Questions
%
%%https://en.wikipedia.org/wiki/Metabolism
%
%Consciousness and quantum physics
%
%consciousness v.s. mind
%
%https://en.wikipedia.org/wiki/Wigner%27s_friend
%
%
%วงการวิทยาศาสตร์ อ้างถึง ความลับของเอกภพ ความลับของชีวิต และความลับของจิต ว่าเป็นสามความลับที่ยิ่งใหญ่ที่สุด 
%
%\begin{center}
%\begin{tabular}{ >{\arraybackslash}m{3.2in}  >{\arraybackslash}m{2.4in} }
%``Two truths cannot contradict one another.''
%&
%``ไม่มีความจริงสองอย่างที่ขัดแย้งกัน''
%\\
%-- Ibn Rushd
%&
%-- อิบน์ รุชิด
%\end{tabular} 
%\end{center}
%\index{words of wisdom}
%
%\end{shaded}
%}%small



%\subsection{ตัวอย่าง}
%ตัวอย่างจาก ศรีวาสทาวาและคณะ\cite{srivastavaEtAl2014a}

\section{กลไกช่วยการฝึก}
\label{sec: adv training opt}

ขณะที่\textit{วิธีลงเกรเดียนต์}
หรือมักนิยมเรียก
\textit{วิธีลงเกรเดียนต์สโทแคสติก} (stochastic gradient descent)
\index{thai}{วิธีลงเกรเดียนต์สโทแคสติก}
\index{english}{stochastic gradient descent}
ที่เน้นถึงการสุ่มลำดับของการฝึกทีละหมู่เล็ก
เป็นวิธีที่นิยมใช้ในการฝึกโครงข่ายประสาทเทียม.

แต่บ่อยครั้งที่อาจพบว่า
\textit{วิธีลงเกรเดียนต์สโทแคสติก}
ทำให้การฝึกทำได้ช้า.
หลาย ๆ เทคนิคจากศาสตร์\textit{การหาค่าดีที่สุด}
ได้ถูกนำมาใช้
เพื่อปรับปรุงประสิทธิภาพการฝึก.
นอกจากนั้น 
ยังมีเทคนิคจำนวนมากที่พัฒนาขึ้นมาโดยเฉพาะสำหรับการเรียนรู้ของเครื่อง โดยเฉพาะการเรียนรู้เชิงลึก.
หัวนี้ อภิปรายเทคนิคต่าง ๆ บางส่วน 
โดยเฉพาะ
เทคนิคเด่น ๆ
ที่มีการใช้อย่างกว้างขวางกับการเรียนรู้เชิงลึก.

\subsection{กลไกโมเมนตัม}
\label{sec: momentum}

ในสถานะการณ์ที่ค่า\textit{ฟังก์ชันจุดประสงค์}ต่อค่า\textit{ตัวแปรตัดสินใจ}ต่าง ๆ
(ซึ่งหมายถึง ฟังก์ชันสูญเสียและค่าน้ำหนักและไบอัสทั้งหลาย ในกรณีโครงข่ายประสาทเทียม)
มีลักษณะความสัมพันธ์ที่มีการเปลี่ยนแปลงเร็ว
หรืออาจจะมีสัญญาณรบกวนมาก
กลไกของ\textbf{โมเมนตัม} (momentum\cite{Polyak1964})
\index{thai}{โมเมนตัม}
\index{english}{momentum}
นิยมถูกนำมาใช้เพื่อช่วยเพิ่มประสิทธิภาพการทำงานของขั้นตอนวิธีการหาค่าดีที่สุด.

รูป~\ref{fig: zig-zag behavior}
แสดงพฤติกรรมการทำงานของ\textit{วิธีลงเกรเดียนต์}
เมื่อ
ความสัมพันธ์ของฟังก์ชันสูญเสียกับค่าน้ำหนัก
ที่มีลักษณะโค้ง
แต่ความโค้งแตกต่างกันมากระหว่างน้ำหนักแต่ละตัว.
พฤติกรรมการทำงานปรับค่าน้ำหนัก
ของวิธีลงเกรเดียนต์
จะแสดงออกในลักษณะส่ายเข้าหาคำตอบ.

แทนที่จะใช้ค่าเกรเดียนต์เพียงอย่างเดียว
กลไกของ\textit{โมเมนตัม}
เสนอที่จะใช้
ทิศทางเดิม
ประกอบกับทิศทางใหม่
เพื่อลดการส่ายเข้าหาคำตอบ
เพื่อปรับค่าตัวแปร.
นั่นคือ
\begin{eqnarray}
\bm{v}^{(i+1)} &=&
\beta \bm{v}^{(i)} - \alpha 
\nabla L(\bm{\theta}^{(i)})
\label{eq: momentum update} \\ 
\bm{\theta}^{(i+1)} &=& \bm{\theta}^{(i)} + \bm{v}^{(i+1)} 
\label{eq: gd with momentum}
\end{eqnarray}
เมื่อ
$\bm{v}$ เป็นเวกเตอร์สำหรับปรับค่าตัวแปร
และ $\beta$ เป็นค่าโมเมนตัม.
ส่วน $\nabla L(\bm{\theta}^{(i)})$
คือ เกรเดียนต์ต่อตัวแปรตัดสินใจ
และ $\alpha$ คืออัตราการเรียนรู้. 
และ $\bm{\theta}$ เป็นตัวแปรตัดสินใจ เช่น ค่าน้ำหนักและไบอัส.
ตัวยก ระบุสมัยฝึก. % เช่น $i^{th}$
ค่าเริ่มต้นของ $\bm{v}$ อาจกำหนดเป็น $\bm{0}$.

เปรียบเทียบกับ
$\bm{\theta}^{(i+1)} = \bm{\theta}^{(i)} -\alpha \nabla L(\bm{\theta}^{(i)})$ ซึ่งเป็นวิธีลงเกรเดียนต์ที่ไม่มีกลไกโมเมนตัม
จะเห็นว่า หากให้ $\beta = 0$ นั่นเท่ากับปิดกลไกโมเมนตัม และการทำงานของสมการ~\ref{eq: momentum update} และ~\ref{eq: gd with momentum} จะลดรูปมาเป็นวิธีลงเกรเดียนต์ดั้งเดิม.

รูป~\ref{fig: momentum compared}
แสดงตัวอย่างที่กลไกโมเมนตัม ช่วยปรับปรุงประสิทธิภาพการฝึก
โดยลดการส่ายเข้าหาคำตอบระหว่างการฝึกลง.
นอกจาก กลไกของโมเมนตัม
ซึ่งเป็นเทคนิคที่รู้จักกันดีในวงการ\textit{การหาค่าดีที่สุด}อยู่แล้ว
อีเลีย ซุตส์เกเวอร์ (Ilya Sutskever)
นักวิจัยการเรียนรู้ของเครื่องชั้นนำ
ได้เสนอ
\textit{เนสเตอรอฟโมเมนตัม} (Nesterov momentum\cite{SutskeverEtAl2013a})
ซึ่งคำนวณสมการ~\ref{eq: nesterov momentum update} แทนสมการ~\ref{eq: momentum update}
\index{thai}{เนสเตอรอฟโมเมนตัม}
\index{english}{Nesterov momentum}

\begin{eqnarray}
\bm{v}^{(i+1)} &=&
\beta \bm{v}^{(i)} - \alpha 
\nabla L(\bm{\theta}^{(i)} + \beta \bm{v}^{(i)})
\label{eq: nesterov momentum update}.
\end{eqnarray}

เปรียบเทียบกับสมการ~\ref{eq: momentum update}
\textit{เนสเตอรอฟโมเมนตัม}
ใช้ค่าเกรเดียนต์ ที่คำนวณ
ณ ตำแหน่งค่าตัวแปรที่ขยับต่อออกมาตามโมเมนตัม
แทนตำแหน่งค่าตัวแปรปัจจุบัน.
ดูแบบฝึกหัด~\ref{ex: deep optim} %{ex: deep momentum} 
เพิ่มเติมสำหรับการใช้งานกลไกโมเมนตัม.

%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[height=3in]{05Deep/deepopt/m0.png}	
		\end{tabular}		
		\caption[ภาพแสดงพฤติกรรมซิกแซกของวิธีลงเกรเดียนต์]{ภาพคอนทัวร์ของฟังก์ชันสูญเสียต่อตัวแปรค่าน้ำหนักสองตัว $w_1$ และ $w_2$
		พร้อมเส้นทางการปรับค่าน้ำหนัก.
		เส้นสีเทา แสดงระดับค่าของฟังก์ชันสูญเสีย.
	เส้นทึบสีแดง แสดงเส้นทางการปรับค่าน้ำหนัก ด้วยวิธีลงเกรเดียนต์.
ในภาพ ค่าเริ่มต้นจาก $(w_1, w_2)=(2, 3.5)$ (บริเวณด้านบนทางขวา).
จุดที่ค่าฟังก์ชันสูญเสียต่ำสุดอยู่ที่ $(0,0)$ (กลางภาพ).
สังเกตเส้นทางการปรับค่าตัวแปร
และเป็นลักษณะซิกแซก.}
		\label{fig: zig-zag behavior}
	\end{center}
\end{figure}
%


%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[height=3in]{05Deep/deepopt/momentum.png}	
		\end{tabular}		
		\caption[ภาพแสดงการทำงานของกลไกโมเมนตัม]{ภาพแสดงการทำงานของกลไกโมเมนตัม (เส้นประสีน้ำเงิน) เปรียบเทียบกับ
		การไม่ใช้โมเมนตัม (เส้นทึบสีแดง).
		พื้นหลังแสดงคอนทัวร์ของฟังก์ชันสูญเสียต่อค่าตัวแปร.
		ในภาพ ทั้งสองวิธีเริ่มต้นจาก $(w_1, w_2)=(2, 3.5)$ (บริเวณด้านบนทางขวา).
		จุดที่ค่าฟังก์ชันสูญเสียต่ำสุดอยู่ที่ $(0,0)$ (กลางภาพ).
	สังเกต โมเมนตัมช่วยลดการส่ายของเส้นทางการปรับค่าตัวแปรลง.}
		\label{fig: momentum compared}
	\end{center}
\end{figure}
%

\subsection{ขั้นตอนวิธีที่ปรับค่าอัตราเรียนรู้}
\label{sub: adv optimization algo.}

\paragraph{อดาแกรต.}
\textit{อดาแกรต} (AdaGrad)
ปรับอัตราเรียนรู้สำหรับพารามิเตอร์แต่ละตัว 
โดยลดขนาดอัตราเรียนรู้ลง
ตามขนาดรากที่สองของผลรวมกำลังสองของเกรเดียนต์ที่ผ่านมา.
นั่นคือ
พารามิเตอร์ $\bm{\theta}$ จะถูกปรับค่าโดย
%
\begin{align}
\bm{\theta}^{(i+1)} = 
\bm{\theta}^{(i)} - \frac{\alpha}{\sqrt{\bm{r}^{(i+1)}} + \epsilon} \odot \bm{g}
\label{eq: adagrad}
\end{align}
เมื่อ ผลรวมกำลังสองของเกรเดียนต์ที่ผ่านมา
$\bm{r}^{(i+1)} = \bm{r}^{(i)} + 
\bm{g} \odot \bm{g}$
และเกรเดียนต์
$\bm{g} = \nabla L(\bm{\theta}^{(i)})$
โดย $\alpha$ คืออัตราเรียนรู้(ฐาน) ที่ผู้ใช้กำหนด
และ $\epsilon$ คือค่าคงที่ขนาดเล็ก เช่น $10^{-7}$ สำหรับเสถียรภาพการคำนวณ.
ค่า $\bm{r}^{(0)}$ อาจกำหนดเป็น $\bm{0}$.
สังเกตว่า 
\textit{อดาแกรต} ปรับอัตราการเรียนรู้แยกกันสำหรับพารามิเตอร์แต่ละตัว.
ในทางปฏิบัติพบว่า
\textit{อดาแกรต} 
ใช้งานได้ดีบางครั้ง และอาจลดอัตราเรียนรู้มากเกินไปในบางครั้ง\cite{GoodfellowEtAl2016}.


\paragraph{อาร์เอมเอสพรอป.}
\textit{อาร์เอมเอสพรอป} (RMSProp)
%ฮินตัน\cite{Hinton2012a}
ปรับปรุง\textit{อดาแกรต}
ด้วยการใช้\textit{ค่าเฉลี่ยเคลื่อนที่ถ่วงน้ำหนักแบบชี้กำลัง} (exponentially weighted moving average).
นั่นคือ
พารามิเตอร์ $\bm{\theta}$ จะถูกปรับค่าโดย
%
\begin{align}
\bm{\theta}^{(i+1)} = 
\bm{\theta}^{(i)} - \frac{\alpha}{\sqrt{\bm{r}^{(i+1)} + \epsilon}} \odot \bm{g}
\label{eq: RMSprop}
\end{align}
เมื่อ\textit{ค่าเฉลี่ยเคลื่อนที่ถ่วงน้ำหนักแบบชี้กำลัง}
$\bm{r}^{(i+1)} = \rho \cdot \bm{r}^{(i)} + (1 - \rho) \cdot \bm{g} \odot \bm{g}$
โดยอัตราการเสื่อมน้ำหนัก $\rho$ เป็นอภิมานพารามิเตอร์ที่เพิ่มขึ้นมา
และค่าคงที่ $\epsilon$ มักถูกเลือกเป็น $10^{-6}$.

แม้ว่า 
การเสนอ\textit{อาร์เอมเอสพรอป}ครั้งแรกไม่ได้ถูกเผยแพร่ด้วยช่องทางปกติสำหรับงานวิชาการ (การตีพิมพ์ในวารสารหรือการประชุมวิชาการ)
แต่เป็นส่วนหนึ่งของการบรรยายในการสอนออนไลน์\cite{Hinton2012a}
ในทางปฏิบัติ \textit{อาร์เอมเอสพรอป}เป็นหนึ่งในขั้นตอนวิธีที่ใช้งานได้ดี และมีการใช้งานอย่างแพร่หลายสำหรับการฝึกแบบจำลองเชิงลึก\cite{GoodfellowEtAl2016}.


\paragraph{อดัม.}
\textit{อดัม} (Adam\cite{KingmaBa2015}
ย่อจาก adaptive moments)
รวม\textit{อาร์เอมเอสพรอป}
เข้ากับโมเมนตัม
โดยเพิ่มกลไก\textit{ค่าเฉลี่ยเคลื่อนที่ถ่วงน้ำหนักแบบชี้กำลัง}กับการคำนวณโมเมนตัม
และการปรับแก้ขนาดตามสมัยฝึก.
นั่นคือ
สำหรับสมัยฝึก $i$ 
และเกรเดียนต์ $\bm{g}$
การปรับค่าพารามิเตอร์สามารถทำได้ดังสมการ~\ref{eq: adam update}.
\begin{align}
\bm{v}^{(i+1)} &= \rho_1 \cdot \bm{v}^{(i)} + (1-\rho_1) \cdot \bm{g}
\label{eq: adam 1st moment} \\
\bm{r}^{(i+1)} &= \rho_2 \cdot \bm{r}^{(i)} + (1-\rho_2) \cdot \bm{g} \odot \bm{g}
\label{eq: adam 2nd moment} \\
\bm{\hat{v}} &= \frac{\bm{v}^{(i+1)}}{1-\rho_1^i}
\label{eq: adam bias correct 1st} \\
\bm{\hat{r}} &= \frac{\bm{r}^{(i+1)}}{1-\rho_2^i}
\label{eq: adam bias correct 2nd} \\
\bm{\theta}^{(i+1)} &=  
\bm{\theta}^{(i)} - \frac{\alpha}{\sqrt{\bm{\hat{r}} + \epsilon}} \odot \bm{\hat{v}}
\label{eq: adam update}
\end{align}
เมื่อ $\bm{\theta}^{(i)}$ 
คือพารามิเตอร์หลังปรับค่าในสมัยฝึก $i^{th}$.
ค่าเริ่มต้นของ $\bm{v}^{(0)}$ และ $\bm{r}^{(0)}$
อาจกำหนดเป็น $\bm{0}$.
อภิมานพารามิเตอร์ $\alpha$ คืออัตราเรียนรู้(ฐาน),
$\epsilon$ คือค่าคงที่ขนาดเล็ก (ซึ่งอาจใช้ $10^{-8}$),
ค่า $\rho_1 \in [0,1]$ และ $\rho_2 \in [0,1]$
โดย ค่าที่แนะนำคือ $\rho_1 = 0.9$ และ $\rho_2 = 0.999$.

\textit{อดัม} เป็นอีกขั้นตอนวิธีที่นิยมใช้กับแบบจำลองเชิงลึก
และพบว่าค่อนข้างทนทานต่อค่าอภิมานพารามิเตอร์ที่เลือก แต่อาจจะต้องปรับค่าอัตราเรียนรู้ $\alpha$ บ้างเท่านั้น.

ปัจจุบันยังไม่มีข้อสรุปถึงขั้นตอนวิธีที่ดีที่สุดโดยทั่วไป
แต่ขั้นตอนวิธีที่นิยมใช้คือ\cite{GoodfellowEtAl2016}
วิธีลงเกรเดียนต์,
วิธีลงเกรเดียนต์กับโมเมนตัม,
อาร์เอมเอสพรอป,
อาร์เอมเอสพรอปกับโมเมนตัม,
และอดัม.

\subsection{แบชนอร์มอไลเซชั่น}
\label{sec: batch norm}
\index{thai}{แบชนอร์ม}
\index{english}{batch norm}

\textbf{แบชนอร์มอไลเซชั่น} (batch normalization)
หรือเรียกสั้นว่า \textit{แบชนอร์ม} (batch norm)
จริง ๆ แแล้ว ไม่ใช่ขั้นตอนวิธีการหาค่าดีที่สุด 
แต่เป็นกลไกเพื่อช่วยให้การฝึกทำได้ง่ายขึ้น.

ไอโอฟีกับเซเจดี\cite{IoffeSzegedy2015a}
ตั้งข้อสังเกตว่า
ความยากของการฝึกโครงข่ายเชิงลึก 
ส่วนหนึ่งมาจากการเปลี่ยนแปลงอยู่ตลอดของการแจกแจงของอินพุตสำหรับแต่ชั้นคำนวณ
ซึ่งการเปลี่ยนแปลงนี้ เกิดจากการเปลี่ยนแปลงของค่าพารามิเตอร์ในชั้นคำนวณก่อนหน้า.
ดังนั้น การฝึกจึงทำได้ช้า 
เพราะไม่สามารถเลือกค่าอัตราเรียนรู้ที่สูงได้
และยังต้องระวังอย่างมากในการกำหนดค่าพารามิเตอร์เริ่มต้น
และยังสร้างปัญหาอย่างมากกับการใช้ฟังก์ชันกระตุ้นที่มีช่วงอิ่มตัว (เช่น ซิกมอยด์).
ไอโอฟีกับเซเจดี เรียก 
การเปลี่ยนแปลงของการแจกแจงของอินพุตสำหรับแต่ชั้นคำนวณ
จากการเปลี่ยนแปลงของค่าพารามิเตอร์ในชั้นคำนวณก่อนหน้า
ว่า \textit{การเลื่อนของความแปรปรวนร่วมเกี่ยวภายใน} (internal covariance shift)
\index{english}{internal covariance shift}
\index{thai}{การเลื่อนของความแปรปรวนร่วมเกี่ยวภายใน}
และเสนอกลไก \textit{แบชนอร์ม} เพื่อลด\textit{การเลื่อนของความแปรปรวนร่วมเกี่ยวภายใน}.

%สำหรับชั้นคำนวณ $L^{th}$ ที่มีอินพุตเป็น $\bm{Z}^{(L-1)}
หากกำหนดให้ $\bm{X} = [x_{ij}]$ เป็นอินพุตของชั้นคำนวณ
โดย $i = 1, \ldots, D;$ $j \in B$
และ $D$ เป็นจำนวนมิติ และ $B$ เป็นเซตของดัชนีจุดข้อมูลในหมู่เล็ก
แล้ว \textit{แบชนอร์ม} เสนอแปลงอินพุตของชั้นคำนวณนี้
ดังสมการ~\ref{eq: batch norm 1} และ~\ref{eq: batch norm 2}.
ค่าคงที่ขนาดเล็ก $\epsilon$ มีเพื่อรักษาเสถียรภาพของการคำนวณ (อาจกำหนดให้ $\epsilon = 10^{-8}$).

\begin{align}
x'_{ij} &= \frac{x_{ij} - \mu_i}{ \sqrt{\sigma_j^2 + \epsilon}}
\label{eq: batch norm 1} , \\
\hat{x}_{ij} &= \gamma_i \cdot x'_{ij} + \beta_i
\label{eq: batch norm 2}.
\end{align}

ค่า $\gamma_i$ กับ $\beta_i$ เป็นพารามิเตอร์ของแบชนอร์ม ที่เรียนรู้ระหว่างการฝึก.
ส่วน $\mu_i$ และ $\sigma_i^2$ 
คือค่าเฉลี่ยและความแปรปรวน
นั่นคือ ในระหว่างการฝึก 
$\mu_i = \frac{1}{|B|}\sum_j x_{ij}$
กับ $\sigma_i^2 = \frac{1}{|B|}\sum_j (x_{ij} - \mu_i)^2$.
สำหรับ การใช้งานหลังฝึกเสร็จ
ค่า $\mu_i$ และ $\sigma_i^2$ 
สามารถใช้ค่าที่ประมาณเตรียมไว้ระหว่างการฝึกได้.

ค่าประมาณ 
$\hat{\mu}_i$ และ 
$\hat{\sigma^2}_i$ 
นิยมประมาณด้วย\textit{ค่าเฉลี่ยเคลื่อนที่ถ่วงน้ำหนักแบบชี้กำลัง}
เช่น
$\hat{\mu}_i^{(new)} = \rho \cdot \mu_i
+ (1-\rho) \cdot \hat{\mu}_i^{(old)}$
และ
$\hat{\sigma^2}_i^{(new)} = \rho \cdot \sigma^2_i
+ (1-\rho) \cdot \hat{\sigma^2}_i^{(old)}$
เมื่อ ค่าเสื่อมน้ำหนัก $\rho$ เป็นอภิมานพารามิเตอร์ 
และ
$\mu_i$ กับ $\sigma_i^2$
เป็นค่าที่คำนวณจากหมู่เล็กที่กำลังฝึก.

\paragraph{การใช้งานแบชนอร์ม.}
%สำหรับชั้นคำนวณ $q^{th}$
%ที่ทำการคำนวณ $\bm{A}^{(q)} = \bm{W}^{(q)} \cdot \bm{Z}^{(q-1)} + \bm{b}^{(q)}$
%และ $\bm{Z}^{(q)} = h(\bm{A}^{(q)})$
%เมื่อ $\bm{Z}^{(q-1)}$ คืออินพุตของชั้น
%และ $\bm{W}^{(q)}$ กับ $\bm{b}^{(q)}$ คือพารามิเตอร์ของชั้น
%แล้ว แบชนอร์มสามารถดำเนินการได้หลายแนวทาง.
%แนวทางที่หนึ่ง การทำแบชนอร์มกับ
ชั้นคำนวณ โดยทั่วไป
ทำการคำนวณ $h(\bm{W}^{(q)} \cdot \bm{Z}^{(q-1)} + \bm{b}^{(q)})$
เมื่อ $h$ เป็นฟังก์ชันกระตุ้น
และ $\bm{W}^{(q)}$ กับ $\bm{b}^{(q)}$ คือพารามิเตอร์ของชั้น.
การทำแบชนอร์ม อาจทำกับค่า $\bm{Z}^{(q-1)}$
โดยตรง หรืออาจทำกับ $\bm{W}^{(q)} \cdot \bm{Z}^{(q-1)} + \bm{b}^{(q)}$ ก็ได้.
ไอโอฟีและเซเจดี\cite{IoffeSzegedy2015a}
แนะนำให้ทำกับตัวกระตุ้น $\bm{A}^{(q)} = \bm{W}^{(q)} \cdot \bm{Z}^{(q-1)} + \bm{b}^{(q)}$.
การทำแบชนอร์มกับตัวกระตุ้นของชั้น ช่วยปรับค่าตัวกระตุ้นให้อยู่ในย่านที่ฟังก์ชันกระตุ้นทำงานง่ายขึ้นด้วย.
นอกจากนั้น
เมื่อรวมผลลัพธ์จาก การคำนวณผลคูณค่าน้ำหนัก
กับสมการ~\ref{eq: batch norm 1} และ~\ref{eq: batch norm 2}
แล้วจะเห็นว่า พารามิเตอร์ $\gamma_i$ และ $\beta_i$
ช่วยให้อิสระและความสะดวกในการเลือกปรับค่าความแปรปรวนและค่าเฉลี่ยได้.
อีกเรื่องที่ควรกล่าวถึงคือ  เมื่อรวมการคำนวณแบชนอร์มเข้าไปด้วยแล้ว 
จะเห็นว่า ไบอัส $\bm{b}^{(q)}$ ซ้ำซ้อนและเกินความจำเป็น สามารถตัดออกได้.
%นั่นคือ ชั้นคำนวณที่มีการใช้แบชนอร์ม สามารถสรุปได้เป็น
%$h(\hat{\bm{A}})$ เมื่อ
%$\hat{\bm{A}} = 

กลไกของแบชนอร์ม ช่วยให้การฝึกของโครงข่ายประสาทเทียมทำได้ง่ายขึ้น
ไอโอฟีและเซเจดี\cite{IoffeSzegedy2015a} 
พบว่า
แบชนอร์ม อาจช่วยให้การฝึกทำได้เร็วขึ้นถึงสิบสี่เท่า
และช่วยให้การกำหนดค่าเริ่มต้นและการเลือกค่าอัตราเรียนรู้ทำได้ง่ายขึ้น
(สามารถเลือกค่าได้ช่วงกว้างขึ้น 
โดยที่ผลลัพธ์ไม่แย่ลงมาก เมื่อเปรียบเทียบกับการเลือกค่าที่ดี).

หมายเหตุ
การทำแบชนอร์ม ควรดำเนินการอย่างระมัดระวัง
เพื่อไม่ใช้สูญเสียสารสนเทศที่สำคัญไป (ดูแบบฝึกหัด~\ref{ex: deep batch norm} ประกอบ).
ดังเช่นที่ไอโอฟีและเซเจดี\cite{IoffeSzegedy2015a}
ได้แนะนำการประยุกต์ใช้กลไกของแบชนอร์ม
กับโครงข่ายโครงข่ายคอนโวลูชั่นไว้เฉพาะ.
โครงข่ายคอนโวลูชั่น
(บทที่~\ref{chapter: Convolution})
นิยมใช้กับงานคอมพิวเตอร์วิทัศน์
ซึ่งข้อมูลมีลักษณะเชิงโครงสร้างของพิกเซล.
นั่นคือ แต่ละจุดข้อมูลประกอบด้วยค่าพิกเซลหลาย ๆ ค่าที่จัดเรียงกันในโครงสร้าง โดยความสัมพันธ์ของค่าพิกเซลกับตำแหน่งในโครงสร้างมีสารสนเทศที่สำคัญอยู่.
หากกำหนดให้
อินพุต (ค่าการกระตุ้น) 
$\bm{X} = [x_{ijc}(n)] \in \mathbb{R}^{H \times W \times C \times N}$ 
เป็นเทนเซอร์\textit{ลำดับชั้น}สี่
%
%ที่มีความสัมพันธ์เชิงพื้นที่ของลำดับชั้นที่หนึ่งและสอง
แทนรูปภาพจำนวน $N$ รูป
แต่ละรูปขนาด $H \times W$ 
และมีช่องสี $C$ ช่อง (ภาพสเกลเทา $C=1$.
ภาพสี $C=3$.
ภาพหลายสเปกตรัม multi-spectral image หรือ multi-band image ซึ่งคือภาพถ่ายของฉากเหตุการณ์เดียวกัน แต่ใช้อุปกรณ์รับสัญญาณหลายตัว และแต่ละตัวทำงานกับช่วงความถี่สัญญาณคลื่นแม่เหล็กไฟฟ้าต่าง ๆ กัน $C > 1$.
ในกรณีทั่วไปจำนวนช่องสีอาจมองเป็นจำนวนลักษณะสำคัญ)
แล้วการทำแบชนอร์มอาจทำได้ดังสมการ~\ref{eq: batch norm conv 1} และ~\ref{eq: batch norm conv 2} สำหรับ $c = 1, \ldots, C$.

\begin{align}
x'_{ijc}(n) &= \frac{x_{ijc}(n) - \mu_c}{ \sqrt{\sigma_c^2 + \epsilon}}
\label{eq: batch norm conv 1} , \\
\hat{x}_{ijc}(n) &= \gamma_c \cdot x'_{ijc}(n) + \beta_c
\label{eq: batch norm conv 2}
\end{align}
เมื่อ $\mu_c = \frac{1}{|B| \cdot H \cdot W} \sum_{n \in B} \sum_i \sum_j x_{ijc}(n)$
และ
$\sigma_c^2 = \frac{1}{|B| \cdot H \cdot W} \sum_{n \in B} \sum_i \sum_j (x_{ijc}(n) - \mu_c)^2$.
สังเกต แบชนอร์มสำหรับชั้นคำนวณคอนโวลูชั่น
จะใช้พารามิเตอร์ $\gamma_c$ และ $\beta_c$ หนึ่งคู่ต่อหนึ่งช่องสี% (หรือเรียกว่า หนึ่ง\textit{แผนที่ลักษณะสำคัญ})
.
\index{english}{batch norm!convoltion neural network}
\index{thai}{แบชนอร์ม!โครงข่ายคอนโวลูชั่น}

\subsection{กลไกช่วยการฝึกอื่น ๆ}
\label{sec: deep adv train}

กลไก\textbf{การฝึกก่อน} (pre-training)
\index{thai}{การฝึกก่อน}
\index{english}{pre-training}
ที่ใช้การฝึกแบบจำลองกับปัญหาที่ง่ายขึ้น หรือปัญหาที่ใกล้เคียง ก่อนจะนำค่าน้ำหนักที่ได้จากการฝึก(เบื้องต้น)
มาฝึกต่อ (หรือบางครั้งนิยมอ้างถึงว่าเป็น \textit{การปรับละเอียด} fine tuning) กับปัญหาที่ต้องการจริง ๆ ที่มักเรียกว่า ปัญหาเป้าหมาย.
\index{english}{fine tuning}
\index{thai}{การปรับละเอียด} 

\textit{การฝึกก่อน} อาจทำโดยใช้แบบจำลองแบบเดียวกับแบบจำลองสุดท้ายที่ต้องการ
แต่ฝึกกับข้อมูลอีกชุด หรือเป้าหมายอีกแบบ
หรือ อาจจะฝึกแบบจำลองที่เล็กกว่า
แล้วค่อยเพิ่มขยายเป็นแบบจำลองที่ต้องการ เมื่อนำมาใช้กับภารกิจเป้าหมาย
เช่น ในงานคอมพิวเตอร์วิทัศน์
การทำแบบจำลองอาจจะเลือกแบบจำลองที่นิยมอยู่แล้ว
เช่น \textit{อเล็กซ์เน็ต} (หัวข้อ~\ref{sec: AlexNet})
พร้อม%
%และนอกจากจะใช้โครงสร้างที่นิยมแล้ว
การเริ่มต้นด้วยค่าน้ำหนักของ\textit{อเล็กซ์เน็ต}ที่ผ่านการฝึกมาแล้ว
แทนที่จะเริ่มจากค่าน้ำหนักสุ่ม.
แล้วจึงค่อยดำเนินการฝึกต่อกับข้อมูลและฟังก์ชันสูญเสียของภารกิจเป้าหมาย.
บางครั้ง เพื่อให้แบบจำลองที่นำมา เหมาะกับภารกิจเป้าหมาย
อาจมีการปรับแต่งแบบจำลองบ้าง ได้แก่ เปลี่ยนชั้นคำนวณท้าย ๆ เช่น เปลี่ยนชั้นสุดท้ายให้มีจำนวนเอาต์พุตสุดท้ายตามที่ต้องการ.

การนำค่าน้ำหนักที่ฝึกแล้วมาใช้ในการฝึกต่อ
หากมีข้อมูลของภารกิจเป้าหมายมีปริมาณไม่มาก
การดำเนินการ นิยมตรึงค่าน้ำหนักที่ฝึกมาก่อนไว้ (ไม่มีการปรับค่าน้ำหนักเหล่านี้)
แต่ปรับค่าน้ำหนักเฉพาะกับชั้นคำนวณหลัง ๆ ซึ่งเชื่อว่าเกี่ยวข้องกับภารกิจเป้าหมายมากกว่า.
แต่หากมีข้อมูลของภารกิจเป้าหมายมีปริมาณมาก
อาจลดจำนวนชั้นคำนวณต้น ๆ ที่ตรึงค่าน้ำหนักที่ฝึกก่อนลง และเพิ่มจำนวนชั้นคำนวณหลัง ๆ ที่ปรับค่าน้ำหนักให้มากขึ้น
หรือ อาจจะเพียงใช้ค่าน้ำหนักที่ฝึกก่อนมาแทนค่าน้ำหนักเริ่มต้น แล้วฝึกค่าน้ำหนักทั้งหมดในโครงข่ายเลย.
รูป~\ref{fig: pre-training} แสดงแนวทางที่นิยมดำเนินการกับค่าน้ำหนักจาก\textit{การฝึกก่อน}.

%
\begin{figure}
	\begin{center}
%		\begin{tabular}{cc}
		\includegraphics[width=0.5\textwidth]{05Deep/pretrain1.png}	
%		\end{tabular}
		\caption[แนวทางที่นิยมดำเนินการกับค่าน้ำหนักจากการฝึกก่อน]{
			แนวทางที่นิยมดำเนินการกับค่าน้ำหนักจาก\textit{การฝึกก่อน}.
			ภาพบนสุด แสดงกรณีที่ข้อมูลเป้าหมายค่อนข้างน้อย วิธีที่นิยม คือ กำหนดค่าน้ำหนักของชั้นคำนวณต้น ๆ ด้วยค่าน้ำหนักจากการฝึกก่อน และตรึงค่าเหล่านี้ไว้ และดำเนินการฝึกโครงข่ายด้วยการปรับค่าน้ำหนักเฉพาะชั้นคำนวณหลัง ๆ.
		    ภาพกลาง กรณีที่มีข้อมูลเป้าหมายมีมากพอสมควร อาจลดจำนวนชั้นที่ตรึงค่าน้ำหนักลง
		    และฝึกจำนวนชั้นคำนวณมากขึ้น.
		    ภาพล่างสุด แสดงกรณีที่ใช้ค่าน้ำหนักจาก\textit{การฝึกก่อน} เป็นเพียงค่าน้ำหนักเริ่มต้น และทำการฝึกทั้งโครงข่ายใหม่. การเริ่มต้นฝึกจากค่าน้ำหนักที่ได้จาก\textit{การฝึกก่อน} จะช่วยให้การฝึกต่อทำได้ง่ายและเร็วขึ้น.
}
		\label{fig: pre-training}
	\end{center}
\end{figure}
%



ค่าน้ำหนักของ\textit{การฝึกก่อน}
อาจได้มาโดย\textit{การเรียนรู้แบบมีผู้สอน} ดังที่ได้อภิปรายไป
หรืออาจได้มาโดย\textit{การเรียนรู้แบบไม่มีผู้สอน} (unsupervised pre-training 
ดูเอรานและคณะ\cite{ErhanEtAl2010a} เพิ่มเติม).
%ซึ่งอาจทำการฝึกค่าน้ำหนักทีละชั้นคำนวณ.
% 
การฝึกก่อน จะช่วยทั้งในแง่ของลดเวลาในฝึกลง
เพิ่มคุณภาพ รวมถึงคุณสมบัติ\textit{ความทั่วไป}
และยังมองได้ว่า เป็นความสามารถใน\textbf{การถ่ายโอนการเรียนรู้} (transfer learning\cite{zhuang2019comprehensive, LinJung2017, PanYang2010}) อีกด้วย.
\index{english}{transfer learning}
\index{thai}{การถ่ายโอนการเรียนรู้}
\textit{การถ่ายโอนการเรียนรู้} 
อ้างถึง สถานการณ์ที่เราสามารถใช้ประโยชน์
จากการเรียนรู้ในภารกิจหนึ่ง
เพื่อช่วยการเรียนรู้ในอีกภารกิจได้
โดย การเรียนรู้ในภารกิจใหม่
ที่ได้รับ\textit{การถ่ายโอนการเรียนรู้}มา
จะทำได้ดีหรือเร็วกว่า 
การเรียนรู้ในภารกิจใหม่ ที่ไม่มี\textit{การถ่ายโอนการเรียนรู้}
และต้องเริ่มเรียนทุกอย่างจากศูนย์.
%หัวข้อ~\ref{sec: pre-train} อภิปรายการฝึกก่อนเพิ่มเติมในรายละเอียด.
ความสามารถใน\textit{การถ่ายโอนการเรียนรู้} ในโครงข่ายลึก
เป็นอีกปัจจัยที่ช่วยให้การประยุกต์ใช้การเรียนรู้เชิงลึกทำได้ง่ายขึ้น กว้างขวางขึ้น และมีส่วนอย่างมากที่ช่วยเร่งการพัฒนาของศาสตร์อย่างมาก.

แนวทางหรือกลไก 
ที่อาจมองว่าคล้ายการฝึกก่อน
เช่น 
\textit{ฟิตเน็ต} (FitNets\cite{RomeroEtAl2015})
ที่ใช้แบบจำลองครู (teacher model) 
กับแบบจำลองนักเรียน (student model)
โดยแบบจำลองครูเป็นแบบจำลองที่ตื้นแต่กว้าง 
(นั่นคือ มีจำนวนชั้นคำนวณน้อย แต่ว่าแต่ละชั้นมีจำนวนหน่วยคำนวณมาก) ซึ่งฝึกได้ง่ายกว่า. 
ส่วนแบบจำลองนักเรียนจะลึกแต่แคบ
ทำให้มีประสิทธิภาพในการคำนวณมากกว่า
แต่ฝึกยากกว่า.
กลไกการฝึกแบบครูนักเรียนนี้
คือ ในการฝึกแบบจำลองนักเรียน
นอกจากจะฝึกแบบจำลองนักเรียนสำหรับจุดประสงค์หลัก
แล้วยังฝึกให้แบบจำลองนักเรียน โดยเฉพาะในชั้นคำนวณต้นๆ ทำนายค่าผลการกระตุ้นของชั้นคำนวณซ่อนในแบบจำลองครูด้วย.
การทำดังนี้ คือการใช้ค่าผลการกระตุ้นของชั้นคำนวณซ่อนในแบบจำลองครู
เป็นเสมือนตัวช่วยนำทาง สำหรับการฝึกชั้นคำนวณซ่อนของแบบจำลองนักเรียน.

\textit{การเรียนหลักสูตร} (curriculum learning\cite{Bengio2009b}) เป็นอีกแนวทางหนึ่งของกลไกฝึกระดับสูง.
\index{english}{curriculum learning}
\index{thai}{การเรียนหลักสูตร}
กล่าวโดยทั่วไป
\textit{การเรียนหลักสูตร} จะจัดการฝึกเป็นหลาย ๆ ยก 
โดยเริ่มจากยกแรก ๆ ที่ทำการฝึกที่ง่าย แล้วเพิิ่มความยากในการฝึกขึ้นในแต่ละยก
จนสุดท้าย คือการฝึกกับปัญหาที่ต้องการ.
เบนจิโอ\cite{Bengio2009b}
ทดลองเปรียบเทียบ
\textit{การเรียนหลักสูตร}
ซึ่งทำการฝึกข้อมูลที่ง่ายก่อน ที่จะฝึกข้อมูลที่ยาก
กับการฝึกปกติ ที่ใช้ข้อมูลที่ยาก ที่เป็นเป้าหมายตั้งแต่แรก
ผลที่ได้พบว่า แบบจำลองสามารถเรียนรู้ได้ดีขึ้นอย่างชัดเจน.

นอกจากขั้นตอนวิธีและกลไกต่าง ๆ ที่ช่วยการฝึกแล้ว
แนวทางที่ประสบความสำเร็จอย่างมากเลย 
คือ การออกแบบโครงสร้างแบบจำลอง เพื่อช่วยให้การฝึกทำได้ง่ายขึ้น.
จริง ๆ แล้ว การเปลี่ยนฟังก์ชันกระตุ้น ก็เป็นการเปลี่ยนโครงสร้างของแบบจำลอง
เพื่อช่วยให้การฝึกทำได้ง่ายขึ้น.
กูดเฟโลและคณะ\cite{GoodfellowEtAl2016}
ตั้งข้อสังเกตว่า
โครงสร้างที่มีลักษณะเชิงเส้นมากขึ้น จะช่วยให้การฝึกทำได้ง่ายขึ้น.
กลไกของแบชนอร์ม ก็มีลักษณะเป็นการเปลี่ยนโครงสร้างของแบบจำลอง.
แบบจำลองหลายชนิด ถูกออกแบบมาให้มีเส้นทางการเชื่อมต่อระหว่างชั้นคำนวณ โดยอาจมีการเชื่อมต่อข้ามชั้นคำนวณได้
เพื่อช่วยในการฝึก
เช่น \textit{เรสเน็ต} ResNet\cite{He_2016_CVPR})
ที่มีการเชื่อมต่อข้ามชั้นคำนวณ
เพื่อช่วยให้การแพร่กระจายของเกรเดียนต์กลับไปหาชั้นคำนวณต้น ๆ ทำได้มีประสิทธิภาพขึ้น.
โครงข่ายคอนโวลูชั่น (บทที่~\ref{chapter: Convolution})
ก็เป็นลักษณะของการเปลี่ยนโครงสร้าง ซึ่งโครงสร้างของโครงข่ายคอนโวลูชั่น
ช่วยลดจำนวนพารามิเตอร์ที่ต้องการลง โดยอาศัยคุณสมบัติที่เหมาะกับข้อมูลที่มีลักษณะเชิงท้องถิ่น เช่น ภาพ.
การลดจำนวนพารามิเตอร์ที่ไม่จำเป็นลง ช่วยโดยตรงต่อกระบวนการฝึก.
แบบจำลองความจำระยะสั้นที่ยาว (บทที่~\ref{chapter: RNN})
ก็ถูกออกแบบมา
สำหรับข้อมูลเชิงลำดับ เพื่อช่วยให้การฝึก เรียนรู้ ความสัมพันธ์เชิงลำดับระยะยาวทำได้มีประสิทธิภาพมากขึ้น.



%\section{Glossary}
\section{อภิธานศัพท์}

\begin{description}
	
	\item[การเรียนรู้เชิงลึก  (deep learning):]
การเรียนรู้ของเครื่องที่ใช้โครงข่ายประสาทเทียมจำนวนชั้นคำนวณมาก รวมไปจนถึงเทคนิคและกลไกอื่นๆที่เกี่ยวข้อง
\index{english}{deep learning}
\index{thai}{การเรียนรู้เชิงลึก}

	\item[ปัญหาการเลือนหายของเกรเดียนต์ (vanishing gradient problem):]
ปัญหา หรือปรากฏการณ์ ที่ขนาดเฉลี่ยของเกรเดียนต์ลดลงอย่างมากที่ชั้นคำนวณต้น ๆ เมื่อเปรียบเทียบกับค่าเฉลี่ยชั้นคำนวณปลาย ๆ
\index{thai}{ปัญหาการเลือนหายของเกรเดียนต์} 
\index{english}{vanishing gradient problem}

	\item[ฟังก์ชันกระตุ้นเรคติไฟด์ลิเนียร์ (rectified linear function) หรือ เรลู  (relu):]
ฟังก์ชันกระตุ้น $\mathrm{relu}(a) = \max(a, 0)$
\index{thai}{ฟังก์ชันกระตุ้น!เรคติไฟด์ลิเนียร์}
\index{thai}{ฟังก์ชันกระตุ้น!เรลู}
\index{english}{relu}
\index{english}{rectified linear}

	\item[หมู่เล็ก (minibatch):]
ส่วนของข้อมูลที่ถูกแบ่งเป็นกลุ่มเล็ก ๆ สำหรับการฝึก โดยในหนึ่งสมัยฝึก จะต้องทำการปรับค่าน้ำหนักหลายครั้ง แต่ละครั้งสำหรับแต่ละหมู่เล็ก 
และการปรับแต่ละครั้ง คำนวณจากจุดข้อมูลต่างๆในหมู่เล็ก
และจะใช้ค่าเฉลี่ยของเกรเดียนต์ภายในหมู่ในการปรับค่าน้ำหนัก
เปรียบเทียบกับการฝึกแบบออน์ไลน์ ที่การปรับค่าน้ำหนักแต่ละครั้ง คำนวณจากหนึ่งจุดข้อมูล
และเปรียบเทียบกับการฝึกแบบออฟไลน์ ที่การปรับค่าน้ำหนัก
คำนวณจากข้อมูลทั้งหมดทีเดียว และปรับค่าแค่ครั้งเดียว ต่อสมัยฝึก
\index{english}{minibatch}
\index{thai}{หมู่เล็ก}

	\item[การตกออก (drop out):]
กลไกการทำเรกูลาไรซ์สำหรับโครงข่ายประสาทเทียม โดยการสุ่มปิดผลการกระตุ้น ของหน่วยคำนวณย่อยต่าง ๆ
\index{english}{drop out}
\index{thai}{การตกออก}

	\item[แบชนอร์ม (batch norm) หรือ แบชนอร์มอไลเซชั่น  (batch normalization):]
กลไก เพื่อช่วยการฝึกโครงข่ายประสาทเทียม โดยปรับค่าเฉลี่ยและความแปรปรวนของตัวกระตุ้นในชั้นคำนวณ
\index{thai}{แบชนอร์ม}
\index{english}{batch norm}

	
\end{description}







%\paragraph{1.} 
%จากบริบทของ\textit{ศาสตร์การหาค่าดีที่สุด} 
%จงอภิปรายเรื่องที่ช่วงพลวัตรแคบของค่าอนุพันธ์ของฟังก์ชันซิกมอยด์ 
%ทำให้การฝึกโครงข่ายประสาทเทียมที่ใช้ฟังก์ชันกระตุ้นเป็นซิกมอยด์ทำได้ยาก

%\paragraph{2.} 
%... ปัญหาการเลือนหายของเกรเดียนต์
%
%\paragraph{3.} 
%... experiment various kinds of activation functions

%
%
%\paragraph{5.}
%จากปัญหาการคำนวณเชิงเลข เมื่อโปรแกรมฟังก์ชัน \texttt{softmax}
%ยกตัวอย่างเปรียบเทียบการคำนวณด้วยสมการต้นฉบับ กับการคำนวณด้วยสมการ~\ref{eq: deep softmax stable}
%สำหรับทั้งสองกรณี. 
%นั่นคือ กรณี $a_q$ เป็นค่าลบที่ใหญ่มากๆ ทุก ๆ ดัชนี $q$
%และกรณี $a_q$ อย่างน้อยหนึ่งตัวที่มีค่าบวกที่ใหญ่มาก ๆ.


