%\chapter{ตัวอย่างการประยุกต์การเรียนรู้ของเครื่องในงานการรู้จำรูปแบบ}
\chapter{การเรียนรู้ของเครื่องในโลกกว้าง}
\label{chap: pattern recognition case studies}

%{\selectlanguage{english}
%``Failure is the key to success; each mistake teaches us something.''\\
%---Morihei Ueshiba
%
%}
%
%{\selectlanguage{thai}
%\strut\hfill ``ล้มเหลวเป็นกุญแจสู่สำเร็จ พลั้งแต่ละผิดสอนเราแต่ละอย่าง'' \\
%\strut\hfill ---โมเรเฮอิ อุเรชิบะ
%}

\begin{Parallel}[c]{0.45\textwidth}{0.45\textwidth}
	\selectlanguage{english}
	\ParallelLText{
		``Failure is the key to success; \\
		each mistake teaches us something.''
		\begin{flushright}
			---Morihei Ueshiba
		\end{flushright}
	}
	\selectlanguage{thai}
	\ParallelRText{
		``ความล้มเหลวเป็นกุญแจสู่ความสำเร็จ \\
		แต่ละความผิดพลาดสอนเราบางอย่าง.''
		\begin{flushright}
			---โมเรเฮอิ อุเรชิบะ
		\end{flushright}
	}
\end{Parallel}
\index{english}{words of wisdom!Morihei Ueshiba}
\index{english}{quote!failure}
\vspace{1cm}


%\begin{Parallel}[c]{0.54\textwidth}{0.46\textwidth}
%	\selectlanguage{english}
%	\ParallelLText{
%		``Failure is the key to success; \\
%		each mistake teaches us something.''\\
%		---Morihei Ueshiba
%	}
%	\selectlanguage{thai}
%	\ParallelRText{
%		``ความล้มเหลวเป็นกุญแจสู่ความสำเร็จ \\
%%		พลั้งแต่ละผิดสอนเราแต่ละอย่าง'' \\
%		แต่ละความผิดพลาดสอนเราบางอย่าง''\\
%		---โมเรเฮอิ อุเรชิบะ
%	}
%\end{Parallel}
%\index{words of wisdom!Morihei Ueshiba}
%\index{quote!failure is a key to success}

\vspace{1cm}

บทนี้
นำเสนอตัวอย่างการนำวิธี\textit{การเรียนรู้ของเครื่อง} ไปประยุกต์ใช้กับงาน\textit{การรู้จำรูปแบบ}
รวมถึง 
วิธี\textit{การเรียนรู้ของเครื่อง}แบบต่าง ๆ ที่หลากหลาย.
หัวข้อ~\ref{sec: customer behavior analytics} นำเสนอตัวอย่างระบบวิเคราะห์พฤติกรรมลูกค้า
ที่ใช้แบบจำลองการเรียนรู้ของเครื่องประกอบอยู่ในระบบ ทั้งในส่วนการตรวจจับตำแหน่งลูกค้า และการแสดงผล.
หัวข้อ~\ref{sec: customer behavior analytics}
อภิปราย เทคนิคหลาย ๆ อย่างที่ใช้ประกอบกับวิธีการเรียนรู้ของเครื่อง 
เพื่อสามารถใช้กับงานการรู้จำรูปแบบได้อย่างมีประสิทธิผล.
หัวข้อ~\ref{sec: SVM}
อภิปราย ซัพพอร์ตเวกเตอร์แมชชีน ซึ่งเป็นหนึ่งในวิธีการเรียนรู้ของเครื่องอีกที่ได้รับความนิยมอย่างมาก.
ซัพพอร์ตเวกเตอร์แมชชีน ออกแบบและพัฒนาโดยอาศัยศาสตร์การหาค่าดีที่สุด
มีทฤษฎีรองรับมั่นคง และมีเบื้องหลังการออกแบบที่สละสลวย.
%ซัพพอร์ตเวกเตอร์แมชชีน มีรายละเอียด 
%และเนื้อหาที่เกี่ยวข้องมาก
%และหัวข้อ~\ref{sec: SVM} อภิปรายเนื้อหาของซัพพอร์ตเวกเตอร์แมชชีน.

\section{การวิเคราะห์พฤติกรรมลูกค้า}
\label{sec: customer behavior analytics}
\index{english}{customer behavior analytic project}
\index{thai}{โครงการวิเคราะห์พฤติกรรมลูกค้า}

เนื้อหาในหัวข้อนี้ ได้รับอิทธิพลหลัก ๆ จากโครงการวิจัยการวิเคราะหพฤติกรรมลูกคาผานขอมูลวิดีโอจากกลองวงจรปด\cite{KatanyukulPonsawat2017a}.

\subsection{การวิเคราะห์พฤติกรรมลูกค้าจากภาพวิดีโอ}

ขอมูลวิดีโอจากกลองวงจรปดในรานคาปลีก 
นอกจากใชเพื่อเหตุผลดานความปลอดภัยแลว 
ขอมูลวิดีโอยังสามารถนำมาใช
เพื่อการวิเคราะหพฤติกรรมการจับจายของลูกคาที่เขามาภายในรานได. 
การเขาใจพฤติกรรมของลูกคาสามารถนำมาชวยเพิ่มโอกาสทางธุรกิจได 
เชน ความเขาใจบริเวณและเวลาที่ลูกคาใชขณะเขามาภายในราน 
สามารถใช้ประกอบการตัดสินใจ
สำหรับการจัดวางสินคา
หรือการจัดกิจกรรมสงเสริมการตลาดได้.

ระบบวิเคราะหพฤติกรรมอัตโนมัติจะชวยอำนวยความสะดวกในการวิเคราะหเบื้องตน ถึงพฤติกรรมของลูกคาที่เข้ามาในร้าน โดยใช้ขอมูลวิดีโอที่ถายจากกลองวงจรปด.
ระบบตัวอย่างนี้
%\textit{การวิเคราะหพฤติกรรมลูกคาผานขอมูลวิดีโอจากกลองวงจรปด}
วิเคราะหพฤติกรรม ซึ่งคือ การตรวจหาตำแหน่งของลูกค้า เมื่อลูกค้าเข้ามาใช้บริการภายในร้านค้า
และนำเสนอผลสรุป เป็นแผ่นภาพสรุป.
แผ่นภาพสรุปที่ได้ สามารถนำไปช่วยประกอบการตัดสินใจด้านการตลาด
และอาจช่วยให้เข้าใจรูปแบบการเข้าใช้พื้นที่ในบริเวณร้านได้ดีขึ้น.

ขอมูลจากกลองวงจรปดภายในรานคาปลีก 
ถูกนำมาใช้เพื่อประมวลผล และตรวจหาตำแหนงของลูกคา
แล้วจัดทำแผนภาพสรุป. 
รูป~\ref{fig: classic heatmap example} แสดงตัวอยาง ของรูปแบบภาพสรุป ที่จัดทำเป็นรูปแบบ\textit{แผนที่ความรอน} (heat map). 
ภาพสรุปแสดงบริเวณที่พบลูกคาบอยที่สุดดวยสีแดง
และสีโทนที่เย็นลงสื่อถึงความถี่ที่พบลูกคาลดลง (ความถี่ต่ำที่สุด แทนดวยสีที่เย็นที่สุด คือ สีน้ำเงิน). 

%
\begin{figure}
\begin{center}
\includegraphics[width=3in]{04Classic/heatmap_example.png}
\caption[แผนภาพความร้อน]{ตัวอย่างภาพสรุป ซึ่งเป็นภาพซ้อนของภาพถ่ายแสดงผังบริเวณร้านจากกล้องวงจรปิด และซ้อนทับด้วยแผนภาพความร้อนที่มีลักษณะกึ่งโปร่งใส.
แผนภาพความร้อนแสดงบริเวณที่พบลูกคาบอยที่สุดดวยสีแดง
และสีโทนที่เย็นลงสื่อถึงความถี่ที่พบลูกคาลดลง 
และความถี่ต่ำที่สุดแทนดวยสีที่เย็นที่สุดคือสีน้ำเงิน
(ภาพนี้เป็นภาพสี และในภาพได้ทำการปิดบังตราสัญญลักษณ์ของร้านค้าไว้)
}
\label{fig: classic heatmap example}
\end{center}
\end{figure}
%

\subsection{ขั้นตอนการทำงานของระบบวิเคราะห์พฤติกรรมลูกค้าจากภาพวิดีโอ}

ตัวอย่างระบบวิเคราะหพฤติกรรมอัตโนมัตินี้ มีส่วนประกอบหลักสองส่วน ได้แก่
(1) ส่วนการตรวจหาและระบุตำแหนงของลูกคาจากขอมูลวิดีโอ
และ (2) ส่วนนำเสนอผล ที่นำตำแหนงของลูกคาที่ตรวจหาไดมาสรุปและแสดงผลเปน\textit{แผนที่ความรอน}.

\subsection{ส่วนของการตรวจหาและระบุตำแหน่งของลูกค้า จากข้อมูลวิดีโอ}
\label{sec: classic task 1}

ลักษณะงานวิเคราะห์พฤติกรรมลูกค้า เป็นงานลักษณะ\textit{ออฟไลน์} (offline mode) 
หรือ\textit{ลักษณะการประมวลผลเป็นชุด} (batch processing ที่ไม่ใช่\textit{ระบบเวลาจริง} real-time processing).
เช่นเดียวกับงานของ\textit{เบเนนสันและคณะ}\cite{BenensonEtAl2014a} ที่ศึกษาการตรวจหาและระบุตำแหน่งคนเดินเท้า
ซึ่งเป็นงานในลักษณะใกล้เคียงกัน
งานการตรวจหาและระบุตำแหน่งลูกค้านี้ ดำเนินการโดย
การแปลงจากวิดีโอมาเป็นชุดลำดับของภาพ แล้วจึงใช้วิธีการประมวลผลภาพ เพื่อตรวจหาและระบุตำแหน่งของลูกค้า. 

การระบุตำแหน่งลูกค้าอาจทำได้หลายวิธี
เช่น 
(ก) แนวทางวิธีลบฉากพื้นหลัง
และ (ข) แนวทางวิธีตรวจหาภาพวัตถุ.

\subsubsection{แนวทางวิธีลบฉากพื้นหลัง}

แนวทางวิธีลบฉากพื้นหลัง ไม่ได้ใช้เทคนิคใดจากศาสตร์การเรียนรู้ของเครื่อง
แต่ใช้อาศัยเทคนิคการประมวลผลภาพ และอาศัยลักษณะเฉพาะของข้อมูล.
%
จากการสังเกตลักษณะเฉพาะของข้อมูล
พื้นหลังของภาพวิดีโอค่อนข้างคงที่
ส่วนใหญ่ สิ่งที่เคลื่อนที่ในภาพมักจะเป็นคน ที่รวมทั้งลูกค้าและพนักงาน.
แนวทางวิธีลบฉากพื้นหลัง อาศัยลักษณะเฉพาะของข้อมูลนี้เข้ามาช่วย.

แนวทางวิธีลบฉากพื้นหลัง (Background Subtraction) 
แสดงดังรูปที่~\ref{fig: Background Subtraction Pipeline} ซึ่งเป็นแผนภูมิลำดับกระบวนการ. 
%
ข้อมูลวิดีโอ (ภาพซ้ายล่าง) จะถูกแปลงเป็นเฟรมภาพ (ขั้นตอน 1.1).
หลังจากนั้น นำแต่ละภาพที่ได้ไปลบออกจาก\textit{ภาพพื้นหลัง} (background หรือภาพในบริเวณร้านที่ไม่มีลูกค้าอยู่) รวมถึงการปรับปรุงเพื่อขยายความต่างให้ชัดเจนขึ้น (ขั้นตอนที่ 1.2a – 1.2c).
สุดท้าย ทำการค้นหาและระบุตำแหน่งลูกค้าในภาพ (ขั้นตอนที่ 1.3a – 1.3b). 

ขั้นตอนที่ 1.1. การแปลงจากวิดีโอไฟล์เป็นภาพ มีกระบวนทางเทคนิคที่มีรายละเอียดมาก
รวมไปถึงข้อกำหนดต่าง ๆ ตามมาตราฐานของชนิดข้อมูลวิดีโอ 
แต่ในทางปฏิบัติมีเครื่องมือที่ช่วยทำงานเหล่านี้ได้มากมาย 
หนึ่งในเครื่องมือที่นิยมใช้ ก็เช่น \textit{โอเพ่นซีวี} (OpenCV\footnote{%
http://opencv.org/})  ซึ่งเป็น\textit{ไลบรารี่รหัสเปิด} (open-source library) สำหรับงานด้าน\textit{ทัศนคอมพิวเตอร์} (Computer Vision).
ขั้นตอนที่ 1.2 การปรับปรุงภาพ เพื่อให้ตำแหน่งของลูกค้าเด่นชัดขึ้น 
เพื่อช่วยให้การตรวจหาตำแหน่งของลูกค้าในขั้นตอนต่อไปทำได้ง่าย. 
ขั้นตอนย่อยในการปรับปรุงภาพนี้ ได้แก่ 
\textit{เทคนิควิธีลบพื้นหลัง} ที่นำภาพพื้นหลังไปลบออกจากภาพที่ต้องการตรวจหาตำแหน่งลูกค้า. 
สิ่งที่ต่างจากพื้นหลังก็จะปรากฏเด่นขึ้น (1.2a และ 1.2b)
และ \textit{เทคนิควิธีแยกส่วนโดยการกำหนดระดับค่าขีดแบ่ง} (segmentation by thresholding) ก็สามารถนำมาใช้
เพื่อแยกฉากหน้า (ที่อาจเป็นลูกค้า) ออกจากฉากหลัง. 
\textit{เทคนิควิธีแยกส่วนโดยการกำหนดระดับค่าขีดแบ่ง} 
คือการตั้งค่าระดับขีดแบ่งขึ้นมาหนึ่งค่า 
โดย หากค่าความเข้มของพิกเซลมากกว่าระดับนั้น 
ก็จะปรับเพิ่มค่าความเข้มของพิกเซลนั้นไปจนมีค่ามากที่สุด (สว่างเต็มที่) 
ไม่เช่นนั้นก็ปรับลดค่าลงเป็นศูนย์ (มืดเต็มที่). 
ดังนั้น ภาพจะถูกแยกเป็นส่วนสว่างและมืดอย่างชัดเจน.  
ขั้นตอน 1.3 เป็นการนำภาพที่ปรับปรุงความต่างระหว่างฉากหน้าและฉากหลัง ไปตรวจหาตำแหน่งของลูกค้า 
ซึ่งใช้\textit{เทคนิคการตรวจหาแบบหน้าต่างเลื่อน} (sliding window detection). 
\textit{เทคนิคการตรวจหาแบบหน้าต่างเลื่อน} เป็นหนึ่งในวิธีที่นิยมใช้สำหรับงานการตรวจหาภาพวัตถุ. 
\textit{เทคนิคการตรวจหาแบบหน้าต่างเลื่อน} 
\index{english}{sliding window}
\index{thai}{วิธีหน้าต่างเลื่อน}
เป็นเทคนิคการเลือกส่วนภาพ 
โดยส่วนภาพ(ในขนาดที่พอเหมาะแก่การตรวจสอบว่าเป็นวัตถุที่ต้องการหรือไม่) 
จะถูกเลือกขึ้นมาจากภาพที่สงสัย.
การเลือกจะเริ่มที่มุมด้านหนึ่งของภาพ และขยับเลือกไปเรื่อย ๆ จนครอบคลุมทั้งภาพ.
%ในลักษณะคล้ายกับการเลื่อนหน้าต่างกวาดไปดูส่วนต่างๆของภาพใหญ่ทีละส่วนๆ. 
%ดังนั้นเทคนิคนี้ จึงถูกเรียกว่าการตรวจหาแบบหน้าต่างเลื่อน.
%
ส่วนภาพขนาดเล็กที่ถูกเลือกออกมานั้นจะถูกตรวจสอบว่ามีวัตถุที่ค้นหาอยู่ในส่วนภาพนั้นหรือไม่
(ขั้นตอน 1.3a). 
เมื่อได้ผลการตรวจสอบส่วนภาพขนาดเล็กจากตำแหน่งต่าง ๆ แล้ว 
%(positive patches ส่วนภาพนั้นๆถูกจำแนกว่าเป็นส่วนภาพของวัตถุที่ค้นหา ซึ่งในที่นี้ก็คือ ลูกค้า)
ส่วนภาพต่าง ๆ ที่ได้ผลการตรวจเป็นบวกที่มีตำแหน่งใกล้ ๆ กัน จะถูกรวมกันตามเกณฑ์การรวมที่กำหนด 
และตำแหน่งของการรวมนั้นจะถูกบันทึกเป็นตำแหน่งของลูกค้าทั้งตัว (ขั้นตอน 1.3b).
เมื่อถึงขั้นตอนนี้ 
ระบบก็จะสามารถระบุได้แล้วว่า ลูกค้าอยู่ที่ตำแหน่งใดในภาพ. 

\begin{figure}
\centering
\includegraphics[width=5.8in]{04Classic/propTask1Diagram.png}
\caption[วิธีการลบฉากหลัง]{แผนภาพแสดงขั้นตอนต่าง ๆ ในการตรวจหาตำแหน่งลูกค้าจากข้อมูลวีดิโอด้วยแนววิธีการลบฉากหลัง}
\label{fig: Background Subtraction Pipeline}
\end{figure}

\subsubsection{แนวทางของวิธีตรวจหาวัตถุ}
\label{sec: classic object detection}

กลไกจริง ๆ ของวิธีการลบฉากหลัง คือ การตรวจหาตำแหน่งของวัตถุที่ต่างจากฉากหลัง ซึ่งสมมติฐานก็คือน่าจะเป็นลูกค้า.
วิธีการลบฉากหลังมีข้อดีคือ สามารถทำได้ง่ายและรวดเร็ว (ในกรณีที่มีภาพฉากหลังแล้ว).
แต่ข้อเสีย คือความสามารถดังกล่าวไม่สามารถขยายไปใช้ในการแยกวัตถุอื่นได้
เช่น ยากที่จะแยกคนกับรถเข็นออกจากกันได้
และไม่สามารถนำไปใช้ในกรณีที่ฉากหลังมีการเปลี่ยนแปลงสูงได้ เช่น ไม่สามารถนำไปใช้ตรวจจับภาพคนเดินถนน ที่ฉากหลังเป็นสภาพการจราจรได้
และก็ไม่สามารถใช้ในกรณีคนมีการขยับน้อย เช่น ในสวนที่มีคนนั่งอ่านหนังสือ ฝึกสมาธิ หรือนอนหลับอยู่.

อีกแนวทางหนึ่งที่สามารถทำได้ คือ แนวทางวิธีตรวจหาวัตถุ.
\textbf{การตรวจหาวัตถุ} (object detection) เป็นภารกิจการหาตำแหน่งของวัตถุในภาพ หากในภาพมีวัตถุปรากฎอยู่.
แนวทางของวิธีตรวจหาวัตถุมีพื้นฐานมาจากสมมติฐานทางสถิติ 
เช่น รูปทรงของคนแม้จะมีความหลากหลาย
แต่ก็มีรูปแบบและลักษณะร่วมกันอยู่มาก 
และวิธีการคือ การหาลักษณะร่วมเหล่านั้นออกมา
และใช้มันช่วยในการจำแนกระหว่างคนกับสิ่งอื่น ๆ ที่ไม่ใช่คน. 
%
รูปที่~\ref{fig: sliding window pipeline} แสดงขั้นตอนย่อย ๆ ในการตรวจหาตำแหน่งของลูกค้าด้วยวิธีการตรวจหาวัตถุ โดยเริ่มตั้งแต่
%
\begin{enumerate}
	\item การแปลงข้อมูลวิดีโอออกมาเป็นข้อมูลภาพหลาย ๆ ภาพ (frame capture) เทียบเท่าขั้นตอน 1.1 ของรูปที่~\ref{fig: Background Subtraction Pipeline} ของแนวทางการลบฉากหลัง.
	นั่นคือ การปรับปัญหาจากการทำงานกับข้อมูลวิดีโอ เป็นปัญหาเสมือนที่ทำงานกับภาพแทน.
	\item การย่อและขยายภาพให้อยู่ในหลาย ๆ ขนาด (scaling)  
	เพื่อให้วัตถุที่อยู่ห่างกล้องในระยะต่าง ๆ มีโอกาสที่จะถูกตรวจพบใกล้เคียงกัน.
	\item การเลือกส่วนภาพ (window sampling).
	แนวทางนี้ ก็ยังคงเลือกใช้เทคนิคหน้าต่างเลื่อน.
	นั่นคือ ตีกรอบปัญหาการตรวจจับภาพวัตถุ เป็นปัญหาการจำแนกค่าทวิภาค 
	โดยใช้การเลือกส่วนภาพออกมา เพื่อเป็นอินพุตของแบบจำลองจำแนก (ในขั้นตอนต่อไป).
	\item การจำแนกส่วนภาพว่ามีภาพคนอยู่หรือไม่ (window classification).
	ภาพขนาดเท่า ๆ กัน (ได้จากการสุ่มภาพด้วยวิธีหน้าต่างเลื่อน) จะถูกผ่านเข้าแบบจำลองจำแนกข้อมูล
	โดยแบบจำลองจะทำหน้าที่ทำนายว่าภาพอินพุตนั้น มีภาพของวัตถุที่ต้องการ (ในที่นี้คือ ภาพคน) หรือไม่.
	\item การลดผลการตรวจหาที่ซ้ำซ้อน (redundancy removal). 
	การสุ่มด้วยวิธีหน้าต่างเลื่อน มักทำในลักษณะที่มีการซ้อนทับกัน เพื่อปัองกันการตัดกรอบที่กลางวัตถุ
	แต่การสุ่มในลักษณะนี้ก็อาจทำให้ การสุ่มในตำแหน่งใกล้เคียงกัน ตรวจพบวัตถุเดียวกันได้.
	ดังนั้น หลังการตรวจพบวัตถุแล้ว จึงต้องมีการทำการลดการซ้ำซ้อนลง.
	หลังจากขั้นตอนการลดการซ้ำซ้อน เราจะสามารถระบุตำแหน่งของวัตถุที่ตรวจพบได้.
\end{enumerate}

\begin{figure}
	\centering
	\includegraphics[width=5.8in]{04Classic/cdPipeline02}
	\caption[วิธีการตรวจจับภาพวัตถุด้วยเทคนิคหน้าต่างเลื่อน]{แผนภาพแสดงขั้นตอนต่าง ๆ ในการตรวจหาตำแหน่งลูกค้าจากข้อมูลวีดิโอด้วยแนววิธีการตรวจจับภาพวัตถุ}
	\label{fig: sliding window pipeline}
\end{figure}


จากนั้น ผลการตรวจหาที่ได้จะนำไปสรุป และแสดงผลด้วยวิธีแผนที่ความร้อน (hot zone analysis ซึ่งเป็นขั้นตอนที่ 6 ในรูป~\ref{fig: sliding window pipeline}).
%
ขั้นตอนที่ 2 ถึง 5 คือขั้นตอนที่ต่างจากแนวทางวิธีลบฉากหลัง (เปรียบเทียบกับ 1.2a ถึง 1.3b ในรูปที่~\ref{fig: Background Subtraction Pipeline}) 
ถึงแม้ทั้งสองแนวทางจะใช้กลไกของการเลือกส่วนภาพด้วยวิธีหน้าต่างเลื่อนก็ตาม.

%\textit{วิธีเลือกด้วยขีดแบ่ง} (thresholding) หรืออาจใช้\textit{วิธียับยั้งค่าไม่มากที่สุด} (non-maximum suppression) ซึ่งจัดเป็นวิธี\textit{การลดผลการตรวจหาที่ซ้ำซอน} (redundancy removal)

\paragraph{การทำเทคนิคหน้าต่างเลื่อน.}
\index{english}{sliding window}
\index{thai}{วิธีหน้าต่างเลื่อน}
\index{english}{object detection!sliding window}
\index{thai}{การตรวจจับภาพวัตถุ!วิธีหน้าต่างเลื่อน}
%
\textbf{วิธีหน้าต่างเลื่อน}\cite{ViolaJones2001a} เป็นวิธีการเลือกส่วนภาพขนาดที่กำหนดจากข้อมูลภาพใหญ่
โดย
%จะเลือกส่วนภาพแบบทั่วถึง (exhaustive sampling).
%นั่นคือ 
การเลือกส่วนภาพจะเลือกทั่วถึงจากทุกบริเวณในภาพใหญ่
โดยอาจเริ่มจากมุมซ้ายบนของภาพใหญ่
เลือกส่วนภาพออกมา แล้วขยับไปทางขวา และทำเช่นนี้ไปจนสุดปลายด้านขวา
แล้วจึงขยับลงล่างและไปเริ่มจากซ้ายสุด 
และทำลักษณะเช่นนี้อีก จนครอบคลุมบริเวณทั้งภาพใหญ่.
ลำดับของภาพที่เลือกออกมา จะดูคล้ายกับลำดับของภาพที่มองจากหน้าต่างที่เลื่อนไปตำแหน่งต่าง ๆ ของภาพใหญ่
ดังนั้นเทคนิคนี้จึงเรียกว่า เทคนิคหน้าต่างเลื่อน.
%
ขนาดของหน้าต่าง (window size) ซึ่งคือขนาดของส่วนภาพที่เลือก
และขนาดของการขยับหน้าต่าง ที่มักเรียกว่า \textbf{ขนาดขยับเลื่อน} (stride) \index{english}{stride}%
\index{thai}{ขนาดขยับเลื่อน}
ซึ่งเป็นจำนวนพิกเซลของการขยับการเลือกส่วนภาพแต่ละครั้ง เป็น\textit{อภิมานพารามิเตอร์}ของวิธีหน้าต่างเลื่อน. 
%

%
\begin{figure}
	\begin{center}
		\includegraphics[width=5in]{04Classic/sliding_window.png}
		\caption[วิธีหน้าต่างเลื่อน]{ภาพแสดงตัวอย่างการเลือกส่วนภาพด้วยวิธีหน้าต่างเลื่อน.
		ภาพใหญ่และกรอบหน้าต่างเลือก แสดงในส่วนบน (พื้นหลังสีขาว).
		กรอบหน้าต่างเลือก แสดงด้วยกรอบสีเขียว.
		ส่วนภาพที่เลือกมาจากการขยับหน้าต่างแต่ละครั้ง แสดงในส่วนล่าง (พื้นหลังสีเข้ม).
		ในตัวอย่าง ภาพใหญ่ขนาด $16 \times 7$ (กว้าง คูณ สูง).
		ขนาดกรอบหน้าต่างเลือกเป็น $8 \times 4$ และขนาดขยับเลื่อนเป็น $4 \times 3$.
		}
		\label{fig: classic sliding window}
	\end{center}
\end{figure}
%

รูป~\ref{fig: classic sliding window}
แสดงตัวอย่างการทำงานของวิธีหน้าต่างเลื่อน.
ในตัวอย่าง ภาพใหญ่ขนาด $16$ พิกเซล
กรอบหน้าต่างเลือกกว้าง $8$ พิกเซล และขนาดขยับเลื่อนแนวนอนเป็น $4$ พิกเซล
ดังนั้นจึงสามารถขยับได้ $3$ ตำแหน่งในแนวนอน (ได้แก่ เริ่มต้นที่พิกเซล 0, ขยับไปพิกเซล 4, และขยับไปพิกเซล 8).
จำนวนตำแหน่งของหน้าต่างในแนวนอนและแนวตั้งสามารถเขียนเป็นการคำนวณทั่วไปได้ดังนี้
กำหนดให้เมทริกซ์ 
$\bm{F} = [f_{m,n}]$, $m = 0, \ldots, C-1$ และ $n = 0, \ldots, R-1$
แทนภาพใหญ่ขนาด $C \times R$
และ $f_{m,n} \in \mathbb{I}$ เป็นค่าความเข้มของพิกเซลที่ตำแหน่ง $(m,n)$
และให้ $\bm{W}_{ij} \in \mathbb{I}^{A \times B}$ แทนส่วนภาพขนาด $A \times B$ ของดัชนีหน้าต่าง $(i,j)$.
% along row and column dimensions,
หาก\textit{ขนาดขยับเลื่อน}ตามแนวนอนและตั้งเป็น $(a,b)$ แล้ว 
\textit{วิธีหน้าต่างเลื่อน} เป็นเสมือนฟังก์ชันแปลง $S: \bm{F} \mapsto \{ \bm{W}_{ij} \}$ %, for  
สำหรับดัชนี $i = 0, \ldots, \lfloor \frac{C - A}{a} \rfloor$ 
และดัชนี $j = 0, \ldots, \lfloor \frac{R - B}{b} \rfloor$.
แต่ละส่วนภาพ $\bm{W}_{ij} = [w_{p,q}(i,j)]$
เมื่อ $p = 0, \ldots, A-1$ และ $q = 0, \ldots, B-1$
เป็นเมทริกซ์ย่อย
%where %$W_{ij} = [w_{p,q}^{(i,j)}]$, for $p = 1, \ldots, A$, $q = 1, \ldots, B$, 
%and 
$w_{p,q}(i,j) = f_{a \cdot i + p, b \cdot j + q}$.

รูป~\ref{fig: classic sliding window on object detection}
แสดงตัวอย่างการใช้วิธีหน้าต่างเลื่อนกับงานการตรวจจับภาพเป้าหมาย.
ผลลัพธ์ที่ได้คือส่วนภาพต่าง ๆ.
ส่วนภาพต่าง ๆ จะถูกนำไปตรวจสอบ
ดัชนีของส่วนภาพที่พบว่ามีเป้าหมายอยู่ จะถูกนำไปใช้ระบุตำแหน่งของเป้าหมายในภาพใหญ่ (หากพบเป้าหมายในภาพ).
จากอภิมานพารามิเตอร์ที่ใช้ (หน้าต่างขนาด $128 \times 64$ ใช้ขนาดขยับเลื่อนเป็น $16 \time 16$)
และขนาดของภาพใหญ่ ($576 \times 704$)
หลังดำเนินการวิธีหน้าต่างเลื่อน จะได้ส่วนภาพ $\bm{W}_{0,0}, \ldots, \bm{W}_{28, 40}$ (จาก $\lfloor \frac{576 - 128}{16} \rfloor = 28$ และ $\lfloor \frac{704 - 64}{16} \rfloor = 40$).
แต่ละส่วนภาพตัดมาจากภาพใหญ่ เช่น 
\[
\bm{W}_{7,27} = 
\begin{bmatrix}
f_{112,432} & \ldots & f_{112,495} \\
\vdots & \ddots & \vdots \\
f_{239,432} & \ldots & f_{239,495} \\
\end{bmatrix}
\]
เมื่อ $f_{m,n}$ คือค่าความเข้มพิกเซลของภาพใหญ่ที่ตำแหน่งตามแนวตั้ง $m$ และแนวนอน $n$.

%
\begin{figure}
	\begin{center}
		\includegraphics[width=5in]{04Classic/sliding_window_example.png}
		\caption[วิธีหน้าต่างเลื่อนกับการตรวจจับภาพเป้าหมาย]{ตัวอย่างการใช้วิธีหน้าต่างเลื่อนกับงานการตรวจจับภาพเป้าหมาย.
			ภาพซ้ายแถวบนสุด แสดงส่วนภาพแรก (\texttt{W[0,0]}) ที่วิธีหน้าต่างเลื่อนเริ่มต้น.
			และภาพขวาแถวล่างสุด แสดงส่วนภาพสุดท้าย (\texttt{W[28,40]}) ที่วิธีหน้าต่างเลื่อนเลือกออกมา
			เมื่อภาพใหญ่มีขนาด $576 \times 704$ และหน้าต่างขนาด $128 \times 64$ ใช้ขนาดขยับเลื่อนเป็น $16 \time 16$.
		}
		\label{fig: classic sliding window on object detection}
	\end{center}
\end{figure}
%



\paragraph{การจำแนกและระบุตำแหน่งวัตถุ.}
สำหรับแต่ละส่วนภาพที่เลือกมา $\bm{W}_{ij}$
แบบจำลองจำแนกค่าทวิภาคสามารถใช้เพื่อทำนายว่าในส่วนภาพมีวัตถุเป้าหมายอยู่หรือไม่.
นั่นคือ แบบจำลองจำแนกค่าทวิภาค เป็นเสมือนฟังก์ชันแปลง $f: \bm{W}_{ij} \mapsto y_{ij}$
เมื่อ $y_{ij}$ คือค่าทวิภาคที่ทำนาย ซึ่งในกรณีตัวอย่างนี้นิยามเป็น $+1$ (มีเป้าหมายที่ค้นหาอยู่) หรือ $-1$ (ไม่มีเป้าหมายที่ค้นหาอยู่).
หาก $y_{ij} = 1$ นั้นหมายถึง ส่วนภาพ $\bm{W}_{ij}$ มีเป้าหมายอยู่ และตำแหน่งของเป้าหมาย ก็คือตำแหน่งต่าง ๆ ที่ $\bm{W}_{ij}$ ครอบคลุม.

การแปลงจากส่วนภาพเป็นค่าทวิภาค ในตัวอย่างนี้ จะดำเนินเป็นสองขั้นตอน ได้แก่ (1) การแปลงส่วนภาพ ที่มักมีจำนวนมิติสูงมาก เป็นลักษณะสำคัญ ที่มีจำนวนมิติน้อยลงและเกี่ยวข้องการจำแนกเป้าหมายจากสิ่งอื่น ๆ
และ (2) การแปลงลักษณะสำคัญที่ได้จากขั้นตอนแรกไปเป็นค่าทวิภาคที่ต้องการทำนาย.
%แนวทางของการเรียนรู้เชิงลึก\cite{SzegedyEtAl2015a} .
ลักษณะสำคัญ เป็นตัวแทนของอินพุตต้นฉบับในแบบที่ช่วยให้ภาระกิจเป้าหมายดำเนินการได้ง่ายขึ้น.
การเทคนิคที่สำคัญต่าง ๆ ในงานการตรวจจับภาพวัตถุ
ล้วนเกี่ยวข้องโดยตรงกับการทำลักษณะสำคัญแทนของอินพุตต้นฉบับ
ไม่ว่าจะเป็น
ลักษณะฮาร์ (Haar features\cite{ViolaJones2001a})
หรือ
แผนภูมิแท่งของทิศทางเกรเดียนต์ (Histogram of Oriented Gradient)
หรือ
ถุงของทัศนะถ้อยคำ (Bag of Visual Words\cite{FeiFeiPerona2005a}) เป็นต้น.
ลักษณะสำคัญที่พัฒนาขึ้นมาในยุคหลัง ๆ อาจสร้างขึ้นจากลักษณะสำคัญที่พัฒนามาก่อนแล้ว 
เช่น แบบจำลองแปลงรูป (deformable model\cite{FelzenszwalbEtAl2008a}) ที่ใช้แผนภูมิแท่งของทิศทางเกรเดียนต์
เป็นพื้นฐาน.
%\textit{แนวทางการรวมกลุ่ม} (ensemble approach) จากทีมของมาลิสเซียวิกซ์\cite{MalisiewiczEtAl2011a} 
%ทำนายผล โดยการรวมผลทำนายจากแบบจำลองทำนายซับพอร์ตเวกเตอร์แมชชีนเชิงเส้นหลายๆตัว (linear Support Vector Machines\cite{CortesVapnik1995a}) ที่แต่ละตัวฝึกด้วยข้อมูลแค่หนึ่งตัวอย่าง.
แม้แต่แนวทาง\textit{การเรียนรู้เชิงลึก} (บท~\ref{chapter: Deep Learning})
ที่โดยทั่วไปแล้ว ไม่จำเป็นต้องดำเนินการแปลงเป็นสองขึ้นตอน นั่นคือ ไม่ต้องเตรียมลักษณะสำคัญให้แบบจำลอง และสามารถรับอินพุตเป็นภาพต้นฉบับได้โดยตรง
ก็มีการใช้ลักษณะสำคัญ เพียงแต่แบบจำลองทำการสร้างลักษณะสำคัญขึ้นได้เองจากข้อมูลจำนวนมากที่ใช้ฝึก.

ตัวอย่างนี้ดำเนินการจำแนกค่าทวิภาคของส่วนภาพโดย (1) ส่วนภาพ $\bm{W}_{ij}$ จะถูกแปลงเป็นเวกเตอร์ลักษณะสำคัญ $\bm{X}_{ij}$ 
และจากนั้น (2) ลักษณะสำคัญ $\bm{X}_{ij}$ จะถูกแปลงเป็นฉลากทำนาย $y_{ij} \in \{-1, +1\}$.
ตัวอย่างนี้ แสดงการใช้แผนภูมิแท่งของทิศทางเกรเดียนต์\cite{DalalTriggs2005a} เป็นลักษณะสำคัญ
และใช้ซัพพอร์ตเวกเตอร์แมชชีน\cite{CortesVapnik1995a} เป็นแบบจำลองทำนาย.

หมายเหตุ
แนวทางนี้ เพียงตรวจจับตำแหน่งของบุคคลในภาพ และไม่ได้มีการจำแนกแยกแยะมโนคติระดับสูง 
ว่าบุคคลในภาพเป็นลูกค้าจริง ๆ หรือเป็นพนักงานร้าน.
เพื่อจะวิเคราะห์ในมโนคติระดับสูงดังกล่าว
รูปแบบของเครื่องแบบ และเส้นทางการเคลื่อนที่ อาจนำมาใช้ประกอบได้.

\paragraph{การสกัดลักษณะสำคัญ.}
\index{english}{HOG}
\index{english}{Histogram of Oriented Gradients}
\index{thai}{เอชโอจี}

แผนภูมิแท่งของทิศทางเกรเดียนต์ (Histogram of Oriented Gradients\cite{DalalTriggs2005a}) 
ที่มักย่อเป็น เอชโอจี (HOG) เป็นลักษณะสำคัญที่นิยมใช้ในงานคอมพิวเตอร์วิทัศน์.
เอชโอจี เป็นฟังก์ชันแปลง $H: \bm{W} \mapsto \bm{x}$ เมื่อ $\bm{W} \in \mathbb{I}^{A \times B}$
เป็นเมทริกซ์ของค่าความเข้มพิกเซล และ $\bm{x} \in \mathbb{R}^D$ เป็นเวกเตอร์ค่าลักษณะสำคัญเอชโอจี. %, $\mathbb{N}$ is a set of natural numbers.
%It should be noted that 
%Subscript is omitted when context is obvious. 
โดยทั่วไปแล้ว ขนาดของเวกเตอร์ $\bm{x}$ จะเล็กกว่าขนาดของ $\bm{W}$ มาก (นั่นคือ $D << A \times B$). 
สมมติฐานของเอชโอจี
คือการแจกแจงพิกเซลเกรเดียนต์ของภาพสามารถเป็นนัยที่บ่งชี้รูปร่างและสามารถใช้ระบุเป้าหมายได้.
%จากภาพที่แทนด้วยเมทริกซ์ $\bm{W} = [w_{ij}]_{i=1,\ldots,A; j=1,\ldots,B}$
%\[
%\bm{W} = \begin{bmatrix}
%w_{11} & w_{12} & \ldots & w_{1B} \\
%w_{21} & w_{22} & \ldots & w_{2B} \\
%\vdots & \vdots & \ddots & \vdots \\
%w_{A1} & w_{A2} & \ldots & w_{AB}
%\end{bmatrix}
%\]

เอชโอจีเริ่มด้วยการคำนวณพิกเซลเกรเดียนต์ $\bm{G}$. % = \{\bm{G}_x, \bm{G}_y \}$. 
พิกเซลเกรเดียนต์ที่ตำแหน่ง $(r,c)$ เขียนด้วยสัญกรณ์ $\bm{g}_{rc} =[g_x(r,c), g_y(r,c)]^T$ 
เมื่อ $g_x(r,c) =f_{r,c+1} - f_{r,c}$ และ $g_y(r,c) = f_{r+1,c} - f_{rc}$
และ $f_{rc}$ คือค่าความเข้มพิกเซลของภาพที่ตำแหน่งแนวตั้ง $r$ และแนวนอน $c$
โดย กำหนดให้ $f_{rc} \equiv 0$ เมื่อดัชนี $r$ หรือ $c$ เกินขอบเขตภาพ ($r > R$ หรือ $c > C$). 
%
%นั่นคือ
%$\bm{G} = \{\bm{G}_x, \bm{G}_y \}$ เมื่อ
%\[
%\bm{G}_x =
% \begin{bmatrix}
%w_{12} - w_{11} & w_{13} - w_{12} & \ldots & w_{1,B} - w_{1,B-1} & - w_{1,B}\\
%w_{22} - w_{21} & w_{23} - w_{22} & \ldots & w_{2,B} - w_{2,B-1} & - w_{2,B}\\
%\vdots & \vdots          & \vdots          & \ddots & \vdots & \vdots \\
%w_{A2} - w_{A1} & w_{A3} - w_{A2} & \ldots & w_{A,B} - w_{A,B-1} & - w_{A,B}\\
% \end{bmatrix}
%\]
%และ
%\[
%\bm{G}_y =
%\begin{bmatrix}
%w_{21} - w_{11} & w_{22} - w_{12} & \ldots & w_{2B} - w_{1B} \\
%w_{31} - w_{21} & w_{32} - w_{22} & \ldots & w_{3B} - w_{2B} \\
%\vdots & \vdots & \ddots & \vdots \\
%w_{A,1} - w_{A-1,1} & w_{A2} - w_{A-1,2} & \ldots & w_{AB} - w_{A-1,B} \\
%- w_{A,1} & - w_{A,2} & \ldots & - w_{A,B} \\
%\end{bmatrix}
%\]
%นั่นคือ 
พิกเซลเกรเดียนต์ สามารถเขียนในรูปขนาดและมุมได้ ด้วยสัญกรณ์ $\bm{g}_{rc} = m_{rc} \angle \theta_{rc}$ 
โดยขนาด $m_{rc} = \sqrt{g_x^2(r,c) + g_y^2(r,c)}$ และมุม%
\footnote{% 
หากเขียนให้สมบูรณ์ขึ้น คือ มุม $\theta_{rc} = \arctan \frac{g_y(r,c)}{g_x(r,c)}$ เมื่อ $g_x(r,c) > 0$.
มุม $\theta_{rc} = \pi + \arctan \frac{g_y(r,c)}{g_x(r,c)}$ เมื่อ $g_x(r,c) < 0$.
มุม  $\theta_{rc} = \pi/2$ เมื่อ $g_x(r,c) = 0$ และ $g_y(r,c) > 0$.
มุม $\theta_{rc} = -\pi/2$ เมื่อ $g_x(r,c) = 0$ และ $g_y(r,c) < 0$.
มุม $\theta_{rc}$ จะไม่มีความหมาย ถ้า $g_x(r,c) = 0$ และ $g_y(r,c) = 0$.
%$\theta_{rc} = \arctan \frac{g_y(r,c)}{g_x(r,c)} + \left(1 -\mathrm{sign}(g_x(r,c))\right) \cdot \pi$ โดย $\mathrm{sign}(a) = 1$ เมื่อ $a \geq 0$ และ $\mathrm{sign}(a) = 0$ เมื่อ $a < 0$
}	
$\theta_{rc} = \arctan \frac{g_y(r,c)}{g_x(r,c)}$.
รูป~\ref{fig: hog gradient} แสดงค่าพิกเซลเกรเดียนต์ของส่วนภาพตัวอย่างที่มีเป้าหมาย และที่ไม่มีเป้าหมายอยู่.

%
\begin{figure}
	\begin{center}
		\includegraphics[width=5in]{04Classic/hog_gradients.png}
		\caption[เกรเดียนต์ของภาพ]{ตัวอย่างพิกเซลเกรเดียนต์ของภาพ.
แถวบน แสดงตัวอย่างเมื่อส่วนภาพมีเป้าหมายอยู่.
แถวล่าง แสดงตัวอย่างเมื่อส่วนภาพไม่มีเป้าหมายอยู่.
ภาพซ้ายสุดแสดงส่วนภาพที่ในสเกลเทา.
ภาพที่สองและสามจากซ้าย แสดงพิกเซลเกรเดียนต์ในแนวนอนและแนวตั้งตามลำดับ.
ถัดอีกสองภาพ แสดงขนาด (\texttt{grad mag} สำหรับ gradient magnitude) และมุม (\texttt{grad angle} สำหรับ gradient angle)
ของพิกเซลเกรเดียนต์ ตามลำดับ.
สำหรับภาพแสดงขนาดเกรเดียนต์ สีขาวแทนขนาดที่มีค่ามาก และสีดำแทนขนาดที่มีค่าน้อย.
สำหรับภาพแสดงมุม สีแทนองศาของมุม ตามที่ระบุด้วยแถบสีด้านข้าง.
สีที่ใช้สำหรับแสดงมุม ใช้ระบบสีลักษณะวัฏจักร เนื่องจาก $360$ องศา เป็นทิศทางเดียวกับ $0$ องศา.
		}
		\label{fig: hog gradient}
	\end{center}
\end{figure}
%

จากนั้น เอชโอจีดำเนินการโดยแบ่งส่วนภาพ $\bm{W}$ เป็นส่วนย่อย ๆ และเรียกแต่ละส่วนย่อยว่า เซลล์ (cell).
ในแต่ละเซลล์ เอชโอจีจัดทำข้อมูล โดยจินตนาการเป็นเสมือนการทำแผนภูมิแท่ง
โดย  แต่ละแท่ง แทนค่าขนาดของเกรเดียนต์ในแต่ละทิศทาง (ทิศทางที่ใกล้เคียงกันจะถูกรวมอยู่ในแท่งเดียวกัน)
และความสูงของแต่ละแท่ง เรียกว่า โหวต (vote) คำนวณจากผลรวมขนาดของเกรเดียนต์ในทิศทางของแท่งนั้น ๆ.
ขึ้นตอนสุดท้าย เซลล์ต่าง ๆ ที่จะถูกรวมกันเป็นบล็อก (block) ในลักษณะซ้อนทับกัน
และค่าโหวตจากเซลล์ในบล็อกจะถูกนอร์มอร์ไลซ์ภายในบล็อก.
ค่าลักษณะสำคัญของเอชโอจี คือค่าโหวตที่ถูกนอร์มอร์ไลซ์แล้วจากบล็อกต่าง ๆ.
นั่นคือ
ขั้นตอนแรก คำนวณโหวตของเซลล์จากส่วนภาพ $\bm{W}$ ที่มีขนาด $A \times B$ 
โดยดำเนินการแปลง
$\bm{W} \mapsto \{\bm{v}_{ij}\}$ 
สำหรับดัชนีแนวตั้ง $i = 0, \ldots, \lfloor \frac{A}{h_v} \rfloor - 1$
และดัชนีแนวนอน  $j = 0, \ldots, \lfloor \frac{B}{w_v} \rfloor - 1$
เมื่อ $\bm{v}_{ij}$ เป็นเวกเตอร์ของเซลล์โหวต ที่เซลล์มีขนาด $h_v \times w_v$.

หากเลือกจำนวนทิศทางของแผนภูมิแท่งเป็น $K$ ทิศทาง 
เซลล์ $\bm{v}_{ij} \in \mathbb{R}^K$ จะมีส่วนประกอบที่ $k^{th}$ ของเซลล์ ที่เขียนเป็นสัญกรณ์ $v_k(i,j)$
และสามารถคำนวณค่าได้จากผลรวมของขนาดของเกรเดียนต์ในทิศทางที่ $k^{th}$ รวมถึงทิศทางใกล้เคียง ของพิกเซลที่อยู่ในขอบเขตของเซลล์.
นั่นคือ
หากส่วนภาพ $\bm{W} = [w_{r,c}]$ โดย $r = 0, \ldots, A-1$ และ $c = 0, \ldots, B-1$
มีขนาดพิกเซลเกรเดียนต์ $m_{rc}$ และมุมพิกเซลเกรเดียนต์ $\theta_{rc}$ 
แล้วเซลล์โหวต
$v_k(i,j) = \sum_{r,c \in \Omega_{\mathrm{cell}}} m_{rc}$
สำหรับ $k = 0, \ldots, K-1$
เมื่อเซต $\Omega_{\mathrm{cell}}$
แทนเงื่อนไขพิกเซลในขอบเขตของเซลล์ ได้แก่
$i \cdot h_v \leq r < (i+1) \cdot h_v$
และ $j \cdot w_v \leq c < (j+1) \cdot w_v$
และเงื่อนไขทิศทาง ได้แก่
$\frac{k \cdot 360}{K} \leq \theta_{rc} < \frac{(k+1) \cdot 360}{K}$.

รูป~\ref{fig: hog cells}
แสดงตัวอย่างการแบ่งส่วนภาพออกเป็นเซลล์.
จากอภิมานพารามิเตอร์ของตัวอย่าง ส่วนภาพถูกแบ่งออกเป็นเซลล์ $\bm{v}_{0,0}, \ldots, \bm{v}_{15, 7}$
รวมทั้งหมด $128$ เซลล์.
ค่าขนาดของพิกเซลเกรเดียนต์ในเซลล์จะถูกนำมารวมกันตามทิศทาง.
ในภาพแสดงตัวอย่างเมื่อเลือกทำ $9$ ทิศทาง 
ดังนั้นผลลัพธ์คือหนึ่งเซลล์จะมีส่วนประกอบ $9$ ตัวสำหรับทิศทาง $0, 40, 80, 120, 160, 200, 240, 280, 320$ องศา.
แต่ละทิศทางครอบคลุมทิศทางใกล้เคียง เช่น $0$ องศา ครอบคลุม $0 \leq \theta_{rc} < 40$.
หมายเหตุ ภาพในรูป~\ref{fig: hog cells} มีการใช้ค่าชดเชย (offset) เพื่อใช้ทิศทางตัวแทนอยู่ตรงกลาง.
นั่นคือใช้เงื่อนไขทิศทาง
$\frac{k \cdot 360}{K} + \delta \leq \theta_{rc} < \frac{(k+1) \cdot 360}{K} + \delta$
และใช้ค่าชดเชย $\delta = -\frac{360}{2 K}$ ซึ่งในกรณีนี้คือ $-20$.
นั่นทำให้ $0$ องศา ครอบคลุม $-20 \leq \theta_{rc} < 20$.

%
\begin{figure}
	\begin{center}
		\begin{tabular}{ccc}
		\includegraphics[height=1.2in]{04Classic/hog_cells.png}
		&
		\includegraphics[height=1.2in]{04Classic/hog_cells2.png}
		&
		\includegraphics[height=1.2in]{04Classic/hog_cellvote.png}
		\end{tabular} 
		\caption[ตัวอย่างการทำเอชโอจีเซลล์]{ตัวอย่างแสดงการทำเอชโอจีเซลล์.
		ภาพซ้ายสุดแสดงตัวอย่างส่วนภาพ.
		ภาพที่สองจากซ้ายแสดงตัวอย่างส่วนภาพ พร้อมขอบเขตของแต่ละเซลล์ ซึ่งแสดงด้วยเส้นสีเขียว.
		เส้นสีเขียวในภาพทำเพื่อการแสดงผลให้เห็นขอบเขตของแต่ละเซลล์เท่านั้น.
		เส้นสีเขียวไม่ได้เกี่ยวข้องกับกับการทำลักษณะสำคัญเอชโอจี.
		ส่วนภาพขนาด $128 \times 64$ ถูกแบ่งเป็นเซลล์ต่าง ๆ ที่แต่ละเซลล์ขนาด $8 \times 8$.
		ภาพที่สามแสดงเซลล์ $\bm{v}_{0,0}$ ซึ่งเป็นเซลล์แรกอยู่มุมซ้ายบนของส่วนภาพ.
		ภาพที่สี่และห้าแสดงขนาดและมุมของพิกเซลเกรเดียนต์ของเซลล์ $\bm{v}_{0,0}$.
		ภาพสุดท้าย (ขวาสุด) แสดงค่าเซลล์โหวต เมื่อเลือกจำนวนทิศทาง $K = 9$.
		}
		\label{fig: hog cells}
	\end{center}
\end{figure}
%

จากเซลล์โหวตที่ได้
เพื่อลดผลกระทบของแสงและเงาในบริเวณต่าง ๆ
เซลล์จะถูกรวมเป็นบล็อก.
นั่นคือ
หากบล็อกมีขนาด $n_y \times n_x$ เซลล์
และมีขนาดขยับเลื่อนบล็อกเป็น $m_y \times m_x$ เซลล์
ส่วนภาพ $\bm{W}$ ที่มีจำนวนเซลล์เป็น $N_y \times N_x$ %ขนาด $A \times B$ พิกเซล จะ
% เมื่อ $N_y = \lfloor \frac{A}{h_v} \rfloor$ และ $N_x = \lfloor \frac{B}{w_v} \rfloor$)
จะมีบล็อก $\bm{b}_{pq}$ สำหรับ $p = 0, \ldots, \lfloor \frac{N_y - n_y}{m_y} \rfloor$ 
และ $q = 0, \ldots, \lfloor \frac{N_x - n_x}{m_x} \rfloor$.
บล็อก $\bm{b}_{pq}$ จะมีส่วนประกอบเป็นค่าเซลล์โหวตของเซลล์ในขอบเขตของบล็อก.
นั่นคือ
$\bm{b}_{pq} = \{ \hat{\bm{v}}_{ij} \}_{i,j \in \Omega_{\mathrm{block}}}$
เมื่อเซต $\Omega_{\mathrm{block}}$ แทนเงื่อนไข $p \cdot m_y \leq i < p \cdot m_y + n_y$
และ $q \cdot m_x \leq j < q \cdot m_x + n_x$.
เวกเตอร์ $\hat{\bm{v}}_{ij}$ คือค่าเซลล์โหวตหลังการทำนอร์มอร์ไลซ์ 
นั่นคือ ส่วนประกอบที่ $k^{th}$ ของมัน $\hat{v}_k(i,j) = (v_k(i,j) - v_{\min}(p,q))/(v_{\max}(p,q) - v_{\min}(p,q))$
โดย 
$v_k(i,j)$ คือเซลล์โหวตที่ $k^{th}$ ของเซลล์ $(i,j)$
และ
$v_{\max}(p,q)$ กับ $v_{\min}(p,q)$ คือค่าเซลล์โหวตที่มากที่สุดกับน้อยที่สุดในบล็อก ตามลำดับ.

รูป~\ref{fig: hog block01} 
แสดงตัวอย่างขั้นตอนการทำบล็อก.
ในตัวอย่าง บล็อกขนาด $2 \times 2$ เซลล์ รวมผลโหวตของ $4$ เซลล์ หรือเท่ากับ $36$ ค่าโหวต.
หากส่วนภาพมีจำนวนเซลล์เป็น $16 \times 8$ เซลล์
จะมีจำนวนบล็อกทั้งหมดเป็น $\left( \lfloor \frac{16 - 2}{1} \rfloor + 1 \right) \times \left( \lfloor \frac{8 - 2}{1} \rfloor + 1 \right)$ $=105$ บล็อก.
ดังนั้น สำหรับส่วนภาพขนาด $128 \times 64$ ใช้เซลล์ขนาด $8 \times 8$ ทำโหวต $9$ ทิศทาง
ใช้บล็อกขนาด $2 \times 2$ และขนาดขยับเลื่อน $1 \times 1$ 
ลักษณะสำคัญของเอชโอจี จะมี $105 \times 2 \times 2 \times 9 = 3780$ ค่า.

%
\begin{figure}
	\begin{center}
			\includegraphics[width=\textwidth]{04Classic/hog_block01.png}
		\caption[ตัวอย่างการทำเอชโอจีบล็อก]{ขั้นตอนการทำเอชโอจีบล็อก.
		บล็อกขนาด $2 \times 2$ เซลล์ และใช้ขนาดขยับเลื่อน $1 \times 1$ เซลล์.
		แต่ละเซลล์ทำโหวต $9$ ทิศทาง.
		ภาพซ้าย แสดงส่วนภาพ โดยเส้นสีเขียวแสดงขอบเขตแบ่งเซลล์ $\bm{v}_{0,0}$ ถึง $\bm{v}_{1,2}$
		และเส้นสีแดงแสดงขอบเขตของบล็อก $\bm{b}_{0,1}$.	
		ภาพถัดมา (ชื่อภาพ \texttt{Cell[0,0]} ถึง \texttt{Cell[1,2]}) แสดงค่าเซลล์โหวตของเซลล์ $\bm{v}_{0,0}$ ถึง $\bm{v}_{1,2}$.
		ภาพขวาบน แสดงเซลล์โหวตภายในบล็อกก่อนทำนอร์มอร์ไลซ์.
		ภาพขวาล่าง ค่าของบล็อก.
		}
		\label{fig: hog block01}
	\end{center}
\end{figure}
%

รูป~\ref{fig: hog descriptors} แสดงตัวอย่างของลักษณะสำคัญของเอชโอจี สำหรับส่วนภาพที่มีเป้าหมาย และส่วนภาพที่ไม่มีเป้าหมาย.
ลักษณะสำคัญที่แปลงมาอาจจะดูยากด้วยตาเปล่า.
การวัดผลที่เหมาะสมจึงมีความสำคัญมาก. 

%
\begin{figure}
	\begin{center}
		\includegraphics[width=0.45\columnwidth]{04Classic/hog_descriptors_pos.png}
		\includegraphics[width=0.45\columnwidth]{04Classic/hog_descriptors_neg.png}
		\caption[ตัวอย่างลักษณะสำคัญเอชโอจี]{ตัวอย่างลักษณะสำคัญเอชโอจี.
		สองภาพทางซ้าย แสดงตัวอย่างสำหรับส่วนภาพที่มีเป้าหมายอยู่.
		สองภาพทางขวา แสดงตัวอย่างสำหรับส่วนภาพที่ไม่มีเป้าหมายอยู่.
		ภาพแรกและสามจากซ้าย แสดงส่วนภาพ $\bm{W}$ ขนาด $128 \times 64$ (เท่ากับ $8192$ มิติ).
		ภาพสองและสี่จากซ้าย แสดงลักษณะสำคัญเอชโอจี $\bm{x}$ ขนาด $3780$.
		มิติของลักษณะสำคัญเอชโอจีน้อยกว่ามิติของส่วนภาพมาก.
		}
		\label{fig: hog descriptors}
	\end{center}
\end{figure}
%

ข้อสังเกต
การตรวจจับภาพวัตถุ ที่ใช้วิธีการหน้าต่างเลื่อนกับลักษณะสำคัญเอชโอจี
มีการทำงานในลักษณะพื้นที่ย่อย.
นั่นคือ 
หน้าต่างและขนาดขยับเลื่อน ในวิธีหน้าต่างเลื่อน แบ่งจากภาพใหญ่เป็นส่วนภาพ.
เซลล์ ในเอชโอจี แบ่งจากส่วนภาพเป็นเซลล์
แล้วใช้บล็อกกับขนาดขยับเลื่อนบล็อก แบ่งจากส่วนภาพเป็นบล็อก โดยอาศัยค่าที่ได้จากเซลล์.
ทั้งสามระดับมีการทำงานในลักษณะคล้าย ๆ กัน ขั้นตอนหนึ่งต่อจากอีกขั้นตอนหนึ่ง.
บทที่~\ref{chapter: Deep Learning} อภิปรายแนวคิดของการเรียนรู้เชิงลึก และโครงสร้างคอนโวลูชั่น
ที่ทำแนวคิดในลักษณะนี้ แต่ทำในลักษณะที่ทั่วไปและยืดหยุ่นขึ้น.
ปัจจุบัน การเรียนรู้เชิงลึกและโครงสร้างคอนโวลูชั่น
เป็นศาสตร์และศิลป์ของการตรวจจับภาพวัตถุ
และสามารถให้ผลการทำงานที่แม่นยำมาก.

\paragraph{การจำแนกค่าทวิภาค.}
จากภาพ $\bm{F}$ เลือกส่วนภาพต่าง ๆ $\bm{W}_{ij}$ ออกมาด้วยวิธีหน้าต่างเลื่อน.
แต่ละส่วนภาพ $\bm{W}_{ij}$ จะถูกสกัดเป็นลักษณะสำคัญ $\bm{x}_{ij}$.
ตอนนี้จากลักษณะสำคัญ $\bm{x}_{ij}$ แบบจำลองจำแนกค่าทวิภาคสามารถนำมาใช้เพื่อทำนายผลว่าที่ตำแหน่ง $(i,j)$
มีเป้าหมายอยู่หรือไม่.
แบบจำลองจำแนกค่าทวิภาค มีหลายชนิด.
บทที่~\ref{chapter: ANN} อภิปรายโครงข่ายประสาทเทียม.
โครงข่ายประสาทเทียม ก็สามารถนำมาใช้ได้
แต่ตัวอย่างนี้ นำเสนอแบบจำลองจำแนกค่าทวิภาค อีกชนิดที่ได้รับความนิยมมาก คือ ซัพพอร์ตเวกเตอร์แมชชีน.

แบบจำลองจำแนกค่า (ทั้งการจำแนกค่าทวิภาค และการจำแนกกลุ่ม) มีหลายชนิด และอาจแบ่งเป็นแนวทางใหญ่ ๆ ได้สามแนวทาง.
แนวทางแรก เรียกว่า แนวทาง\textbf{แบบจำลองแบ่งแยก} (discriminative model).
แนวทางนี้ เริ่มจากการสร้าง\textbf{แบบจำลองแบ่งแยก} ที่ทำนายความน่าจะเป็นแบบมีเงื่อนไข
ที่เอาต์พุตจะเป็นหนึ่ง สำหรับอินพุตที่ถาม นั่นคือ $\mathrm{Pr}(y=1|\bm{x})$ 
หรือสำหรับการจำแนกกลุ่ม ความน่าจะเป็นแบบมีเงื่อนไขของเอาต์พุตกลุ่มที่ $k^{th}$ สำหรับอินพุตที่ถาม นั่นคือ $\mathrm{Pr}(y=k|\bm{x})$.
หลังจากนั้น ใช้ทฤษฎีการตัดสินใจ เช่น วิธีระดับค่าขีดแบ่ง เพื่อเลือกค่าทวิภาพ หรือฉลากของกลุ่ม สำหรับกรณีการจำแนกกลุ่ม.
โครงข่ายประสาทเทียม (บท~\ref{chapter: ANN}) สำหรับการจำแนกค่าทวิภาค หรือสำหรับการจำแนกกลุ่ม ก็จัดเป็น\textbf{แบบจำลองแบ่งแยก}.
\index{thai}{แบบจำลองแบ่งแยก}
\index{english}{discriminative model}
อย่างไรก็ตาม 
การตีความเอาต์พุตของโครงข่ายประสาทเทียมในเชิงความน่าจะเป็น โดยเฉพาะกรณีจำแนกกลุ่มว่า 
$\hat{y}_k \approx \mathrm{Pr}(y=k|\bm{x})$
มีข้อสังเกต ข้อสงสัย และประเด็นที่กำลังสำรวจและศึกษาวิจัยอยู่\cite{NakjaiEtAl2019a}.

แนวทางที่สอง เรียกว่า แนวทาง\textbf{แบบจำลองสร้างกำเนิด} (generative model).
แนวทางนี้ อาศัยความน่าจะเป็นแบบมีเงื่อนไขของอินพุต สำหรับเอาต์พุตแต่ละแบบ นั่นคือ $\mathrm{Pr}(\bm{x}|y)$
และ\textit{ความน่าจะเป็นก่อน}ของเอาต์พุตแต่ละแบบ นั่นคือ $\mathrm{Pr}(y)$
เพื่ออนุมาน\textit{ความน่าจะเป็นภายหลัง} จาก\textit{กฎของเบส์}.
นั่นคือ 
\begin{eqnarray}
\mathrm{Pr}(y=k|\bm{x}) = \frac{\mathrm{Pr}(\bm{x}|y=k) \cdot \mathrm{Pr}(y=k)}{\mathrm{Pr}(\bm{x})}
\label{eq: generative model}
\end{eqnarray}
โดย $\mathrm{Pr}(\bm{x}) = \sum_k \mathrm{Pr}(\bm{x}|y=k) \cdot \mathrm{Pr}(y=k)$.
หมายเหตุ บางแบบจำลอง แม้อาศัยการอนุมานการแจกแจงของอินพุต แต่อาจไม่ได้ประมาณ $\mathrm{Pr}(\bm{x}|y)$ ออกมาโดยตรง
เช่น โครงข่ายปรปักษ์เชิงสร้างกำเนิด (Generative Adversarial Network\cite{GoodfellowEtAl2014a} คำย่อ GAN)
หรือ ตัวเข้าอัตรหัส (Autoencoder\cite{KingmaWelling2019a}).
\index{thai}{แบบจำลองสร้างกำเนิด}
\index{english}{generative model}
\index{thai}{ตัวเข้าอัตรหัส}
\index{thai}{โครงข่ายปรปักษ์เชิงสร้างกำเนิด}
\index{english}{Generative Adversarial Network}
\index{english}{Autoencoder}

แนวทางที่สาม เป็นการสร้างฟังก์ชันที่แปลงอินพุตไปเป็นเอาต์พุตโดยตรง นั่นคือ $f: \bm{x} \mapsto y$ โดยไม่ได้อาศัยความน่าจะเป็น
หรือไม่สามารถตีในเชิงความเป็นความน่าจะเป็น.
แนวทางนี้ เรียกว่า แนวทาง\textbf{ฟังก์ชันแบ่งแยก} (discriminant function).
ซัพพอร์ตเวกเตอร์แมชชีน ก็จัดอยู่ในแนวทางนี้.
\index{thai}{ฟังก์ชันแบ่งแยก}
\index{english}{discriminant function}
หัวข้อ~\ref{sec: SVM} อภิปรายซัพพอร์ตเวกเตอร์แมชชีน และทฤษฎีเบื้องหลัง.

\subsection{การกำจัดการระบุซ้ำซ้อน}
\label{sec: classic redundancy removal}
\index{thai}{การกำจัดการระบุซ้ำซ้อน}
\index{english}{redundancy removal}

หลังจากขั้นตอนการจำแนกส่วนภาพที่มีเป้าหมายแล้ว
ตำแหน่งของส่วนภาพที่ถูกจำแนกว่ามีเป้าหมาย จะถูกบันทึกเป็นค่าตำแหน่งที่ตรวจพบ.
ตำแหน่ง อาจบันทึกเป็นพิกัดของ\textbf{กล่องขอบเขต} (bounding box) 
เช่น พิกัด $(x,y)$ มุมขวาบนของ\textit{กล่องขอบเขต} กับพิกัดมุมล่างซ้าย
หรืออาจจะเป็น พิกัดมุมขวาบนกับความกว้างและความสูง.
\textit{กล่องขอบเขต} สามารถนิยามเป็นขอบเขตของหน้าต่างที่เลือกส่วนภาพออกมา.
\index{thai}{กล่องขอบเขต}
\index{english}{bounding box}
ส่วนภาพในบริเวณใกล้เคียงกัน อาจถูกระบุว่ามีเป้าหมาย โดยที่เป้าหมายที่ส่วนภาพเหล่านั้นมี เป็นเป้าหมายเดียวกัน
ซึ่งเป็นการระบุซ้ำซ้อน.
รูป~\ref{fig: classic sliding window on object detection} แสดงตัวอย่างส่วนภาพ ที่ได้จากวิธีหน้าต่างเลื่อน.
สังเกตว่า มีหลายส่วนภาพที่ครอบคลุมเป้าหมายเดียวกัน และอาจมีการระบุซ้ำซ้อนเกิดขึ้น.

จากตำแหน่งของส่วนภาพต่าง ๆ ที่ซ้ำซ้อน จะมีแค่ตำแหน่งเดียวที่จะเป็นตัวแทนของตำแหน่งของเป้าหมาย
และที่เหลือจะถูกลบทิ้งไป.
ขั้นตอนการกำจัดการระบุซ้ำซ้อนนี้ จะเรียกว่า \textbf{การกำจัดความซ้ำซ้อน} (redundancy removal).
\index{thai}{การกำจัดความซ้ำซ้อน}
\index{english}{redundancy removal}
\textit{การกำจัดความซ้ำซ้อน} ดำเนินการตั้งแต่ตรวจสอบความซ้ำซ้อน และกำจัดความซ้ำซ้อนที่พบ.
ผลลัพธ์คือ ตำแหน่งต่าง ๆ ของการตรวจจับ ที่ไม่ซ้ำซ้อนกัน.

เพื่อตรวจสอบความซ้ำซ้อน
แนวทางที่นิยม คือ
กำหนดค่าระดับขีดแบ่ง $\tau$
และ\textit{กล่องขอบเขต}สองกล่อง
จะถือว่าซ้ำซ้อนกัน 
เมื่อ
บริเวณซ้อนทับกันมีค่าการซ้อนทับมากกว่า ค่าระดับขีดแบ่ง $\tau$.
ค่าการซ้อนทับ มักถูกวัดด้วย \textbf{ไอโอยู} (IoU ซึ่งย่อมาจาก Intersect over Union)
ซึ่งเป็นสัดส่วนพื้นที่ซ้อนทับกันต่อพื้นที่รวม.
นั่นคือ 
\begin{eqnarray}
\mathrm{IoU} = \frac{A_1 \cap A_2}{A_1 \cup A_2}
\label{eq: IoU}
\end{eqnarray}
เมื่อ $A_1$ และ $A_2$ คือพื้นที่ของ\textit{กล่องขอบเขต}สองกล่องที่พิจารณา.
\index{english}{IoU}
\index{thai}{ไอโอยู}

สำหรับ\textit{กล่องขอบเขต}ต่าง ๆ ที่ซ้ำซ้อนกัน
การกำจัดความซ้ำซ้อน อาจทำโดยสุ่มเลือก\textit{กล่องขอบเขต}ไว้กล่องหนึ่ง และตัดกล่องที่เหลือทิ้งก็ได้
แต่อาจทำให้คุณภาพโดยรวมของการตรวจจับด้อยลง.
\textbf{วิธีการระงับค่าไม่มากสุดท้องถิ่น} (non-local-maximum suppression\cite{MalisiewiczEtAl2011a})
%
%เทคนิค\textit{การตรวจจับขอบภาพ} (edge detection) ก็มีีปัญหาเช่นเดียวกัน
%และ\textit{วิธีแคนนี่} (Canny Edge Detection\cite{Canny1986a}) ซึ่งเป็นวิธีตรวจจับขอบภาพที่รู้จักกันดีในวงการ\textit{การประมวลผลภาพ}
%ใช้แนวทาง\textit{การระงับค่าไม่มากที่สุดท้องถิ่น} (non-local maxima suppression) เพื่อแก้ปัญหา.
จะระงับหรือตัดทิ้ง\textit{กล่องขอบเขต} 
ที่มี\textit{ค่าความเหมาะสม}ไม่มากที่สุด เมื่อเปรียบเทียบกับ\textit{กล่องขอบเขต}อื่น ๆ ที่อยู่รอบ ๆ กล่องนั้น.
\textit{ค่าความเหมาะสม}ของ\textit{กล่องขอบเขต} อาจได้มาจากแบบจำลองจำแนก
เช่น 
กรณีโครงข่ายประสาทเทียม ค่าเอาต์พุตของโครงข่าย (ก่อนผ่านการตัดสินใจด้วยวิธีระดับค่าขีดแบ่ง)
ถูกตีความเป็นค่าความน่าจะเป็น และสามารถนำมาใช้เป็น\textit{ค่าความเหมาะสม}ได้.
สำหรับกรณีซัพพอร์ตเวกเตอร์แมชชีน
ค่า\textit{คะแนนตัดสินใจ} (decision score สมการ~\ref{eq: svm discriminant function support vectors})
สามารถนำมาใช้ได้.
อาณาเขตรอบกล่องที่พิจารณา
โดยทั่วไป มักหมายถึงกล่องที่อยู่ติดกัน
แต่อย่างไรก็ตาม ความกว้างของอาณาเขตนี้ สามารถกำหนดเป็นอภิมานพารามิเตอร์ได้.

\subsection{การนำเสนอผลด้วยแผนที่ความร้อน}
\label{sec: classic heat map}
\index{english}{heat map}
\index{thai}{แผนที่ความร้อน}

แผนที่ความร้อน เป็นแผนภาพสี ที่ให้ข้อมูลความถี่เชิงพื้นที่.
ความถี่ของตำแหน่งที่พบลูกค้าบ่อย ๆ อนุมานมาจากพิกัดตำแหน่งต่าง ๆ ที่ตรวจพบลูกค้า จากชุดลำดับภาพของวิดีโอ.
เวลาที่ลูกค้าใช้ในแต่ละตำแหน่ง จะสะท้อนออกมาในความถี่ที่แสดงนี้.

จากพิกัดตำแหน่งที่ตรวจพบลูกค้า ซึ่งอาจเป็นพิกัดของ\textit{กล่องขอบเขต} จะถูกแปลงเป็นพิกัดตัวแทน
ซึ่งอาจใช้จุดศูนย์กลางของ\textit{กล่องขอบเขต}.
จากนั้น พิกัดตำแหน่งที่ตรวจพบลูกค้าในแต่ละภาพจะถูกนำมารวมกัน ซึ่งเขียนเป็น $\bm{D} = \{\bm{d}_i\}$
สำหรับ $i=1,\ldots, N$ เมื่อ $\bm{d}_i = [x_i, y_i]^T$ เป็นพิกัดตำแหน่งที่พบลูกค้า ในแนวนอนและแนวตั้งของภาพ ตามลำดับ.
ดัชนี $i$ เป็นดัชนีของพิกัด และ $N$ คือจำนวนพิกัดทั้งหมดที่ต้องการนำมาสรุปเป็นความถี่เชิงพื้นที่.
หมายเหตุ ในทางปฏิบัติ การเลือกพิกัดตรวจพบมาสรุปนั้น อาจเลือกตามระยะเวลา เช่นภายในหนึ่งเดือนที่ผ่านมา 
หรือ อาจเลือกตามช่วงเวลาที่สนใจได้ เช่นแยกสรุประหว่างวันธรรมดา และวันเสาร์อาทิตย์.
แต่ ณ ที่นี้ แสดงดัชนี $i$ เป็นดัชนีสำหรับพิกัดที่คัดเลือกมาแล้ว และ $N$ เป็นจำนวนทั้งหมดที่ต้องการนำมาสรุปรวมกัน.

การสร้างแผนที่ความร้อน ก็คือ
การแปลงข้อมูล $\{\bm{d}_i\}$ 
%\in \mathbb{R}^{2 \times N}$ 
ไปเป็นภาพสีขนาด $H \times W$.
%นั่นคือ $\mathbb{R}^{2 \times N} \mapsto \mathbb{I}^{3 \times H \times W}$.
วิธีหนึ่งที่นิยมคือ \textbf{วิธีการประมาณความหนาแน่นแก่น} (Kernel Density Estimation คำย่อ KDE).
\index{thai}{วิธีการประมาณความหนาแน่นแก่น}
\index{english}{Kernel Density Estimation}
\textit{วิธีการประมาณความหนาแน่นแก่น}
เป็นการคำนวณการแจกแจงความน่าจะเป็นของข้อมูล
และจัดเป็น\textit{แบบจำลองสร้างกำเนิด}ชนิดหนึ่ง.
อย่างไรก็ตาม \textit{วิธีการประมาณความหนาแน่นแก่น}ใช้การคำนวณมาก
ดังนั้น \textit{วิธีการประมาณความหนาแน่นแก่น}จึงมีการใช้งานค่อนข้างจำกัด ในทางปฏิบัติ.
และข้อจำกัดนี้ จะเห็นได้ชัดมากขึ้น เมื่อจำนวนข้อมูลมีมากขึ้น หรือข้อมูลมีมิติมากขึ้น.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{04Classic/kde/kde_sigmas.png}
		%
	\end{center}
	\caption[วิธีการประมาณความหนาแน่นแก่น]{ผลการประมาณความหนาแน่นความน่าจะเป็น ด้วยวิธีการประมาณความหนาแน่นแก่น เมื่อใช้ค่า $\sigma$ ต่าง ๆ ได้แก่ $0.01$, $0.1$, $1$, $2$, และ $3$.
		ค่า $\sigma$ ระบุไว้เหนือรูป.
		ภาพซ้ายแสดงความหนาแน่นที่ประมาณ ซ้อนอยู่บนฮิสโตแกรมของจุดข้อมูล โดยความหนาแน่นที่ประมาณถูกปรับขนาด เพื่อเปรียบเทียบกับฮิสโตแกรมได้ชัดเจน.
		ภาพขวาแสดงความหนาแน่นที่ประมาณ โดยแกนนอนแสดงค่าข้อมูล $x$ และแกนตั้งแสดงค่าความหนาแน่นความน่าจะเป็น $p(x)$ ที่ได้จากการประมาณ.
	}
	\label{fig: kde sigma's}
\end{figure}

\textit{วิธีการประมาณความหนาแน่นแก่น}
ประมาณการแจกแจงความน่าจะเป็น
ที่ $\bm{v} \in \mathbb{R}^M$
จากข้อมูล $\bm{d}_i \in \mathbb{R}^M$ สำหรับ $i = 1, \ldots, N$
โดยคำนวณ
\begin{eqnarray}
\hat{p}(\bm{v}) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot z(\bm{v})
\label{eq: KDE shell}
\end{eqnarray}
เมื่อ
\begin{eqnarray}
z(\bm{v}) = \frac{1}{N} \sum_{i=1}^N \exp \left( - \frac{\| \bm{v} - \bm{d}_i \|^2}{2 \sigma^2} \right)
\label{eq: KDE kernel}
\end{eqnarray}
และ $\sigma$ เป็นอภิมานพารามิเตอร์ ซึ่งควบคุมความราบรื่นความต่อเนื่องของผลลัพธ์.

ตัวหารในสมการ~\ref{eq: KDE shell}
ทำเพื่อให้ $\hat{p}(\bm{v})$ มีคุณสมบัติความหนาแน่นความน่าจะเป็นที่ถูกต้อง.
เพื่อการสร้างแผนที่ความร้อน 
การคำนวณเฉพาะค่า $z$ ที่ตำแหน่งต่าง ๆ ก็เพียงพอ.
นั่นคือ สำหรับภาพขนาด $H \times W$
แผนที่ความร้อน $\bm{Z} = [z([c, r]^T)]$ สำหรับ $c = 0, \ldots, W-1$
และ $r = 0, \ldots, H-1$.
จากนั้น $\bm{Z}$ จะถูกนำไปวาดบนภาพสี โดยการแปลงค่า $z$ ที่แต่ละพิกเซลเป็นสีตามแต่ระบบสีที่จะเลือกใช้.

รูป~\ref{fig: kde sigma's}
แสดงผลการประมาณความหนาแน่นความน่าจะเป็น ด้วยวิธีการประมาณความหนาแน่นแก่น เมื่อใช้ค่า $\sigma$ ต่าง ๆ.
ค่า $\sigma$ อาจมองเหมือนเป็นการปรับความเรียบของความหนาแน่นที่จะประมาณ
หรืออาจเปรียบเสมือนสมมติฐานเบื้องต้น เกี่ยวกับการความหนาแน่นของข้อมูล
ว่าข้อมูลมีความหนาแน่นที่มีลักษณะเป็น\textit{การแจกแจงฐานนิยมเดี่ยว} (unimodal distribution)
หรือแบบ\textit{พหุฐาน} (multimodal) และการแจกแจกมีความหลากหลาย มีความซับซ้อนมากขนาดไหน.
ภาพในรูป แสดง ค่า $\sigma$ ขนาดเล็กให้ผลการประมาณความหนาแน่นที่มีความซับซ้อนมาก มีจำนวนฐานมาก ฐานแคบ.
ค่า $\sigma$ ขนาดใหญ่ให้ผลการประมาณความหนาแน่นที่ซับซ้อนน้อย มีจำนวนฐานน้อยลง
และความหนาแน่นมีการกระจายตัวออกไปมากขึ้น ฐานกว้าง.


\subsection{การประเมินผลการตรวจจับ}
\label{sec: object detection evaluation}
\index{english}{mean Average Precision}
\index{english}{mAP}
\index{thai}{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}

การประเมินผลเป็นหัวใจของงานการเรียนรู้ของเครื่อง เป็นหัวใจของงานวิศวกรรมและวิทยาศาสตร์ 
และเป็นหัวใจของ น่าจะเรียกได้ว่า ทุกภาระกิจ.
การประเมินผลการตรวจจับวัตถุ อาจทำได้หลายวิธี.
หนึ่งในวิธีทีี่นิยม คือ การประเมินด้วย\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}.

\textbf{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง} (mean Average Precision คำย่อ mAP) เป็นตัวชี้วัดที่นิยมใช้ประเมินคุณภาพ\textit{ระบบตรวจจับภาพวัตถุ}.
\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}สามารถใช้ประเมินความแม่นยำในการตรวจจับตำแหน่ง พร้อมกับประเมินความแม่นยำในการทายชนิดของวัตถุ 
โดยเป็นการประเมินความสามารถของระบบโดยรวม ไม่เฉพาะเจาะจงกับการเลือก\textit{ระดับค่าขีดแบ่ง}.
(ดูแบบฝึกหัด~\ref{ex: binding affinity} สำหรับตัวอย่างผลจากการเลือก\textit{ระดับค่าขีดแบ่ง}ที่ต่างกัน.)
%
\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง} สามารถคำนวณได้จาก
%
\begin{eqnarray}
\mathrm{mAP} &=& \frac{1}{K} \sum_{k \in \mathrm{Classes}} \mathrm{AP}_k
\label{eq: mAP}
\end{eqnarray}
เมื่อ $\mathrm{Classes}$ คือเซตของฉลากกลุ่มต่าง ๆ ที่เกี่ยวข้อง.
ค่า $K$ คือจำนวนของชนิดฉลากที่เกี่ยวข้อง (ขนาดของเซต $\mathrm{Classes}$).
ค่า $\mathrm{AP}_k$ คือ\textit{ค่าประมาณความเที่ยงตรง}ของสำหรับการตรวจจับภาพของวัตถุชนิด $k$.

\textit{ค่าประมาณความเที่ยงตรง}ของวัตถุแต่ละชนิด $\mathrm{AP}_k$ สามารถประเมินได้จาก
\begin{eqnarray}
\mathrm{AP}_k = \sum_{j \in \mathrm{Ranks}} p_{kj} \cdot \Delta r_{kj}
\label{eq: mAP class AP}
\end{eqnarray}
เมื่อ $\mathrm{Ranks}$ คือเซตลำดับของผลลัพธ์การตรวจพบวัตถุชนิด $k$.
ค่า $p_{kj}$ เป็น\textit{ค่าความเที่ยงตรง}สำหรับชนิด $k$ ที่ลำดับ $j$.
และ $\Delta r_{kj} = r_{kj} - r_{k,j-1}$
โดย $r_{kj}$ เป็น\textit{ค่าการเรียกกลับ}สำหรับวัตถุชนิด $k$ ที่ลำดับ $j$.
\index{english}{precision/recall}
\index{thai}{ค่าความเที่ยงตรง/ค่าการเรียกกลับ}

%
\begin{figure}
	\begin{center}
		\includegraphics[width=6in]{./04Classic/mAP/mAP_Example.png}
		\caption[ตัวอย่างแสดงผลลัพธ์การตรวจจับและผลเฉลย]{ตัวอย่างแสดงผลลัพธ์การตรวจจับและผลเฉลยของวัตถุชนิด A และชนิด B สำหรับภาพ $2$ ภาพ.
			ทุก\textit{กล่องขอบเขต}ของการตรวจจับ จะมีค่าการตรวจจับ $v$ และตัวเลขกำกับ.
			ค่าการตรวจจับ $v$ จะนำไปใช้จัดลำดับการตรวจจับได้.
			ภาพ 1 (ทางซ้าย) มีวัตถุชนิด A (กรอบเส้นทึบสีน้ำเงิน) อยู่ $8$ วัตถุ.
			วัตถุชนิด B (กรอบเส้นทึบสีเขียวภายในสีเหลือง) อยู่ $3$ วัตถุ
			การตรวจจับให้ผลลัพธ์ออกมาเป็น \textit{กล่องขอบเขต} $12$ ตำแหน่งสำหรับชนิด A (กรอบเส้นประสีแดง) และตรวจไม่พบวัตถุชนิด B เลย (ไม่\textit{กล่องขอบเขต}ของ B ที่แทนด้วยกรอบลายไม้).
			ภาพ 2 (ทางขวา) มีวัตถุชนิด A อยู่ $4$ วัตถุ.
			วัตถุชนิด B มีอยู่ $4$ วัตถุ.
			การตรวจจับให้ผลลัพธ์ออกมาเป็น \textit{กล่องขอบเขต} $4$ ตำแหน่งสำหรับชนิด A
			และ $4$ ตำแหน่งสำหรับชนิด B.
			ผลลัพธ์จากการตรวจจับมีทั้ง (1) กรณีที่ตรวจจับได้ถูกต้องทั้งตำแหน่งและชนิด 
			ได้แก่ 
			ชนิด A คือ R1, R2, R4, R5, R6, R8, R9, R11, A2, A3
			และชนิด B คือ B1, B3.
			%ที่ความแม่นยำในการระบุตำแหน่งแตกต่างกัน
			(2) กรณีตรวจจับตำแหน่งได้ถูกต้องแต่ผิดชนิด
			ได้แก่ 
			ทาย A ให้ B คือ
			R3, R7, R10, A1
			และทาย B ให้ A คือ B2.
			(3) กรณีผิดทั้งตำแหน่งและชนิด
			ได้แก่ R12, A4, B4.
			และ (4) กรณีตรวจไม่พบวัตถุ ทั้ง ๆ ที่มีวัตถุอยู่ ได้แก่ ภาพขวา (Image 2) วัตถุ A แถบบนตำแหน่งที่สองจากขวา และวัตถุ B แถวล่างตำแหน่งสองจากขวา.
			นอกจากนั้น ตัวอย่างนี้ยังแสดงความแม่นยำในการระบุตำแหน่งที่แตกต่างกันอีกด้วย.}
		\label{fig: mAP Example}
	\end{center}
\end{figure}
%

%
\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[width=3in]{./04Classic/mAP/PR_A.png}
			&
			\includegraphics[width=3in]{./04Classic/mAP/AP_A.png}
		\end{tabular} 
		\caption[ความเที่ยงตรงและการเรียกกลับและพื้นที่ใต้กราฟ]{ภาพซ้าย กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ} ของการตรวจจับวัตถุชนิด A (ดูตาราง~\ref{tbl: mAP Example} ประกอบ).
			ภาพขวา ค่าประมาณพื้นที่ใต้กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ} ของการตรวจจับวัตถุชนิด A}
		\label{fig: mAP P-R Curve}
	\end{center}
\end{figure}
%

รูป~\ref{fig: mAP Example}
แสดงเฉลย และผลการตรวจจับวัตถุสองชนิด จากภาพสองภาพ.
แต่ละการตรวจจับจะมี\textit{ค่าการตรวจจับ}ระบุอยู่ ($v$ ในภาพ).
มีหลายวิธีในการนำความแม่นยำในการตรวจจับตำแหน่ง เข้ามารวมด้วย.
วิธีง่าย ๆ อาจใช้วิธี\textit{ระดับค่าขีดแบ่ง} กับการวัด\textit{ไอโอยู} เพื่อตัดการตรวจจับที่ตำแหน่งคลาดเคลื่อนมากทิ้ง.
ตัวอย่างเช่น
หาก\textit{กล่องขอบเขต}ของการตรวจจับ มีค่า\textit{ไอโอยู}กับ\textit{กล่องขอบเขต}เฉลย
ต่ำกว่า $0.5$ ถือว่าผิด
นั่นคือเท่ากับ ตรวจจับเกินหนึ่ง 
%($FP + 1$) 
สำหรับการตรวจจับลอย 
และตรวจไม่พบหนึ่ง 
%($FN + 1$) 
สำหรับเฉลยที่ไม่มี\textit{กล่องขอบเขต}ตรวจพบ.
หรืออาจใช้ค่า\textit{ไอโอยู}เข้าไปคำนวณประกอบกับค่าความมั่นใจอื่น เพื่อสรุปออกมาเป็น\textit{ค่าการตรวจจับ}ก็ได้
ซึ่งจะทำให้\textit{กล่องขอบเขต}ของการตรวจจับ 
ที่มีตำแหน่งคลาดเคลื่อนไปมาก จะได้คะแนน\textit{ค่าการตรวจจับ}น้อย.

ตัวอย่าง การประเมิน\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}
จากรูป~\ref{fig: mAP Example} \textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}ของการตรวจจับ สามารถหาได้จาก
(1) เรียงลำดับการตรวจจับตามชนิด
(2) ตรวจสอบผลลัพธ์จากการตรวจจับ เปรียบเทียบกับผลเฉลย
(3) คำนวณหาค่า\textit{ความเที่ยงตรง} $p_{kj}$ และค่า\textit{การเรียกกลับ} $r_{kj}$
(4) คำนวณพื้นที่ใต้กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ} (Area under P-R curve)
และ (5) คำนวณพื้นที่ใต้กราฟเฉลี่ยของทุก ๆ ชนิด ซึ่งคือ\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง} $\mathrm{mAP}$.
ตาราง~\ref{tbl: mAP Example} แสดงตัวอย่างการคำนวณ\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}.

\begin{table}
	\caption[ค่าเฉลี่ยค่าประมาณความเที่ยงตรง]{ตัวอย่างการคำนวณ\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}}
	\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
	\hline 
	ผลลัพธ์ & ค่าการตรวจจับ & ความถูกต้อง & ชนิด & $TP$ & $FP$ & $p_{kj}$ & $r_{kj}$ & $\Delta r_{kj}$ & $p_{kj} \cdot \Delta r_{kj}$ \\
	\hline 
	%\multicolumn{10}{|c|}{ทำนายวัตถุชนิด A (เฉลยมีจำนวณวัตถุชนิด A อยู่ $12$ วัตถุ หรือ $N_A = 12$)}  \\
	%\hline 
	R1 & 0.99 & 1 & A & 1 & 0 & 1.00 & 0.08 & 0.08 & 0.08 \\
	R3 & 0.98 & 0 & A & 1 & 1 & 0.50 & 0.08 & 0.00 & 0.00 \\
	R2 & 0.97 & 1 & A & 2 & 1 & 0.67 & 0.17 & 0.08 & 0.06 \\
	R4 & 0.95 & 1 & A & 3 & 1 & 0.75 & 0.25 & 0.08 & 0.06 \\
	A2 & 0.94 & 1 & A & 4 & 1 & 0.80 & 0.33 & 0.08 & 0.07 \\
	R5 & 0.92 & 1 & A & 5 & 1 & 0.83 & 0.42 & 0.08 & 0.07 \\
	R6 & 0.91 & 1 & A & 6 & 1 & 0.86 & 0.50 & 0.08 & 0.07 \\
	R7 & 0.90 & 0 & A & 6 & 2 & 0.75 & 0.50 & 0.00 & 0.00 \\
	R8 & 0.81 & 1 & A & 7 & 2 & 0.78 & 0.58 & 0.08 & 0.06 \\
	A3 & 0.75 & 1 & A & 8 & 2 & 0.80 & 0.67 & 0.08 & 0.07 \\
	A4 & 0.70 & 0 & A & 8 & 3 & 0.73 & 0.67 & 0.00 & 0.00 \\
	R9 & 0.35 & 1 & A & 9 & 3 & 0.75 & 0.75 & 0.08 & 0.06 \\
	A1 & 0.30 & 0 & A & 9 & 4 & 0.69 & 0.75 & 0.00 & 0.00 \\
	R12 & 0.25 & 0 & A & 9 & 5 & 0.64 & 0.75 & 0.00 & 0.00 \\
	R10 & 0.20 & 0 & A & 9 & 6 & 0.60 & 0.75 & 0.00 & 0.00 \\
	R11 & 0.10 & 1 & A & 10 & 6 & 0.63 & 0.83 & 0.08 & 0.05 \\
	\hline
	\multicolumn{9}{|r|}{$AP_A = $} & $0.65$ \\
	\hline 
	%\multicolumn{10}{|c|}{ทำนายวัตถุชนิด B (เฉลยมีจำนวณวัตถุชนิด B อยู่ $7$ วัตถุ หรือ $N_B = 7$)}\\
	%\hline
	B2 & 0.51 & 0 & B & 0 & 1 & 0 & 0 & 0 & 0 \\
	B3 & 0.45 & 1 & B & 1 & 1 & 0.50 & 0.14 & 0.14 & 0.07 \\
	B4 & 0.42 & 0 & B & 1 & 2 & 0.33 & 0.14 & 0.00 & 0.00 \\
	B1 & 0.10 & 1 & B & 2 & 2 & 0.50 & 0.29 & 0.14 & 0.07 \\
	\hline
	\multicolumn{9}{|r|}{$AP_B = $} & $0.14$ \\
	\hline 
	\multicolumn{9}{|r|}{\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง} $\mathrm{mAP} = \mathrm{mean}_k AP_k = \frac{AP_A + AP_B}{2} \approx $} & $0.40$ \\
	\hline 
\end{tabular} 
\end{center}
	\label{tbl: mAP Example}
\end{table}

ผลลัพธ์การตรวจหาวัตถุถูกนำมาจัดลำดับตาม\textit{ค่าการตรวจจับ}.
\textit{ค่าการตรวจจับ}นี้อาจเป็น\textit{ค่าความน่าจะเป็น}ที่ระบบตรวจจับวัตถุให้ออกมา 
หรืออาจจะเป็นค่าอื่นในลักษณะคล้ายกัน 
เช่น
\textit{ค่าความมั่นใจ}\cite{YOLO1} 
หรือ\textit{ค่าไอโอยู} (สมการ~\ref{eq: IoU})
หรือ\textit{ค่าความน่าจะเป็น}คูณกับ\textit{ค่าไอโอยู}\cite{YOLO2}
ก็ได้.
ในตัวอย่างนี้ \textit{ค่าการตรวจจับ}ที่สูง หมายถึงการตรวจจับได้รับลำดับความสำคัญเป็นลำดับต้น ๆ.
สังเกตว่า การจัดลำดับ ทำตามชนิดวัตถุที่ทำนาย
เช่น ทุกการตรวจจับวัตถุที่ถูกทำนายเป็นชนิด A จะถูกนำมาจัดลำดับด้วยกัน ไม่ว่าจะเป็นการทำนายที่ภาพใด (หรือผลทำนายถูกหรือไม่ หรือว่าเฉลยจริงเป็นชนิดใด).

แต่ละผลลัพธ์จากการตรวจจับ จะถูกเปรียบเทียบกับผลเฉลย. 
ในตาราง~\ref{tbl: mAP Example} สดมภ์\textit{ความถูกต้อง} จะระบุเป็น 1 หากมีผลเฉลยชนิดนั้นในตำแหน่งบริเวณนั้น
และค่า\textit{ความถูกต้อง} จะระบุเป็น 0 หากไม่ใช่.
ตัวอย่างเช่น ในรูป~\ref{fig: mAP Example} ภาพซ้าย 
กล่องขอบเขต R3, R7, และ R10 ที่ทายตำแหน่งของวัตถุชนิด A แต่บริเวณนั้นไม่มีวัตถุชนิด A อยู่ (มีแต่วัตถุชนิด B) หรือ กล่องขอบเขต R12 ที่ทายตำแหน่งของวัตถุชนิด A แต่บริเวณนั้นไม่มีวัตถุใดอยู่เลย
ค่า\textit{ความถูกต้อง}ของกล่องขอบเขตเหล่านี้ จะเป็นศูนย์.

หากผลลัพธ์จากการตรวจจับถูกต้อง ค่า\textit{บวกจริง} $TP$ จะเพิ่มขึ้นหนึ่ง (เริ่มจากลำดับบนสุด)
แต่หากผลลัพธ์จากการตรวจจับไม่ถูกต้อง ค่า\textit{บวกเท็จ} $FP$ จะเพิ่มขึ้นหนึ่ง (เริ่มจากลำดับบนสุด).
ค่า\textit{ความเที่ยงตรง} $p_{kj}$ ซึ่งเป็นอัตราส่วนการทายถูกต่อการทายทั้งหมด
จะสามารถคำนวณได้จาก $p_{kj} = TP/(TP +FP)$ เมื่อ $TP$ และ $FP$ เป็นค่า\textit{บวกจริง} และค่า\textit{บวกเท็จ} สำหรับชนิดวัตถุ $k$ ที่ลำดับ $j$
เช่น สำหรับการทายวัตถุชนิด A ที่ลำดับแรกสุด นั่นคือ การทายกล่องขอบเขต R1 ทำให้ได้ $TP = 1, FP = 0$ และ $p_{A1} = 1$.
แต่ที่ลำดับที่สอง (เปรียบเสมือนการตั้งค่าขีดแบ่งอ่อนลง) 
นั่นคือ การทายกล่องขอบเขต R1 กับกล่องขอบเขต R3 ทำให้ได้ $TP = 1, FP = 1$ และ $p_{A2} = 0.5$.
ค่า\textit{การเรียกกลับ} $r_{kj}$ เป็นอัตราส่วนการทายถูกต่อผลเฉลยทั้งหมด สำหรับชนิดวัตถุ $k$ ที่ลำดับ $j$
ซึ่งคำนวณได้จาก $r_{kj} = TP/N_k$ เมื่อ $N_k$ คือจำนวนผลเฉลยทั้งหมดของวัตถุชนิด $k$.
ในที่นี้ (ดูรูป~\ref{fig: mAP Example} ประกอบ)
จำนวนผลเฉลยทั้งหมดชนิด A หรือ $N_A = 12$
และ
จำนวนผลเฉลยทั้งหมดชนิด B หรือ $N_B = 7$.
ดังนั้น ที่ลำดับล่าง ๆ (เทียบเท่ากับ เมื่อทายมากขึ้น) ค่า\textit{การเรียกกลับ} $r_{kj}$ จึงมีแนวโน้มเพิ่มขึ้น เช่น
สำหรับการทายวัตถุชนิด A ที่ลำดับที่หนึ่ง (เทียบเท่า การทาย R1 อันเดียว) ค่า\textit{การเรียกกลับ} $r_{A1} = 1/12 \approx 0.08$.
แต่ที่ลำดับที่สิบหก (ลำดับสุดท้าย เทียบเท่า การทายผลลัพธ์ทั้ง $16$ อันออกไป) ซึ่งทายถูก $10$ อัน ($TP = 10$) ทำให้ได้ค่า\textit{การเรียกกลับ} $r_{A16} = 10/12 \approx 0.83$.

ค่า $p_{kj}$ และ $r_{kj}$ ที่ได้สามารถนำมาวาดกราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ} (Area under P-R curve) ได้.
รูป~\ref{fig: mAP P-R Curve} (ภาพซ้าย) แสดงกราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ}ของการตรวจจับวัตถุชนิด A.
\textit{ค่าประมาณความเที่ยงตรง}เป็นการประมาณพื้นที่ใต้กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ}. 
หากพื้นที่ใต้กราฟมีขนาดใหญ่ หมายถึงคุณภาพที่ดีของ\textit{ตรวจจับภาพวัตถุ}.
การประมาณพื้นที่ใต้กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ}ของการตรวจจับวัตถุชนิด A
แสดงในรูป~\ref{fig: mAP P-R Curve} (ภาพขวา).

การประมาณพื้นที่ใต้กราฟ\textit{ความเที่ยงตรง}และ\textit{การเรียกกลับ} อาจใช้วิธีการคำนวณสี่เหลี่ยมคางหมูที่ช่วยให้ได้พื้นที่ที่แม่นยำกว่าได้
แต่โดยทั่วไปแล้ว การประมาณคร่าว ๆ ด้วยสี่เหลี่ยมก็เพียงพอ.
พื้นที่ใต้กราฟ หรือ\textit{ค่าประมาณความเที่ยงตรง}ของวัตถุแต่ละชนิด จะถูกนำมาเฉลี่ยกัน 
เพื่อคำนวณเป็น\textit{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง} เช่นในตัวอย่างนี้ $mAP = 0.40$.

\section{ซัพพอร์ตเวกเตอร์แมชชีน}
\label{sec: SVM}
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน}
\index{english}{SVM}
\index{english}{Support Vector Machine}

ซัพพอร์ตเวกเตอร์แมชชีน (Support Vector Machine\cite{CortesVapnik1995a} คำย่อ SVM)
เป็นแบบจำลองจำแนกค่าทวิภาค ในแนวทาง\textit{ฟังก์ชันแบ่งแยก} 
ซึ่งแปลงค่าอินพุต $\bm{x} \in \mathbb{R}^D$ ไปเป็นเอาต์พุต $\hat{y} \in \{-1, +1\}$ โดยตรง
และไม่มีความเชื่อมโยงกับค่าความน่าจะเป็น.
%แนวคิดและกลไกการทำงานของซัพพอร์ตเวกเตอร์แมชชีน แตกต่างจากโครงข่ายประสาทเทียมมาก.
%เริ่มแรก ซัพพอร์ตเวกเตอร์แมชชีนถูกออกแบบมาสำหรับงานการจำแนกค่าทวิภาค.
กลไกการทำงานของซัพพอร์ตเวกเตอร์แมชชีน
อาศัยการแปลงข้อมูลจากปริภูมิของข้อมูลต้นฉบับไปสู่ปริภูมิใหม่
ซึ่งอาจเรียกว่า \textbf{ปริภูมิลักษณะสำคัญ} (feature space)
\index{thai}{ปริภูมิลักษณะสำคัญ}
\index{english}{feature space}
โดย\textit{ปริภูมิลักษณะสำคัญ}นี้จะช่วยให้การแบ่งแยกข้อมูลออกเป็นกลุ่มได้ง่ายขึ้น
และอาศัย\textit{อภิระนาบ}ใน\textit{ปริภูมิลักษณะสำคัญ}
เพื่อตัดแบ่งแยกข้อมูลออกเป็นสองกลุ่ม.

\textbf{อภิระนาบ} (hyperplane) หมายถึง ระนาบในปริภูมิหลายมิติ.
\index{thai}{อภิระนาบ}
\index{english}{hyperplane}
ในปริภูมิสองมิติ \textit{อภิระนาบ} จะหมายถึง เส้นตรง.
นั่นคือ \textit{อภิระนาบ} จะสามารถระบุได้ด้วยสมการ $w_1 x_1 + w_2 x_2 + b = 0$
เมื่อ $\bm{x} = [x_1, x_2]^T$ เป็นตัวแปรในปริภูมิ
และ
$\bm{w} = [w_1, w_2]^T$ กับ $b$ เป็นค่าสัมประสิทธิ์.
ในปริภูมิสามมิติ \textit{อภิระนาบ} จะหมายถึง ระนาบ (แผ่นตรงเรียบ ในสามมิติ).
นั่นคือ \textit{อภิระนาบ} จะสามารถระบุได้ด้วยสมการ $w_1 x_1 + w_2 x_2 + w_3 x_3 + b = 0$
เมื่อ $\bm{x} = [x_1, x_2, x_3]^T$ เป็นตัวแปรในปริภูมิ.
ในปริภูมิ $D$ มิติ \textit{อภิระนาบ} อาจจะยากที่จะจินตนาการ
แต่\textit{อภิระนาบ} ก็จะสามารถระบุได้ด้วยสมการ $\bm{w}^T \bm{x} + b = 0$
เมื่อ $\bm{w}, \bm{x} \in \mathbb{R}^D$.

\begin{figure}
	\begin{center}
		\begin{tabular}{ccccc}
			\includegraphics[width=0.18\columnwidth]{04Classic/svm/hplane2.png} &
			\includegraphics[width=0.18\columnwidth]{04Classic/svm/hplane3.png} &
			\includegraphics[width=0.18\columnwidth]{04Classic/svm/hplane1.png} &
			\includegraphics[width=0.18\columnwidth]{04Classic/svm/hplane4.png} &
			\includegraphics[width=0.18\columnwidth]{04Classic/svm/hplane5.png} 
			%
		\end{tabular} 
	\end{center}
	\caption[อภิระนาบ]{อภิระนาบในสองมิติ เทียบเท่าเส้นตรง.
	แต่ละภาพแสดงอภิระนาบในสองมิติ เมื่อใช้ค่า $\bm{w}$ และ $b$ ต่าง ๆ ซึ่งค่าระบุอยู่ด้านบนของภาพ.
	อภิระนาบแสดงด้วยเส้นทึบสีฟ้า.
}
	\label{fig: hyperplane in 2D}
\end{figure}

รูป~\ref{fig: hyperplane in 2D}
แสดงอภิระนาบในสองมิติ.
สังเกตความสัมพันธ์ระหว่างทิศทางและตำแหน่งของอภิระนาบ
กับค่าของ $\bm{w}$ และ $b$.
หากกำหนดให้ $\bm{x}_a$ และ $\bm{x}_b$ เป็นจุดใด ๆ บนระนาบ.
นั่นคือ $\bm{w}^T \bm{x}_a + b = 0$ และ $\bm{w}^T \bm{x}_b + b = 0$
เวกเตอร์จาก $\bm{x}_a$ ไป $\bm{x}_b$ ซึ่งคือ $\bm{x}_b - \bm{x}_a$
เป็นเวกเตอร์ในแนวของระนาบ.
ดูรูป~\ref{fig: hyperplane w and b} ภาพซ้ายประกอบ.
พิจารณาผลคูณเวกเตอร์ระหว่าง $\bm{w}$ กับเวกเตอร์ในแนวระนาบ
%นั่นคือ พิจารณา $\bm{w}^T \cdot (\bm{x}_b - \bm{x}_a)$.
และจากคุณสมบัติของจุดบนระนาบ ทำให้พบว่า
%\begin{eqnarray}
%\bm{w}^T \cdot (\bm{x}_b - \bm{x}_a) &=& 
%   \bm{w}^T \bm{x}_b - \bm{w}^T\bm{x}_a
%\nonumber \\
%   &=& 
%   \bm{w}^T \bm{x}_b + b - \bm{w}^T\bm{x}_a -b = 0
%\nonumber
%\end{eqnarray}
$\bm{w}^T \cdot (\bm{x}_b - \bm{x}_a)$
$=\bm{w}^T \bm{x}_b - \bm{w}^T\bm{x}_a$
$=\bm{w}^T \bm{x}_b + b - \bm{w}^T\bm{x}_a -b = 0$.
%นั่นคือ
ผลคูณของ $\bm{w}$ กับเวกเตอร์ในแนวระนาบ จะเป็นศูนย์เสมอ.
นั่นแปลว่า เวกเตอร์ $\bm{w}$ ตั้งฉากกับแนวระนาบ.
ดังนั้น ทิศทางของระนาบกำหนดด้วยค่าของเวกเตอร์ $\bm{w}$.

แต่ระนาบจะห่างจาก\textit{จุดกำเนิด}เท่าไร
พิจารณารูป~\ref{fig: hyperplane w and b} ภาพขวา.
\textit{จุดกำเนิด} (origin) คือจุด $[0,0]^T$ ในปริภูมิสองมิติ แสดงด้วยจุดกลมสีดำในภาพ (ภาพขวา จุดนี้อาจถูกบังจากเวกเตอร์ต่าง ๆ).
ระยะห่างระหว่างจุดกำเนิดกับระนาบ คือระยะจากจุดกำเนิดไประนาบในทิศทางที่ตั้งฉากกับระนาบ. 
นั่นคือ 
%ระยะจากจุดกำเนิดไประนาบในทิศทางของ $\bm{w}$.
หากกำหนดให้ $\bm{x}_c$ เป็นจุดใด ๆ ในระนาบ
ขนาดของภาพฉายของ $\bm{x}_c$ ลงบนเวกเตอร์หนึ่งหน่วยในแนว $\bm{w}$ 
จะเป็นระยะทางจากจุดกำเนิดไปถึงระนาบ.
ดังนั้น ระยะทางจากจุดกำเนิดไประนาบ สามารถคำนวณได้จาก
$d = \frac{\bm{w}^T}{\|\bm{w}\|} \cdot \bm{x}_c = \frac{\bm{w}^T \bm{x}_c}{\|\bm{w}\|}$
เมื่อ $d$ คือระยะทางจากจุดกำเนิดไประนาบ.
จากคุณสมบัติของระนาบ $\bm{w}^T \bm{x}_c + b = 0$ 
ทำให้ พบว่า
%\begin{eqnarray}
%d = \frac{-b}{\|w\|} 
%\label{eq: svm distance}
%\end{eqnarray}
$d = \frac{-b}{\| \bm{w}\|}$.
หมายเหตุ ขนาดของการฉายภาพเป็นลบ หมายถึงทิศทางของภาพที่ฉาย จะกลับทิศกับเวกเตอร์หนึ่งหน่วยที่เป็นเสมือนฉาก.
ถ้า $b < 0$ ทำให้ ระยะ $d > 0$ ระนาบจะห่างจุดกำเนิดออกไปขนาด $|d|$ ทางทิศ $\bm{w}$
ถ้า $b > 0$ ทำให้ ระยะ $d < 0$ ระนาบจะห่างจุดกำเนิดออกไปขนาด $|d|$ ทางทิศตรงข้ามกับ $\bm{w}$
และถ้า $b = 0$ หมายถึง ระนาบจะผ่านจุดกำเนิด.

\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[width=0.5\columnwidth]{04Classic/svm/wandplane.png} &
			\includegraphics[width=0.5\columnwidth]{04Classic/svm/xcprojected.png}
			%
		\end{tabular} 
	\end{center}
	\caption[อภิระนาบและค่าพารามิเตอร์]{ความสัมพันธ์ระหว่างพารามิเตอร์ $\bm{w}$ และ $b$ กับคุณลักษณะของอภิระนาบ.
	ภาพซ้าย $\bm{x}_a$ และ $\bm{x}_b$ เป็นจุดใด ๆ บนระนาบ.
	ระนาบแสดงด้วยเส้นสีฟ้า.
	เวกเตอร์จาก $\bm{x}_a$ ไป $\bm{x}_b$ แสดงด้วยลูกศรสีฟ้าเขียว.
	เวกเตอร์จากจุดกำเนิดไป $\bm{w}$ แสดงด้วยลูกศรสีเขียว.
	ภาพขวา $\bm{x}_c$ เป็นจุดใด ๆ บนระนาบ ซึ่งระนาบแสดงด้วยเส้นสีฟ้า.
	เวกเตอร์จากจุดกำเนิดไปหาจุด $\bm{x}_c$ แสดงด้วยลูกศรสีส้ม.
	เวกเตอร์หนึ่งหน่วยในทิศทาง $\bm{w}$ ซึ่งคือ $\bm{w}/\|\bm{w}\|$ แสดงด้วยลูกศรสีเขียว.
	ขนาดของเวกเตอร์ $\bm{x}_c$ ที่ฉายลงบน $\bm{w}/\|\bm{w}\|$ แสดงด้วยเส้นสีเหลือง.
แต่บริเวณนั้นมีการซ้อนทับกันมาก อาจมองไม่ชัด เส้นสีฟ้าเขียวที่มีลูกศรสองทาง ขยับออกมาแสดงขนาดของการฉายให้ชัดเจนขึ้น.
	}
	\label{fig: hyperplane w and b}
\end{figure}

\paragraph{การวางกรอบปัญหาของซัพพอร์ตเวกเตอร์แมชชีน.} 
ซัพพอร์ตเวกเตอร์แมชชีน ในปัจจุบันถูกประยุกต์ในงานหลากหลายทั้งงานการหาค่าถดถอย และการจำแนกกลุ่ม
แต่ดั้งเดิมเริ่มต้น
ซัพพอร์ตเวกเตอร์แมชชีน ถูกออกแบบสำหรับการจำแนกค่าทวิภาค.
ข้อมูลจะถูกแบ่งออกเป็น กลุ่มบวก และกลุ่มลบ.
แนวคิดของซัพพอร์ตเวกเตอร์แมชชีน
คือ
การใช้อภิระนาบเป็นเสมือนเส้นแบ่งการตัดสินใจ
และข้อมูลฝึกจะถูกนำมาใช้ 
เพื่อเลือกอภิระนาบในปริภูมิลักษณะสำคัญ 
ที่ทำให้ช่องว่างที่แบ่งระหว่างจุดข้อมูลกลุ่มบวกกับกลุ่มลบห่างกันมากที่สุด.
%หรือมักถูกอ้างถึงสั้นๆว่า เพื่อทำให้ช่องว่างของการแบ่งกว้างที่สุด (maximize the margin of separation).

ซัพพอร์ตเวกเตอร์แมชชีน อาศัยกลไกที่สำคัญสองอย่าง.
กลไกสำคัญแรก คือ การแปลงจุดข้อมูลไปสู่ปริภูมิลักษณะสำคัญ.
เนื่องจากอภิระนาบเป็นฟังก์ชันเชิงเส้น
บทบาทของการแปลงข้อมูล จะช่วยในการแปลงข้อมูลไปสู่ปริภูมิที่ข้อมูลจะสามารถถูกแบ่งได้ดีด้วยอภิระนาบ.
กำหนดให้ลักษณะสำคัญ $\bm{z} = \phi(\bm{x})$
เมื่อ $\bm{x}$ เป็นจุดข้อมูลในปริภูมิข้อมูลดั้งเดิม
และ $\phi: \mathbb{R}^D \mapsto \mathbb{R}^M$ เป็นฟังก์ชันที่ใช้แปลงข้อมูลไปสู่ปริภูมิลักษณะสำคัญ
โดย $D$ และ $M$ คือจำนวนมิติของปริภูมิข้อมูลดั้งเดิมและของปริภูมิลักษณะสำคัญตามลำดับ.
ดังนั้น จุดข้อมูล $\bm{x}$ (ในปริภูมิข้อมูลดั้งเดิม) จะถูกแทนด้วย $\bm{z}$ ในปริภูมิลักษณะสำคัญ.
%และข้อมูลฝึก คือ $\{ \bm{z}_i, y_i \}$ 
%เมื่อ $i$ แทนดัชนีของตัวอย่างข้อมูลฝึก.
%ตัวแปร $\bm{z}_i \in \mathbb{R}^D$ คือลักษณะสำคัญของอินพุตของข้อมูลฝึกที่ $i^{th}$ 
%และ $y_i \in \{-1, +1\}$ คือเอาต์พุตเฉลย.
การใช้งานซัพพอร์ตเวกเตอร์แมชชีนให้มีประสิทธิผล เกี่ยวพันโดยตรงกับการเลือกฟังก์ชันลักษณะสำคัญให้เหมาะสม
ซึ่งจะได้อภิปรายรายละเอียดในหัวข้อ~\ref{sec: svm kernel function}.
หมายเหตุ
การเลือกที่จะทำการแบ่งข้อมูลในปริภูมิดั้งเดิม สามารถทำได้
และในหลาย ๆ กรณี ก็เป็นทางเลือกที่เหมาะสม.
การเลือกที่จะทำการแบ่งข้อมูลในปริภูมิดั้งเดิม
ก็เปรียบเสมือนการเลือกใช้ฟังก์ชันเอกลักษณ์เป็นฟังก์ชันลักษณะสำคัญ นั่นคือ $\phi(\bm{x}) = \bm{x}$ 
และ $\bm{z} = \bm{x}$.

กลไกสำคัญที่สอง คือ
การหาอภิระนาบที่ทำให้\textit{ขอบเขตของการแบ่ง}กว้างที่สุด.
รูป~\ref{fig: margin of separation}
แสดงตัวอย่างความสัมพันธ์ระหว่าง จุดข้อมูลต่าง ๆ ในปริภูมิลักษณะสำคัญ
อภิระนาบตัดสินใจ และ\textit{ขอบเขตของการแบ่ง}.
ออกแบบเพื่อการแบ่งกลุ่มสองกลุ่มโดยเฉพาะ
จุดประสงค์ของซัพพอร์ตเวกเตอร์แมชชีน
ไม่ใช่แค่หาอภิระนาบใดก็ได้ที่แบ่งข้อมูลได้
แต่ต้องการหาอภิระนาบที่แบ่งข้อมูลได้ และแบ่งได้โดยมี\textbf{ขอบเขตของการแบ่ง} (margin of separation)
ที่กว้างที่สุดด้วย.
\index{english}{margin of separation}
\index{thai}{ขอบเขตของการแบ่ง}
สังเกตว่า ในรูป~\ref{fig: margin of separation}
หากเอียงอภิระนาบตัดสินใจเพิ่มขึ้นหรือลดลงเล็กน้อย
ผลที่ได้ก็จะยังคงสามารถแบ่งข้อมูลได้สมบูรณ์
แต่\textit{ขอบเขตของการแบ่ง}จะแคบลง.

จุดข้อมูลที่อยู่บนแนว\textit{ขอบเขตของการแบ่ง}
ซึ่งเป็นจุดข้อมูลที่อยู่ใกล้กับจุดข้อมูลจากต่างกลุ่มมากที่สุด
เป็นจุดที่แบ่งยากที่สุด
และอภิระนาบจะถูกกำหนดด้วยจุดข้อมูลเหล่านี้.
จุดข้อมูลเหล่านี้จะเรียกว่า \textbf{ซัพพอร์ตเวกเตอร์} (support vectors)
\index{english}{support vectors}
\index{thai}{ซัพพอร์ตเวกเตอร์}
ซึ่งเป็นที่มาของชื่อ ซัพพอร์ตเวกเตอร์แมชชีน.
จุดข้อมูลอื่น ๆ ที่อยู่ลึกลงไปในเขตของกลุ่ม เป็นจุดข้อมูลที่แบ่งแยกได้ง่ายกว่า จะไม่มีบทบาทในการกำหนดอภิระนาบ.

\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{04Classic/svm/feature_space_hyperplane.png}
\end{center}
	\caption[ช่องว่างของการแบ่งกลุ่ม]{จุดข้อมูลต่าง ๆ ในปริภูมิลักษณะสำคัญ (จุดกากบาทสีแดงกลุ่มลบ และจุดดาวสีเขียวกลุ่มบวก) และอภิระนาบตัดสินใจ (เส้นทึบสีน้ำเงิน).
	เส้นประสีฟ้าเขียว แสดง\textit{ขอบเขตของการแบ่ง}. 
	ระยะจากอภิระนาบถึง\textit{ขอบเขตของการแบ่ง} แสดงในรูปด้วยลูกศรสีม่วง. 
	จุดข้อมูลที่อยู่บนแนว\textit{ขอบเขตของการแบ่ง} เน้นด้วยสีเหลืองรอบ ๆ จุด.
	}
	\label{fig: margin of separation}
\end{figure}

ปัญหาการจำแนกค่าทวิภาคในทางปฏิบัติ 
อาจจะมี\textit{ขอบเขตของการแบ่ง} ที่ซับซ้อนกว่าสถานการณ์ในรูป~\ref{fig: margin of separation}
ซึ่งสามารถแบ่งกลุ่มได้อย่างสมบูรณ์ด้วยอภิระนาบ.
ในที่นี้ พิจารณาการพัฒนาซัพพอร์ตเวกเตอร์แมชชีน สำหรับกรณีที่ข้อมูลสามารถแบ่งแยกได้สมบูรณ์ก่อน
และหัวข้อ~\ref{sec: svm non-separable problem} อภิปรายการพัฒนาขยายความสามารถสำหรับกรณีทั่วไป 
(ซึ่งรวมถึงสถานการณ์ที่ไม่สามารถแบ่งแยกกลุ่มได้สมบูรณ์).

\paragraph{การหาอภิระนาบ.}
หากอภิระนาบตัดสินใจ บรรยายด้วย $\bm{w}^T \bm{z} + b = 0$
และมีข้อมูลฝึก $\{\bm{x}_i, y_i\}_{i=1, \ldots, N}$ 
ซึ่งเทียบเท่า $\{\bm{z}_i, y_i\}$ โดย $\bm{z}_i = \phi(\bm{x}_i)$
แล้ว 
อภิระนาบที่แบ่งแยกข้อมูลได้อย่างสมบูรณ์ คือ
อภิระนาบที่มีค่าพารามิเตอร์ $\bm{w}$ กับ $b$ ที่ทำให้
\begin{eqnarray}
\bm{w}^T \bm{z}_i + b &> 0 & \mbox{ เมื่อ } y_i = 1
\nonumber \\
\bm{w}^T \bm{z}_i + b &< 0 & \mbox{ เมื่อ } y_i = -1
\label{eq: svm separable starting criteria}
\end{eqnarray}
สำหรับ $i = 1, \ldots, N$ โดย $N$ เป็นจำนวนข้อมูลฝึก.

เพื่อความสะดวก
กำหนดฟังก์ชันแบ่งแยก $f$ เป็น
\begin{eqnarray}
f(\bm{z}) = \bm{w}^T \bm{z} + b
\label{eq: svm discriminant function}.
\end{eqnarray}

ผลทายกลุ่ม หรือผลตัดสินใจ $\hat{y}$ สามารถคำนวณได้จาก
\begin{eqnarray}
\hat{y} =
\left\{
\begin{array}{l l}
1 & \mbox{ เมื่อ } f(\bm{z}) > 0, \\
-1 & \mbox{ เมื่อ } f(\bm{z}) < 0. \\
\end{array}
\right.
\label{eq: svm separable yhat}
\end{eqnarray}

นอกจากแบ่งแยกข้อมูลได้สมบูรณ์แล้ว 
เรายังต้องการให้\textit{ขอบเขตของการแบ่ง}กว้างที่สุดด้วย.
พิจารณาระยะจากอภิระนาบไปสู่จุดข้อมูลใด ๆ.
ดูรูป~\ref{fig: distance from hyperplane to point} ประกอบ.
เมื่อแทนจุดใด ๆ ในปริภูมิด้วยเวกเตอร์จากจุดกำเนิดไปจุดนั้น
เวกเตอร์ $\bm{z}_i$ สามารถเขียนในรูปส่วนประกอบได้ว่า
\[
\bm{z}_i = \bm{z}_p + r \vec{u}
\]
เมื่อ $\bm{z}_p$ คือจุดภาพฉายเชิงตั้งฉากจากจุด $\bm{z}_i$ ลงบนอภิระนาบ
และ $\vec{u} = \bm{w}/\|\bm{w}\|$ คือเวกเตอร์หนึ่งหน่วยในทิศทางตั้งฉากกับอภิระนาบ
และ $r$ คือระยะห่างระหว่างจุด $\bm{z}_i$ กับอภิระนาบ.
ดังนั้น จุดใด ๆ $\bm{z}_i = \bm{z}_p + r \bm{w}/\|\bm{w}\|$
และค่าฟังก์ชันแบ่งแยก
\begin{eqnarray}
f(\bm{z}_i) &=& f(\bm{z}_p + r \bm{w}/\|\bm{w}\|)
= \bm{w}^T \cdot (\bm{z}_p + r \bm{w}/\|\bm{w}\|) + b
\nonumber \\
&=& \bm{w}^T \bm{z}_p + b + r \|\bm{w}\|^2/\|\bm{w}\| = r \|\bm{w}\|
\nonumber \\
r &=& \frac{f(\bm{z}_i)}{\|\bm{w}\|}
\label{eq: svm distance from hyperplane}.
\end{eqnarray}
นั่นคือ ระยะห่างระหว่างจุดใด ๆ $\bm{z}_i$ กับอภิระนาบจะเท่ากับค่าฟังก์ชันแบ่งแยกหารด้วยขนาดของ $\bm{w}$.
ถ้า $f(\bm{z}_i) = 0$ ก็คือระยะห่าง $r = 0$ จุดอยู่บนระนาบ.
ถ้า $f(\bm{z}_i) > 0$ และจุดนั้นอยู่ห่างระนาบตามคำนวณ ไปทางฝั่งกลุ่มบวก.
ถ้า $f(\bm{z}_i) < 0$ และจุดนั้นอยู่ห่างระนาบไปทางฝั่งกลุ่มลบ.
หมายเหตุ จุดกำเนิด $\bm{0}$ จะอยู่ห่างระนาบ $r = f(\bm{0})/\|\bm{w}\| = b/\|\bm{w}\|$.
%โดย ถ้า $b > 0$ จุดกำเนิดอยู่ฝั่งของกลุ่มบวก.
%ถ้า $b < 0$ จุดกำเนิดอยู่ฝั่งของกลุ่มลบ
%และถ้า $b = 0$ จุดกำเนิดอยู่บนระนาบ.
สังเกตว่า ระยะ $r$ คือระยะจากระนาบไปจุดใด ๆ
ซึ่งเมื่อพิจารณาที่จุดกำเนิด
ระยะ $r$ กับระยะจากจุุดกำเนิดไประนาบ $d$ (ที่อภิปรายตอนต้นหัวข้อ) จะกลับทิศทางกัน.


\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{04Classic/svm/point_on_feature_space.png}
	\end{center}
	\caption[ระยะทางจากอภิระนาบไปจุดข้อมูล]{ระยะไปสู่จุดข้อมูลใด ๆ จากอภิระนาบแทนด้วย $r$ ซึ่งคือขนาดของเวกเตอร์จากจากจุด $\bm{z}_p$ ไป $\bm{z}_i$.
	จุดข้อมูลใด ๆ $\bm{z}_i$ แสดงด้วยดาวสีเขียวเข้ม (บางส่วนถูกบัง).
	อภิระนาบ แสดงด้วยเส้นหนาสีน้ำเงิน.
	จุด $\bm{z}_p$ (จุดกลมสีม่วง บางส่วนถูกบัง) คือจุดที่ถูกฉาย จาก $\bm{z}_i$ ลงบนระนาบ.
	เวกเตอร์จากจุดกำเนิดไป $\bm{z}_i$ (เวกเตอร์สีเขียว) เท่ากับเวกเตอร์จากจุดกำเนิดไป $\bm{z}_p$ (เวกเตอร์สีฟ้าอ่อน) บวกกับเวกเตอร์จาก $\bm{z}_p$ ไป $\bm{z}_i$ (เวกเตอร์สีฟ้าเข้ม).
	}
	\label{fig: distance from hyperplane to point}
\end{figure}

พารามิเตอร์ของอภิระนาบที่ทำให้\textit{ขอบเขตของการแบ่ง}กว้างที่สุด
อาจเขียนเป็นเงื่อนไขทั่วไปได้ว่า
\begin{eqnarray}
\bm{w}^T \bm{z}_i + b & \geq +1 & \mbox{ สำหรับ } y_i = +1
\nonumber \\
\bm{w}^T \bm{z}_i + b & \leq -1 & \mbox{ สำหรับ } y_i = -1
\label{eq: svm margin constraints}
\end{eqnarray}
ในสถานการณ์ที่ข้อมูลสามารถแบ่งแยกได้โดยสมบูรณ์
ค่าพารามิเตอร์ $\bm{w}$ และ $b$ ที่ได้ สามารถนำไปปรับขนาดโดยคูณค่าคงที่เข้าไป 
เพื่อให้เงื่อนไขในอสมการ~\ref{eq: svm margin constraints} เป็นจริงได้.
จุดข้อมูล $i^{th}$ ที่ทำให้เงื่อนไขในอสมการ~\ref{eq: svm margin constraints} \textit{ทำงาน}%
\footnote{%
	ในทฤษฎีการหาค่าดีที่สุด \textit{เงื่อนไขอสมการ}หรือ\textit{ข้อจำกัดอสมการ} (inequality constraint) 
	เมื่อนำมาเขียนในรูป $g(x) \geq 0$ 
	จะเรียกว่า \textit{ทำงาน} (active) ที่ค่า $x_0$ ถ้า $g(x_0) = 0$.
	ตัวอย่างเช่น $\bm{w}^T \bm{z}_i + b \geq 1$ ซึ่งเทียบเท่า 
	$\bm{w}^T \bm{z}_i + b - 1 \geq 0$ จะเรียกว่า
	ทำงานที่ $\bm{z}_0$ เมื่อ $\bm{w}^T \bm{z}_i + b - 1 = 0$.
	\index{english}{active constraint}
	\index{thai}{ข้อจำกัดที่ทำงาน}
}
จะอยู่บนแนว\textit{ขอบเขตของการแบ่ง} และจุดเหล่านี้จะเรียกว่า \textit{ซัพพอร์ตเวกเตอร์}. (ดูรูป~\ref{fig: margin of separation} ประกอบ).
ค่าฟังก์ชันแบ่งแยกของซัพพอร์ตเวกเตอร์ เป็น
\[
f(\bm{z}'_i) = \bm{w}^T \bm{z}'_i + b = 
\left\{
\begin{array}{ll}
+1 & \mbox{ เมื่อ } y'_i = +1, \\
-1 & \mbox{ เมื่อ } y'_i = -1. 
\end{array}
\right.
\]
โดย $\bm{z}'_i$ และ $y'_i$ คือค่าลักษณะสำคัญและเฉลยของซัพพอร์ตเวกเตอร์ดัชนี $i^{th}$.

ระยะจากอภิระนาบไปซัพพอร์ตเวกเตอร์ $\bm{z}'_i$ จะเป็น
\[
r = \frac{f(\bm{z}'_i)}{\|\bm{w}\|}
= \left\{
\begin{array}{lll}
\frac{+1}{\|\bm{w}\|} & \mbox{ เมื่อ } & y'_i = +1, \\
\frac{-1}{\|\bm{w}\|} & \mbox{ เมื่อ } & y'_i = -1. \\
\end{array}
\right.
\]
ดังนั้นความกว้างของ\textit{ขอบเขตของการแบ่ง} $\rho = 2 r = 2/\|\bm{w}\|$
และปัญหาค่ามากที่สุด
$\max_{\bm{w}, b} \rho = \frac{2}{\|\bm{w}\|}$
ก็เทียบเท่าปัญหาค่าน้อยที่สุด
$\min_{\bm{w}, b} \|\bm{w}\|$.
%
นอกจากนั้น
อสมการ~\ref{eq: svm margin constraints} สามารถเขียนให้กระชับขึ้นได้เป็น
\begin{eqnarray}
y_i \cdot (\bm{w}^T \bm{z}_i + b) & \geq 1 
\label{eq: svm concise margin constraints}.
\end{eqnarray}
นั่นคือ กรอบปัญหาการฝึกซัพพอร์ตเวกเตอร์แมชชีน สามารถเขียนได้เป็น
\begin{eqnarray}
\underset{\bm{w}, b}{\mathrm{minimize}} &  \frac{1}{2} \bm{w}^T \bm{w} &
\nonumber \\
\mbox{s.t.} 
& y_i (\bm{w}^T \bm{z}_i + b) \geq 1 & \mbox{ for } i =1, \ldots, N.
\label{eq: svm primal separable}
\end{eqnarray}
ฟังก์ชันจุดประสงค์ของซัพพอร์ตเวกเตอร์แมชชีน เป็น\textit{คอนเวกซ์} (convex function)
และข้อจำกัดเป็นฟังก์ชันเชิงเส้น 
ซึ่งเหล่านี้ล้วนเป็นคุณสมบัติที่ดี
จากมุมมองของการแก้\textit{ปัญหาค่าดีที่สุด} (หัวข้อ~\ref{sec: optimization}) เพราะว่า
ลักษณะเหล่านี้
ทำให้ 
เมื่อแก้ปัญหาและพบ\textit{ค่าทำให้น้อยที่สุดท้องถิ่น}แล้ว 
\textit{ค่าทำให้น้อยที่สุดท้องถิ่น}จะเป็น
\textit{ค่าทำให้น้อยที่สุดทั่วหมด}ด้วย.

การใช้งานซัพพอร์ตเวกเตอร์แมชชีนในทางปฏิบัติจะไม่แก้ปัญหานี้โดยตรง
แต่จะใช้คุณสมบัติของ\textit{ภาวะคู่กัน} (ดูแบบฝึกหัด~\ref{ex: opt duality} เพิ่มเติม) เพื่อแปลงปัญหาในนิพจน์~\ref{eq: svm primal separable} ซึ่งเป็น\textit{ปัญหาปฐม}
ไปอยู่ในรูป\textit{ปัญหาคู่} ซึ่งจะสามารถใช้งานได้มีประสิทธิภาพกว่า.

\paragraph{ปัญหาคู่.}
จาก\textit{วิธีลากรานจ์}\cite{ChongZak2ndEd}
จุดประสงค์และข้อจำกัดที่ระบุด้วยนิพจน์~\ref{eq: svm primal separable}
จะแทนด้วยลากรานจ์ฟังก์ชัน
\begin{eqnarray}
J(\bm{w}, b, \bm{\alpha}) = \frac{1}{2} \bm{w}^T \bm{w} 
- \sum_{i=1}^N \alpha_i \cdot \left( y_i \cdot (\bm{w}^T \bm{z}_i + b)- 1\right)
\label{eq: svm primal lagrange objective}
\end{eqnarray}
เมื่อ $\bm{\alpha} = [\alpha_1, \ldots, \alpha_N]^T$
เป็นลากรานจ์พารามิเตอร์ โดย $\alpha_i \geq 0$ สำหรับ $i=1, \ldots, N$.

จาก\textit{ทฤษฎีบทคารูชคุนทักเกอร์} (ดูแบบฝึกหัด~\ref{ex: opt kkt})
ที่กล่าวว่า 
หากกำหนดให้ $\bm{w}_o$ และ $b_o$ แทนชุดค่าพารามิเตอร์ที่ดีที่สุด (\textit{ค่าทำให้น้อยที่สุด}) แล้ว
ณ จุดที่ดีที่สุด  เงืื่อนไขต่อไปนี้จะต้องเป็นจริง.
เงื่อนไขที่หนึ่ง คือ $\alpha_i \geq 0$ สำหรับทุก ๆ ค่าของ $i$.
เงื่อนไขที่สอง คือ
\begin{eqnarray}
\nabla_{\bm{w}} J (\bm{w}_o, b_o, \bm{\alpha}) = 0
\nonumber \\
\nabla_{b} J(\bm{w}_o, b_o, \bm{\alpha}) = 0
\nonumber 
\end{eqnarray}
ซึ่งเมื่อหาอนุพันธ์และแก้สมการแล้วจะได้ว่า
\begin{eqnarray}
\bm{w}_o &=& \sum_{i=1}^N \alpha_i y_i \bm{z}_i
\label{eq: svm w} \\
\sum_{i=1}^N \alpha_i y_i &=& 0
\label{eq: svm dual eq constraint}
\end{eqnarray}
และเงื่อนไขที่สาม คือ
\[
\sum_{i=1}^N \alpha_i \cdot \left( y_i (\bm{w}_o^T \bm{z}_i + b_o) - 1 \right) = 0.
\]
ดังนั่น เมื่อพิจารณาเงื่อนไขที่หนึ่งกับเงื่อนไขที่สามแล้วจะพบว่า
ณ จุดที่ดีที่สุด
ถ้า $\alpha_i > 0$ แล้ว เรารู้แน่ ๆ เลยว่า $y_i (\bm{w}_o^T \bm{z}_i + b_o) - 1 = 0$.
นั่นคือข้อจำกัด\textit{ทำงาน}
ซึ่งหมายถึง จุดข้อมูลที่ $i^{th}$ เป็นซัพพอร์ตเวกเตอร์.
%ระวังว่า ถ้า $\alpha_i = 0$ แล้ว $y_i (\bm{w}^T \bm{z}_i + b) - 1$ อาจจะเป็นศูนย์ หรือมากกว่าศูนย์ก็ได้.

กำหนดให้ $J'$ แทนลากรานจ์ฟังก์ชัน เมื่อใช้ชุดค่าพารามิเตอร์ที่ดีที่สุด (ใช้ $\bm{w}_o$ และ $b_o$)
พร้อมแทนค่าจากสมการ~\ref{eq: svm w} และ~\ref{eq: svm dual eq constraint}
จะได้
\[
J'(\bm{\alpha}) = \sum_{i=1}^N \alpha_i 
   - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j \bm{z}^T_i \bm{z}_j
\]
เมื่อ $\alpha \geq 0$ และ $\sum_{i=1}^N \alpha_i y_i = 0$.
กำหนดให้ \textbf{ฟังก์ชันเคอร์เนล}
$k(\bm{x}_i, \bm{x}_j) = \phi(\bm{x})^T_i \phi(\bm{x})_j$
$=\bm{z}^T_i \bm{z}_j$. 
\index{thai}{ฟังก์ชันเคอร์เนล}
\index{english}{kernel}

ดังนั้น\textit{ปัญหาคู่} สามารถระบุได้เป็น
\begin{eqnarray}
\underset{\bm{\alpha}}{\mathrm{maximize}} &  \sum_{i=1}^N \alpha_i 
- \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j k(\bm{x}_i, \bm{x}_j) 
\nonumber \\
\mbox{s.t.} & 
\nonumber \\
%& \sum_{i=1}^N \alpha_i y_i = 0 \mbox{ for } i =1, \ldots, N 
%\nonumber \\
%& \alpha \geq 0 \mbox{ for } i =1, \ldots, N
&
\begin{array}{ll}
\sum_{i=1}^N \alpha_i y_i = 0, &  \\
\alpha \geq 0 & \mbox{ for } i =1, \ldots, N.
\end{array}
\label{eq: svm dual separable}
\end{eqnarray}
สังเกตว่า 
(1) ปัญหาปฐมเป็นปัญหาค่าน้อยที่สุด แต่ปัญหาคู่เป็นปัญหาค่ามากที่สุด%
\footnote{%
พิจารณาฟังก์ชันจุดประสงค์ของปัญหาคู่ จะเห็นว่าฟังก์ชันจุดประสงค์ของปัญหาคู่ ได้รับอิทธิพลส่วนหนึ่งมาจากข้อจำกัดในปัญหาปฐม.
ปัญหาปฐม ต้องการหาค่าทำน้อยที่สุด ภายใต้ข้อจำกัด.
ปัญหาคู่
% มองข้อจำกัดของปัญหาปฐมเป็นจุดประสงค์ และ
ต้องการหาค่าทำมากที่สุด เพื่อจะรักษาข้อจำกัดปฐมไว้ได้โดยไม่ทำร้ายจุดประสงค์ปฐม.
}.
(2) ปัญหาคู่อยู่ในรูปของตัวแปร $\bm{\alpha}$ เท่านั้น ไม่มี $\bm{w}$ ไม่มี $b$.


หาก $\bm{\alpha}^\ast$ เป็นลากรานจ์พารามิเตอร์ที่ดีที่สุดที่หาได้มา
แล้วการทำนายกลุ่มของจุดข้อมูล $\bm{x}$
สามารถคำนวณได้โดย\textit{ค่าฟังก์ชันแบ่งแยก} $f(\phi(\bm{x})) = \bm{w}_o^T \phi(\bm{x}) + b_o$.
เพื่อความสะดวก นิยาม $g(\bm{x}) =  \bm{w}_o^T \phi(\bm{x}) + b_o$.
เมื่อแทนค่าสมการ~\ref{eq: svm w} เข้าไปแล้วจะได้
\begin{eqnarray}
g(\bm{x}) &=& \sum_{i=1}^N \alpha^\ast_i y_i \phi(\bm{x}_i)^T \phi(\bm{x}) + b_o
\nonumber \\
&=& \sum_{i=1}^N \alpha^\ast_i y_i k(\bm{x}_i,\bm{x}) + b_o
\label{eq: svm score function}
\end{eqnarray}
เมื่อ $\bm{x}_i, y_i$ คือจุดข้อมูลฝึก.

เนื่องจากตัวแปร $\alpha^\ast_i$ ทำหน้าที่ของลากรานจ์พารามิเตอร์
ดังนั้น สำหรับข้อจำกัดที่ไม่ได้\textit{ทำงาน} 
ซึ่งสัมพันธ์กับจุดข้อมูลที่อยู่ลึกลงไปในกลุ่ม ไม่ได้อยู่บริเวณ\textit{ขอบเขตของการแบ่ง}
ไม่ใช่ซัพพอร์ตเวกเตอร์ 
ค่า $\alpha^\ast_i$ ของจุดข้อมูลเหล่านั้นจะเป็นศูนย์ (เงื่อนไขที่สามและที่หนึ่งของคารูชคุนทักเกอร์).
สำหรับ $\alpha^\ast_i = 0$ ไม่ได้ส่งผลต่อการคำนวณสมการ~\ref{eq: svm score function} เลย.
ดังนั้น การใช้ซัพพอร์ตเวกเตอร์แมชชีนอนุมานกลุ่ม จึงไม่จำเป็นต้องใช้ข้อมูลทุกตัว
ใช้เฉพาะซัพพอร์ตเวกเตอร์ก็พอ.
นั่นคือ ค่าการอนุมานกลุ่ม คำนวณจาก
\begin{eqnarray}
g(\bm{x}) 
&=& \sum_{i \in S} \alpha^\ast_i y_i k(\bm{x}_i,\bm{x}) + b_o
\label{eq: svm discriminant function support vectors}
\end{eqnarray}
เมื่อ $S$ คือ 
เซตของดัชนีของซัพพอร์ตเวกเตอร์
นั่นคือ
$S = \{i: \alpha^\ast_i > 0\}$.

สำหรับค่าของพารามิเตอร์ $b_o$ พิจารณาจากซัพพอร์ตเวกเตอร์ ($i \in S$)
ที่มี $\alpha^\ast_i > 0$.
จากทฤษฎีบทของคารูชคุนทักเกอร์ทำให้รู้ว่า
ถ้า $\alpha^\ast_i > 0$ หมายถึง เงื่อนไข $y_i (\bm{w}_o^T \bm{z}_i + b_o) \geq 0$ ทำงาน.
นั่นคือ
สำหรับ $i \in S$ แล้ว
\begin{eqnarray}
y_i \cdot g(\bm{x}_i) &=& 1
\label{eq: svm active constraints for b} \\
y_i \cdot (\sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i) + b_o) &=& 1
\label{eq: svm solve for b}.
\end{eqnarray}
เมื่อคูณ $y_i$ เข้าไปทั้งสองข้าง (ซึ่ง $y_i^2 = 1$) และจัดรูปใหม่จะได้
\begin{eqnarray}
b_o &=& y_i - \sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i)
\nonumber
\end{eqnarray}
การคำนวณ อาจสุ่มเลือกดัชนี $i$ ของซัพพอร์ตเวกเตอร์ขึ้นมาหนึ่งตัว
หรือที่นิยม\cite{Haykin2009a} คือการใช้ค่าเฉลี่ย
\begin{eqnarray}
b_o &=& \frac{1}{|S|} \sum_{i \in S} \left( y_i - \sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i) \right)
\label{eq: svm b}
\end{eqnarray}
เมื่อ $|S|$ คือจำนวนของซัพพอร์ตเวกเตอร์.

ตัวอย่างผลลัพธ์จากการฝึกซัพพอร์ตเวกเตอร์แมชชีน แสดงในรูป~\ref{fig: discriminant function}.
ภาพซ้ายเป็นค่า $\bm{\alpha}$ ที่ได้จากการฝึก.
ภาพขวาแสดงค่าฟังก์ชันแบ่งแยก ที่คำนวณจากสมการ~\ref{eq: svm discriminant function support vectors}.
ซัพพอร์ตเวกเตอร์ คือจุดข้อมูลที่เน้น ดังแสดงในภาพขวา ซึ่งระบุได้จากค่า $\alpha_i$ ที่สัมพันธ์กับมันมีค่ามากกว่าศูนย์.

\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
		\includegraphics[width=0.48\columnwidth]{04Classic/svm/OptimizedAlphas_wSV.png}
		&
		\includegraphics[width=0.48\columnwidth]{04Classic/svm/separable_dual_discriminant_wSV.png}
		\end{tabular}
	\end{center}
	\caption[ผลลัพธ์ของซัพพอร์ตเวกเตอร์แมชชีน]{ผลลัพธ์ของซัพพอร์ตเวกเตอร์แมชชีน.
		ภาพซ้าย แสดงค่า $\bm{\alpha}$ ที่ได้จากการฝึก.
		$\alpha_i > 0$ เน้นด้วยสีฟ้าเขียวรอบ ๆ.
		เส้นประสีดำ แสดงแนวของค่าศูนย์.
		ภาพขวา แสดงค่าฟังก์ชันแบ่งแยกของซัพพอร์ตเวกเตอร์แมชชีนในปริภูมิลักษณะสำคัญ.
		ค่าฟังก์ชันแบ่งแยกเป็นค่าต่อเนื่อง แต่ในภาพค่าฟังก์ชันแบ่งแยกแสดงด้วยระดับสีเทา $18$ ระดับ ซึ่งค่าระบุด้วยแถบสีด้านข้าง.
		เส้นประสีม่วง สีน้ำเงิน และสีฟ้าเขียว แสดงแนวที่ค่าฟังก์ชันแบ่งแยกเป็น $-1$, $0$, และ $1$ ตามลำดับ.		
		จุดข้อมูลฝึก แสดงด้วยวงกลมสีเขียว (กลุ่มบวก) และดาวสีแดง (กลุ่มลบ).
		จุดข้อมูลฝึกที่ถูกเลือกเป็นซัพพอร์ตเวกเตอร์ เน้นด้วยสีเหลืองรอบ ๆ.
	}
	\label{fig: discriminant function}
\end{figure}




\subsection{สถานการณ์ที่ไม่สามารถแบ่งแยกกลุ่มได้สมบูรณ์}
\label{sec: svm non-separable problem}

ในทางปฏิบัติ การแจกแจงของกลุ่มข้อมูลอาจจะทำให้บริเวณของข้อมูลมีการซ้อนทับกันได้.
สมมติฐานการแบ่งแยกได้อย่างสมบูรณ์ อาจจะทำให้ได้แบบจำลองที่\textit{การโอเวอร์ฟิต} ขาด\textit{คุณสมบัติความทั่วไป}.
ดังนั้น เพื่อผ่อนสมมติฐานการแบ่งแยกได้อย่างสมบูรณ์
ควรจะยอมให้มีบางจุดข้อมูลที่อาจล้ำเข้าไปใน\textit{ขอบเขตของการแบ่ง}บ้าง หรือแม้แต่ยอมให้มีบางจุดข้อมูลที่ถูกจำแนกกลุ่มผิดบ้าง.

ซัพพอร์ตเวกเตอร์แมชชีนผ่อนปรน\textit{สมมติฐานการแบ่งแยกสมบูรณ์}ลง
ด้วยการผ่อนปรนข้อจำกัดของ\textit{ขอบเขตของการแบ่ง} (อสมการ~\ref{eq: svm margin constraints})
ผ่านกลไกของ\textit{ตัวแปรช่วย} $\xi_i$ เป็น
\begin{eqnarray}
\bm{w}^T \bm{z}_i + b & \geq +1 - \xi_i & \mbox{ สำหรับ } y_i = +1 
\nonumber \\
\bm{w}^T \bm{z}_i + b & \leq -1 + \xi_i & \mbox{ สำหรับ } y_i = -1 
\label{eq: svm margin xi constraints}
\end{eqnarray}
โดย $\xi_i \geq 0$
สำหรับ $i = 1, \ldots, N$ เมื่อ $N$ เป็นจำนวนข้อมูลฝึก.

ดังนั้น ถ้า $\xi_i = 0$ หมายถึง จุดข้อมูลที่ $i^{th}$ จะอยู่ห่างอภิระนาบออกมาทางกลุ่มที่ถูกต้อง และอยู่นอก\textit{ขอบเขตของการแบ่ง}.
รูป~\ref{fig: non-linearly-separable data} แสดงตัวอย่างต่าง ๆ เมื่อผ่อนปรนเงื่อนไขแบ่งแยกสมบูรณ์ลง.
โดยในรูป จุด b, จุด c, และจุด f จะมี $\xi_b, \xi_c, \xi_f = 0$.
ถ้า $\xi_i > 0$ หมายถึง จุดข้อมูลที่ $i^{th}$ อยู่ล้ำแนวของ\textit{ขอบเขตของการแบ่ง}ออกไป.
ในรูป จุด a, จุด d, และจุด e จะมี $\xi_a, \xi_d, \xi_e > 0$.

พิจารณากรณี ที่ $0 < \xi_i < 1$ นั่นคือ จุดข้อมูลอยู่ล้ำแนวของ\textit{ขอบเขตของการแบ่ง}ออกไป
แต่ยังไม่ถึงอภิระนาบ
เช่น จุด e ในรูป จะมี $0 < \xi_e < 1$. 
และเนื่องจากจุดข้อมูลยังอยู่ฝั่งของกลุ่มอยู่ จุดข้อมูลที่มี $0 < \xi_i < 1$ จะยังถูกจำแนกได้ถูกต้อง.
กรณีที่ $\xi_i = 1$ นั่นคือ จุดข้อมูลอยู่ล้ำแนวของ\textit{ขอบเขตของการแบ่ง}ออกไป
และไปอยู่บนอภิระนาบพอดี
เช่น ในรูป $\xi_d = 1$.
กรณีที่ $\xi_i > 1$
นั่นคือ จุดข้อมูลอยู่ล้ำแนวของ\textit{ขอบเขตของการแบ่ง}ออกไปมาก
มากจนเลยแนวของอภิระนาบ ข้ามไปอยู่อีกฝั่งของการจำแนก
ดังนั้น จุดข้อมูลจะถูกจำแนกผิด
เช่น ในรูป $\xi_a > 1$.

เงื่อนไขในอสมการ~\ref{eq: svm margin xi constraints} เขียนให้กระชับขึ้นเป็น
\begin{eqnarray}
y_i \left( \bm{w}^T \bm{z}_i + b \right) &\geq 1 - \xi_i & \mbox{ สำหรับ } i = 1, \ldots, N
\label{eq: svm margin xi concise}
\end{eqnarray}
เมื่อ $\xi_i \geq 0$.
การปรับเงื่อนไขนี้ เปรียบเสมือนการปรับจากข้อจำกัดที่เข้มงวด ผ่อนปรนลงมาเป็นข้อจำกัดที่อ่อนลง.
กรอบปัญหา จึงถูกวางใหม่เป็น
\begin{eqnarray}
\underset{\bm{w}, b, \bm{\xi}}{\mathrm{minimize}} &  \frac{1}{2} \bm{w}^T \bm{w} + C \sum_{i=1}^N \xi_i
\nonumber \\
\mbox{s.t.} & 
%\nonumber \\
%&
\begin{array}{ll}
y_i \left( \bm{w}^T \phi(\bm{x}_i) + b \right) \geq 1 - \xi_i & \mbox{ for } i =1, \ldots, N
\end{array}
\label{eq: csvm primal}
\end{eqnarray}
เมื่อ $C > 0$.
อภิมานพารามิเตอร์ $C$ เป็นเหมือนค่าที่ใช้ควบคุมความเข้มงวดของข้อจำกัด.
ค่า $C$ ที่เล็กจะยอมให้มีการจำแนกผิดได้มากขึ้น ในขณะที่ค่า $C$ ใหญ่จะบังคับให้แบบจำลองจำแนกผิดให้น้อยลง.

\begin{figure}
	\begin{center}
	\includegraphics[width=0.9\textwidth]{04Classic/svm/nonseparable.png}
	\end{center}
	\caption[ข้อมูลที่ไม่สามารถแบ่งแยกกลุ่มได้สมบูรณ์]{ข้อมูลที่ไม่สามารถแบ่งแยกกลุ่มได้สมบูรณ์.
	ภาพซ้าย แสดงจุดข้อมูลต่าง ๆ ในปริภูมิลักษณะสำคัญ.
	จุดข้อมูลไม่สามารถถูกแบ่งแยกกลุ่มได้อย่างสมบูรณ์ด้วยอภิระนาบ.
	วงกลมสีเขียว แทนจุดข้อมูลของกลุ่มบวก.
	ดาวสีแดง แทนจุดข้อมูลของกลุ่มลบ.
	ภาพขวา แสดงจุดข้อมูลกับบริเวณของการแบ่งต่าง ๆ.
	เส้นทึบสีน้ำเงิน แทนอภิระนาบ.
	เส้นประสีฟ้าเขียว แทนแนวของ\textit{ขอบเขตของการแบ่ง}.
	บริเวณพื้นหลังสีเขียวอ่อน แทนบริเวณของกลุ่มบวก.
	บริเวณพื้นหลังสีเขียวมะกอก แทนบริเวณที่อยู่ภายใน\textit{ขอบเขตของการแบ่ง}ฝั่งกลุ่มบวก.
	บริเวณพื้นหลังสีชมพูอ่อน แทนบริเวณของกลุ่มลบ.
	บริเวณพื้นหลังสีม่วง แทนบริเวณที่อยู่ภายใน\textit{ขอบเขตของการแบ่ง}ฝั่งกลุ่มลบ.
	จุดข้อมูล a เป็นจุดข้อมูลกลุ่มบวก ที่ตำแหน่งไปอยู่ในฝั่งของกลุ่มลบ จุดนี้จะถูกจำแนกผิดเป็นกลุ่มลบ.
	จุดข้อมูล b อยู่พอดีบนแนวของ\textit{ขอบเขตของการแบ่ง}ฝั่งกลุ่มบวก.
	จุดข้อมูล c และ f อยู่ลึกลงไปในบริเวณของกลุ่มบวก.
	จุดข้อมูล d อยู่พอดีบนอภิระนาบแบ่ง.
	จุดข้อมูล e อยู่ล้ำแนวออกมาจนเข้าไปอยู่ใน\textit{ขอบเขตของการแบ่ง} แต่ยังอยู่ในฝั่งของกลุ่มบวก.
	}
	\label{fig: non-linearly-separable data}
\end{figure}

\paragraph{ปัญหาคู่.}
ในทำนองเดียวกัน
\textit{ลากรานจ์ฟังก์ชัน}ของ\textit{ปัญหาปฐม} (นิพจน์~\ref{eq: csvm primal})
คือ
\[
J(\bm{w}, b, \bm{\xi}, \bm{\alpha})
= \frac{1}{2} \bm{w}^T \bm{w} + C \sum_{i=1}^N \xi_i
- \sum_{i=1}^N \alpha_i \cdot \left(y_i \left( \bm{w}^T \phi(\bm{x}_i) + b \right) - 1 + \xi_i\right)
- \sum_{i=1}^N \beta_i \xi_i
\]
โดย $\alpha_i, \beta_i \geq 0$.
ทั้ง $\alpha_i$ และ $\beta_i$ เป็นลากรานจ์พารามิเตอร์.

จากทฤษฎีบทคารูชคุนทักเกอร์
ณ จุดที่ดีที่สุด
ค่าพารามิเตอร์ที่ดีที่สุด $\bm{w}_o, b, \bm{\xi}_o$ จะทำให้เงื่อนไขดังนี้เป็นจริง.
เงื่อนไขที่หนึ่ง $\alpha_i \geq 0$ และ $\beta_i \geq 0$.
เงื่อนไขที่สอง
\begin{eqnarray}
\nabla_{\bm{w}} J (\bm{w}_o, b, \bm{\xi}_o) = 0
\nonumber \\
\nabla_{b} J (\bm{w}_o, b, \bm{\xi}_o) = 0
\nonumber \\
\nabla_{\bm{\xi}} J (\bm{w}_o, b, \bm{\xi}_o) = 0
\nonumber .
\end{eqnarray}
หลังจากหาอนุพันธ์และแก้สมการแล้ว 
สรุปได้ว่า
\begin{eqnarray}
\bm{w}_o &=& \sum_{i=1}^N \alpha_i y_i \phi(\bm{x}_i) 
\label{eq: kkt2 w} \\
\sum_{i=1}^N \alpha_i y_i &=& 0 
\label{eq: kkt2 b} \\
C - \alpha_i - \beta_i &=& 0 \mbox{ for } i =1, \ldots, N
\label{eq: kkt2 xi}.
\end{eqnarray}
สมการ~\ref{eq: kkt2 xi} 
คือ $\beta_i = C - \alpha_i$.
แทนค่าเหล่านี้ เข้าไปใน\textit{ลากรานจ์ฟังก์ชัน}แล้ว
\textit{ลากรานจ์ฟังก์ชัน} ณ จุดที่ดีที่สุด $J'$ สามารถเขียนได้ว่า
\begin{eqnarray}
J'(\bm{\alpha}) &=& \sum_{i=1}^N \alpha_i 
-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j k(\bm{x}_i, \bm{x}_j)
\label{eq: csvm J'}
\end{eqnarray}
เมื่อ
$\sum_{i=1}^N \alpha_i y_i = 0$
และ
$\alpha_i \geq 0$
กับ $\beta_i \geq 0$.
แต่ $\beta_i \geq 0$ เทียบเท่ากับ $\alpha_i \leq C$.
ดังนั้น\textit{ปัญหาคู่}สามารถระบุได้เป็น
\begin{eqnarray}
\underset{\bm{\alpha}}{\mathrm{maximize}} &  \sum_{i=1}^N \alpha_i 
- \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j k(\bm{x}_i, \bm{x}_j) 
\nonumber \\
\mbox{s.t.} & 
\nonumber \\
%& \sum_{i=1}^N \alpha_i y_i = 0 \mbox{ for } i =1, \ldots, N 
%\nonumber \\
%& \alpha \geq 0 \mbox{ for } i =1, \ldots, N
&
\begin{array}{ll}
\sum_{i=1}^N \alpha_i y_i = 0, &  \\
0 \leq \alpha_i \leq C & \mbox{ for } i =1, \ldots, N.
\end{array}
\label{eq: svm dual}
\end{eqnarray}
สังเกตุว่า
ปัญหาคู่สำหรับกรณีทั่วไป แทบจะเหมือนกับปัญหาคู่กรณีข้อมูลแบ่งแยกได้โดยสมบูรณ์เลย
ต่างกันเพียงแต่เงื่อนไขของค่า $\alpha_i$ 
ที่เปลี่ยนมาเป็น $0 \leq \alpha_i \leq C$.
%หมายเหตุ 
%ถ้า $\alpha_i < C$ นั่นคือ $\beta_i > 0$ ซึ่งจากคารูชคุนทักเกอร์ ทำให้รู้ว่า $\xi_i = 0$.
%แต่ $\alpha_i = C$ นั่นคือ $\beta_i = 0$ แต่ $\xi_i$ อาจจะ $>0$ หรือ $=0$ ก็ได้.

การอนุมานค่า\textit{ฟังก์ชันแบ่งแยก}ก็ทำได้โดยการคำนวณสมการ~\ref{eq: svm score function} เช่นเดิม.
และค่าพารามิเตอร์ $b_o$ 
%ก็พิจารณาจาก $y_i \cdot g(\bm{x}_i) = 1$ (เช่นเดิม สมการ~\ref{eq: svm active constraints for b})
ในกรณีที่ข้อมูลไม่สามารถแบ่งแยกได้สมบูรณ์ 
สามารถพิจารณาจาก
เงื่อนไข $y_i \left( \bm{w}^T \bm{z}_i + b \right) \geq 1 - \xi_i$ (อสมการ~\ref{eq: svm margin xi concise})
ที่เมื่อ $\alpha_i > 0$ แล้ว เงื่อนไขจะ\textit{ทำงาน}. 
นั่นคือ $y_i \left( \bm{w}^T \bm{z}_i + b \right) = 1 - \xi_i$.
แต่เรายังไม่สามารถแก้สมการนี้ได้ เพราะเรายังไม่รู้ค่า $\xi_i$.
อย่างไรก็ตาม จากการที่ $\xi_i \geq 0$ เป็นเงื่อนไข ที่ควบคุมด้วยลากรานจ์พารามิิเตอร์ $\beta_i$.
นั่นคือ เรารู้ว่า เมื่อ $\beta_i > 0$ (เทียบเท่า $\alpha_i < C$) แล้ว เงื่อนไขจะ\textit{ทำงาน} ซึ่งคือ $\xi_i = 0$.
ดังนั้น จุดข้อมูลที่ $0 < \alpha_i < C$ จะบอกได้ว่า $y_i \left( \bm{w}^T \bm{z}_i + b \right) = 1$
ซึ่งเราสามารถใช้จุดข้อมูลเหล่านี้ แก้สมการหาค่า $b_o$ ได้.
นั่นคือ
\begin{eqnarray}
y_i \cdot g(\bm{x}_i) &=& 1  \mbox{ เมื่อ } i \in \{j: 0 < \alpha_j < C \}
\label{eq: non-sep for b} \\
\sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i) + b_o &=& y_i 
\nonumber \\
b_o &=& y_i - \sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i)
\label{eq: svm b general}.
\end{eqnarray}
เช่นเดียวกัน $i$ อาจเลือกจากดัชนีหนึ่ง ซึ่งทำให้ $0 < \alpha_i < C$ 
หรือ อาจใช้ค่าเฉลี่ย ซึ่งคือ
\begin{eqnarray}
b_o &=& \frac{1}{|S'|} \sum_{i \in S'} \left( y_i - \sum_{j \in S} \alpha^\ast_j y_j k(\bm{x}_j,\bm{x}_i) \right)
\label{eq: svm bo}
\end{eqnarray}
เมื่อ $S = \{i: \alpha_i > 0\}$ และ $S' = \{i: 0 < \alpha_i < C\}$ เป็นเซตดัชนีของซัพพอร์ตเวกเตอร์ และซัพพอร์ตเวกเตอร์ที่แนว\textit{ขอบเขตของการแบ่ง} ตามลำดับ.
หมายเหตุ $S$ มาจากสมการ~\ref{eq: kkt2 w} ที่คำนวณทุกตัว แต่ $\alpha_i = 0$ ไม่มีผล ดังนั้นจึงเลือกเฉพาะที่ $\alpha_i > 0$ มาคำนวณ เพื่อลดการคำนวณที่ไม่จำเป็นและลดข้อมูล $(\bm{x}_i, y_i)$ ที่ต้องเก็บรักษาไว้.
ส่วน $S'$ มาจากทฤษฎีบทคารูชคุนทักเกอร์ที่ทำให้อสมการ~\ref{eq: svm margin xi concise} เปลี่ยนมาอยู่ในรูปสมการ~\ref{eq: non-sep for b} เพื่อทำให้สามารถคำนวณค่า $b_o$ ได้.

รูป~\ref{fig: csvm results} แสดงค่า $\alpha_i$ ต่าง ๆ ที่ฝึกเสร็จ (ภาพซ้าย)
และค่าฟังก์ชันแบ่งแยก (ภาพขวา).
รูป~\ref{fig: csvm different Cs} แสดงตัวอย่างของพฤติกรรมการจำแนกของแบบจำลอง
เมื่อเลือกค่า $C$ ต่าง ๆ.
สังเกตุว่า ที่ค่า $C$ ขนาดเล็ก จะเห็น\textit{ขอบเขตของการแบ่ง}กว้าง 
และมีจุดข้อมูลล้ำแนว\textit{ขอบเขตของการแบ่ง}จำนวนมาก.
ที่ค่า $C$ ขนาดใหญ่ ฟังก์ชันแบ่งแยกจะปรับการคำนวณ 
เพื่อให้จุดข้อมูลล้ำแนว\textit{ขอบเขตของการแบ่ง}ออกไปน้อยลง แต่ก็ทำให้\textit{ขอบเขตของการแบ่ง}แคบลง.

\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
			\includegraphics[width=0.45\columnwidth]{04Classic/svm/cSVM_alpha.png}
			&
			\includegraphics[width=0.45\columnwidth]{04Classic/svm/cSVM_sv.png}
		\end{tabular}				
	\end{center}
	\caption[ผลการฝึกซัพพอร์ตเวกเตอร์แมชชีนกรณีทั่วไป]{ผลการฝึกซัพพอร์ตเวกเตอร์แมชชีนกรณีทั่วไป.
ภาพซ้าย แสดงค่า $\alpha_i$ ที่ดัชนี $i$ ต่าง ๆ.
เส้นประสีดำ และเส้นประสีแดง แสดงแนวศูนย์ และแนวค่า $C$ ซึ่งเป็นขอบของช่วงค่าที่อนุญาตสำหรับ $\alpha_i$.
ค่า $\alpha_i > 0$ เน้นด้วยสีเหลือง
และค่า $\alpha_i = C$ เน้นด้วยขอบสีดำอีกที.
ภาพขวา แสดงจุดข้อมูลและค่าฟังก์ชันแบ่งแยกในปริภูมิลักษณะสำคัญ.
ซัพพอร์ตเวกเตอร์ ($\alpha_i > 0$) เน้นด้วยสีเหลือง
และซัพพอร์ตเวกเตอร์ที่มี $\alpha_i = C$ เน้นด้วยขอบสีดำอีกที.
	}
	\label{fig: csvm results}
\end{figure}

\begin{figure}
	\begin{center}
	\begin{tabular}{cc}
	\includegraphics[width=0.45\columnwidth]{04Classic/svm/C0p1.png}
	&
	\includegraphics[width=0.45\columnwidth]{04Classic/svm/C1.png}
\\
	\includegraphics[width=0.45\columnwidth]{04Classic/svm/C10.png}
	&
	\includegraphics[width=0.45\columnwidth]{04Classic/svm/C100.png}	
	\end{tabular}				
	\end{center}
	\caption[พฤติกรรมของซัพพอร์ตเวกเตอร์แมชชีนที่ค่า $C$ ต่าง ๆ]{พฤติกรรมของซัพพอร์ตเวกเตอร์แมชชีนที่ค่า $C$ ต่าง ๆ.
แต่ละภาพ 
แสดง จุดข้อมูล (วงกลมสีเขียว แทนจุดข้อมูลกลุ่มบวก. ดาวสีแดง แทนจุดข้อมูลกลุ่มลบ)
ค่าฟังก์ชันแบ่งแยก (สีพื้น ที่แสดงในระบบระดับสีเทา โดยค่าของสีแสดงด้วยแถบสีด้านข้าง) 
อภิระนาบ (เส้นประสีน้ำเงิน) และแนวขอบเขตของการแบ่ง (เส้นประสีม่วงอ่อน สำหรับแนวฝั่งลบ และเส้นประสีฟ้าเขียว สำหรับแนวฝั่งบวก).
ค่าพารามิเตอร์ $C$ แสดงอยู่เหนือภาพ.
	}
	\label{fig: csvm different Cs}
\end{figure}

นอกจาก
ซัพพอร์ตเวกเตอร์แมชชีนในรูปแบบดั้งเดิม ที่ระบุด้วยนิพจน์~\ref{eq: svm dual} 
ยังมีรูปแบบที่พัฒนาขึ้นมาใหม่อีก
เช่น \textit{นูซัพพอร์ตเวกเตอร์แมชชีน} ($\nu$-SVM\cite{SchoelkopfEtAl2000a})
ที่วางกรอบปัญหาเป็น
\begin{eqnarray}
\underset{\bm{\alpha}}{\mathrm{maximize}} &  
- \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j k(\bm{x}_i, \bm{x}_j) 
\nonumber \\
\mbox{s.t.} & 
\nonumber \\
%& \sum_{i=1}^N \alpha_i y_i = 0 \mbox{ for } i =1, \ldots, N 
%\nonumber \\
%& \alpha \geq 0 \mbox{ for } i =1, \ldots, N
&
\begin{array}{ll}
\sum_{i=1}^N \alpha_i y_i = 0, &  \\
\sum_{i=1}^N \alpha_i \geq \nu , & \\
0 \leq \alpha_i \leq 1/N & \mbox{ for } i =1, \ldots, N.
\end{array}
\label{eq: nu-svm}
\end{eqnarray}
โดย $\nu$ เป็นอภิมานพารามิเตอร์ แทน $C$ ในนิพจน์~\ref{eq: svm dual}.

\subsection{ฟังก์ชันเคอร์เนล}
\label{sec: svm kernel function}

ซัพพอร์ตเวกเตอร์แมชชีน จัดการการคำนวณอย่างสละสลวยที่ในการฝึก (นิพจน์~\ref{eq: svm dual} และสมการ~\ref{eq: svm bo})
และการอนุมาน (สมการ~\ref{eq: svm discriminant function support vectors})
สามารถทำงานโดยตรงกับ\textit{ฟังก์ชันเคอร์เนล} (kernel function) $k(\bm{x}, \bm{x}')$
โดยไม่จำเป็นต้องอาศัย\textit{ฟังก์ชันลักษณะสำคัญ} $\phi(\bm{x})$.
โอกาสเช่นนี้ ทำให้การใช้งานซัพพอร์ตเวกเตอร์แมชชีน สามารถกำหนด\textit{ฟังก์ชันเคอร์เนล}
ที่เทียบเท่าการทำงานในปริภูมิลักษณะสำคัญที่มีจำนวนมิติมาก ๆ ได้ โดยไม่จำเป็นต้องเข้าไปทำงานในปริภูมิที่มีมิติสูงนั้นโดยตรง.
การใช้ประโยชน์แง่มุมนี้ มักถูกเรียกว่า \textbf{ลูกเล่นเคอร์เนล} (kernel tricks).
\index{english}{kernel tricks}
\index{thai}{ลูกเล่นเคอร์เนล}

\textit{ฟังก์ชันเคอร์เนล} อาจนิยมตรง ๆ จาก\textit{ฟังก์ชันลักษณะสำคัญ}ด้วย
\begin{eqnarray}
k(\bm{x}, \bm{x}') &=& \phi(\bm{x})^T \phi(\bm{x}') = \sum_{m=1}^M \phi_m(\bm{x}) \phi_m(\bm{x}')
\label{eq: kernel function}
\end{eqnarray}
เมื่อ $\bm{x}$ และ $\bm{x}'$ เป็นจุดข้อมูลสองจุด.
ฟังก์ชัน $\phi(\bm{x})$ เป็นฟังก์ชันลักษณะสำคัญ 
และ $\phi_m(\bm{x})$ เป็นส่วนประกอบที่ $m^{th}$ ของค่าฟังก์ชันลักษณะสำคัญ.
\textit{ฟังก์ชันเคอร์เนล} สามารถถูกออกแบบได้หลายวิธี.
วิธีหนึ่ง (1) อาจกำหนดผ่านฟังก์ชันลักษณะสำคัญ และสมการ~\ref{eq: kernel function}
อีกวิธีหนึ่งในการสร้างเคอร์เนล (2) อาจกำหนดฟังก์ชันเคอร์เนลโดยตรง โดยไม่ต้องอาศัยฟังก์ชันลักษณะสำคัญ
แต่ต้องตรวจสอบว่าฟังก์ชันที่กำหนดนั้น มีคุณสมบัติเป็นฟังก์ชันเคอร์เนลได้.
การตรวจสอบนั้น อาจจะใช้\textit{ทฤษฎีบทของเมอร์เซอร์} (Mercer's theorem ดู \cite{Haykin2009a} สำหรับรายละเอียด)
%ที่กล่าวว่า
%กำหนดให้ $k(\bm{x}, \bm{x}')$ เป็นเคอร์เนลที่ต่อเนื่องและสมมาตร ที่ถูกนิยามในช่วงปิด $\bm{x}_{\min} \leq \bm{x} \leq \bm{x}_{\max}$ และ $\bm{x}_{\min} \leq \bm{x}' \leq \bm{x}_{\max}$.
%\textit{ฟังก์ชันเคอร์เนล} $k(\bm{x}, \bm{x}')$ จะสามารถถูกกระจายออกในรูปอนุกรม ดังนี้
%\begin{eqnarray}
%k(\bm{x}, \bm{x}') &=& \sum_{}
%\end{eqnarray}
หรือ ใช้การตรวจ\textit{แกรมเมทริกซ์} (Gram matrix) $\bm{K} = [k_{ij}]$ สำหรับ $i,j = 1, \ldots, N$ เมื่อ $N$ เป็นจำนวนจุดข้อมูลฝึก (หรือจำนวนซัพพอร์ตเวกเตอร์)
และ $k_{ij} = k(\bm{x}_i, \bm{x}_j)$.
ฟังก์ชันจะมีคุณสมบัติเป็น\textit{ฟังก์ชันเคอร์เนล}ได้
หาก\textit{แกรมเมทริกซ์}เป็น\textit{เมทริกซ์บวกแน่นอน}%
\footnote{%
\textit{เมทริกซ์บวกแน่นอน} ไม่ได้หมายถึง ทุกส่วนประกอบเป็นบวก.
แต่มีความหมาย ดังนิยามว่า
เมทริกซ์สมมาตร $\bm{Q}$ จะเรียกว่า \textit{บวกแน่นอน} (positive definite) 
ก็ต่อเมื่อทุก ๆ \textit{ค่าลักษณะเฉพาะ} (eigenvalues) ของ $\bm{Q}$ เป็นบวก.
เมทริกซ์สมมาตร $\bm{Q}$ จะเรียกว่า \textit{บวกกึ่งแน่นอน} (positive semidefinite) 
ก็ต่อเมื่อทุก ๆ \textit{ค่าลักษณะเฉพาะ}ของ $\bm{Q}$ เป็นบวกหรือศูนย์.
\index{english}{positive definite}
\index{english}{positive semidefinite}
\index{thai}{บวกแน่นอน}
\index{thai}{บวกกึ่งแน่นอน}
}
(positive definite matrix) สำหรับทุก ๆ ค่าที่เป็นไปได้ของ $\bm{x}$ และ $\bm{x}'$.

แต่ (3) วิธีที่สะดวกกว่าในการสร้างฟังก์ชันเคอร์เนล 
คือสร้างจากฟังก์ชันเคอร์เนลที่ถูกตรวจสอบมาแล้ว ด้วยคุณสมบัติดังนี้ (จาก \cite{Bishop2006a}).
หาก $k_1(\bm{x}, \bm{x}')$ และ $k_2(\bm{x}, \bm{x}')$ มีคุณสมบัติเป็นฟังก์ชันเคอร์เนลได้แล้ว
ฟังก์ชันต่อไปนี้ก็จะมีคุณสมบัติเป็นเคอร์เนลได้เช่นกัน 
\begin{eqnarray}
k(\bm{x}, \bm{x}') &=& c k_1(\bm{x}, \bm{x}')
\label{eq: kernel 1} \\
k(\bm{x}, \bm{x}') &=& f(\bm{x}) k_1(\bm{x}, \bm{x}') f(\bm{x}')
\label{eq: kernel 2} \\
k(\bm{x}, \bm{x}') &=& \mathrm{polynomial}^+( k_1(\bm{x}, \bm{x}') )
\label{eq: kernel 3} \\
k(\bm{x}, \bm{x}') &=& \exp( k_1(\bm{x}, \bm{x}') )
\label{eq: kernel 4} \\
k(\bm{x}, \bm{x}') &=& k_1(\bm{x}, \bm{x}') + k_2(\bm{x}, \bm{x}')
\label{eq: kernel 5} \\
k(\bm{x}, \bm{x}') &=& k_1(\bm{x}, \bm{x}') \cdot k_2(\bm{x}, \bm{x}')
\label{eq: kernel 6} \\
k(\bm{x}, \bm{x}') &=& k_3(g(\bm{x}), g(\bm{x}'))
\label{eq: kernel 7} \\
k(\bm{x}, \bm{x}') &=& \bm{x}^T \bm{A} \bm{x}'
\label{eq: kernel 8} \\
k(\bm{x}, \bm{x}') &=& k_a(\bm{x}_a, \bm{x}'_a) + k_b(\bm{x}_b, \bm{x}'_b)
\label{eq: kernel 9} \\
k(\bm{x}, \bm{x}') &=& k_a(\bm{x}_a, \bm{x}'_a) \cdot k_b(\bm{x}_b, \bm{x}'_b)
\label{eq: kernel 10}
\end{eqnarray}
เมื่อ $c > 0$ เป็นค่าคงที่.
ฟังก์ชัน $f: \mathbb{R}^D \mapsto \mathbb{R}$ เป็นฟังก์ชันใด ๆ.
ฟังก์ชัน $\mathrm{polynomial}^+$ เป็น\textit{ฟังก์ชันพหุนาม}ที่สัมประสิทธิ์ไม่มีค่าลบ.
ฟังก์ชัน  $g: \mathbb{R}^D \mapsto \mathbb{R}^M$ 
และ $k_3$ มีคุณสมบัติเป็นฟังก์ชันเคอร์เนลสำหรับ $\mathbb{R}^M$.
เมทริกซ์ $\bm{A}$ สมมาตร และเป็น\textit{บวกกึ่งแน่นอน}%
\footnote{%
ดูนิยาม \textit{บวกกึ่งแน่นอน} (positive semidefinite).
}
เวกเตอร์ $\bm{x}_a, \bm{x}'_a$ เป็นส่วนหน้าของ $\bm{x}, \bm{x}'$
และเวกเตอร์ $\bm{x}_b, \bm{x}'_b$ เป็นส่วนหน้าของ $\bm{x}, \bm{x}'$
นั่นคือ $\bm{x} = [\bm{x}_a, \bm{x}_b]^T$ 
และ $k_a$ กับ $k_b$ เป็นคุณสมบัติเป็นฟังก์ชันเคอร์เนลสำหรับปริภูมิย่อยดังกล่าว. 

ฟังก์ชันเคอร์เนลที่นิยม และแนะนำสำหรับการเริ่มต้นใช้งานซัพพอร์ตเวกเตอร์แมชชีน\cite{ChangEtAl2011a}
ได้แก่
\textit{ฟังก์ชันเคอร์เนลเชิงเส้น} 
และ\textit{ฟังก์ชันเคอร์เนลเกาส์เซียน}.

\textbf{ฟังก์ชันเคอร์เนลเชิงเส้น} (linear kernel) 
\index{english}{linear kernel}
\index{thai}{ฟังก์ชันเคอร์เนลเชิงเส้น}
\index{english}{SVM!linear kernel}
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ฟังก์ชันเคอร์เนลเชิงเส้น}
ที่นิยามเป็น 
\begin{eqnarray}
k(\bm{x}, \bm{x}') = \bm{x}^T \bm{x}'
\label{eq: svm linear kernel}
\end{eqnarray}
ซึ่งคือ ฟังก์ชันลักษณะสำคัญเป็นฟังก์ชันเอกลักษณ์ $\phi(\bm{x}) = \bm{x}$.
\textit{ฟังก์ชันเคอร์เนลเชิงเส้น} สร้างจากนิยามของเคอร์เนลในสมการ~\ref{eq: kernel function}.


\textbf{ฟังก์ชันเคอร์เนลเกาส์เซียน} (Gaussian kernel)
\index{english}{Gaussian kernel}
\index{thai}{ฟังก์ชันเคอร์เนลเกาส์เซียน}
\index{english}{SVM!Gaussian kernel}
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ฟังก์ชันเคอร์เนลเกาส์เซียน}
นิยามเป็น
\begin{eqnarray}
k(\bm{x}, \bm{x}') = \exp\left( -\frac{\|\bm{x} - \bm{x}'\|^2}{2 \sigma^2}\right)
\label{eq: svm gaussian kernel}
\end{eqnarray}
\textit{ฟังก์ชันเคอร์เนลเกาส์เซียน} อาจจะมองว่าสร้างมาจากคุณสมบัติของเคอร์เนล.
พิจารณา
\[
\|\bm{x} - \bm{x}'\|^2 = \bm{x}^T \bm{x} - 2 \bm{x}^T \bm{x}' + (\bm{x}')^T \bm{x}'
\]
ซึ่งเท่ากับว่า
\[
k(\bm{x}, \bm{x}') =  \exp \left(-\frac{1}{2 \sigma^2}\bm{x}^T \bm{x}\right) \cdot \exp \left(\frac{1}{\sigma^2} \bm{x}^T \bm{x}'\right) \cdot \exp \left(-\frac{1}{2 \sigma^2} (\bm{x}')^T \bm{x}'\right).
\]
นั่นคือ ใช้\textit{ฟังก์ชันเคอร์เนลเชิงเส้น} $\bm{x}^T \bm{x}'$ เป็นพื้นฐาน
และใช้คุณสมบัติในสมการ~\ref{eq: kernel 2},~\ref{eq: kernel 4},~และ~\ref{eq: kernel 1} ประกอบ.

รูป~\ref{fig: csvm gaussian kernels} แสดง\textit{ค่าฟังก์ชันแบ่งแยก}
เมื่อใช้\textit{ฟังก์ชันเคอร์เนลเกาส์เซียน} 
ที่ค่า $\sigma$ ต่าง ๆ.
สังเกตุว่า ในภาพเส้นค่า\textit{ค่าฟังก์ชันแบ่งแยก}เป็นศูนย์ (ซึ่งสะท้อนถึงอภิระนาบในปริภูมิลักษณะสำคัญ) สามารถโค้งเลี้ยวไปตามข้อมูลได้ในปริภูมิข้อมูล.
ภาพต่างทางขวา แสดง\textit{ขอบเขตตัดสินใจ} (decision boundary)
ซึ่งเป็นส่วนในปริภูมิข้อมูล ที่ข้อมูลที่อยู่ภายในบริเวณจะถูกตัดสินตามชนิดของ\textit{ขอบเขตตัดสินใจ}.
\index{english}{decision boundary}
\index{thai}{ขอบเขตตัดสินใจ}

รูป~\ref{fig: csvm linear kernel on wave datapoints} แสดงการใช้\textit{ฟังก์ชันเคอร์เนลเชิงเส้น}
เพื่อเปรียบเทียบ. 

\begin{figure}
	\begin{center}
		\begin{tabular}{cc}
\includegraphics[width=0.3\columnwidth]{04Classic/svm/GaussianC1s01.png}
&			
\includegraphics[width=0.3\columnwidth]{04Classic/svm/decision_boundary_gaussianC1s01.png}
\\
%\includegraphics[width=0.3\textwidth]{04Classic/svm/GaussianC1s025.png}
%&
%\includegraphics[width=0.3\textwidth]{04Classic/svm/decision_boundary_gaussianC1s025.png}
%\\
\includegraphics[width=0.3\columnwidth]{04Classic/svm/GaussianC1s05.png}
&
\includegraphics[width=0.3\columnwidth]{04Classic/svm/decision_boundary_gaussianC1s05.png}
\\
\includegraphics[width=0.3\columnwidth]{04Classic/svm/GaussianC1s1.png}
&
\includegraphics[width=0.3\columnwidth]{04Classic/svm/decision_boundary_gaussianC1s1.png}
\\
\includegraphics[width=0.3\columnwidth]{04Classic/svm/GaussianC1s2.png}
&
\includegraphics[width=0.3\columnwidth]{04Classic/svm/decision_boundary_gaussianC1s2.png}
\\
\includegraphics[width=0.3\columnwidth]{04Classic/svm/GaussianC1s4.png}
&
\includegraphics[width=0.3\columnwidth]{04Classic/svm/decision_boundary_gaussianC1s4.png}
		\end{tabular}				
	\end{center}
	\caption[ซัพพอร์ตเวกเตอร์แมชชีน ด้วยเกาส์เซียนเคอร์เนลที่ค่า $\sigma$ ต่าง ๆ]{การทำงานของซัพพอร์ตเวกเตอร์แมชชีน ด้วยเกาส์เซียนเคอร์เนลที่ค่า $\sigma$ ต่าง ๆ.
ค่า $C$ และ $\sigma$ ระบุไว้เหนือภาพ.
ภาพซ้าย แสดงค่า\textit{ฟังก์ชันแบ่งแยก}ของซัพพอร์ตเวกเตอร์แมชชีนในปริภูมิข้อมูล.
ค่า\textit{ฟังก์ชันแบ่งแยก}แสดงด้วยระดับสีเทา ซึ่งค่าระบุด้วยแถบสีด้านข้าง.
เส้นประสีม่วง สีน้ำเงิน และสีฟ้าเขียว แสดงแนวที่ค่าฟังก์ชันแบ่งแยกเป็น $-1$, $0$, และ $1$ ตามลำดับ.		
จุดข้อมูลฝึก แสดงด้วยวงกลมสีเขียว (กลุ่มบวก) และดาวสีแดง (กลุ่มลบ).		
ภาพขวา แสดงจุดข้อมูลฝึก กับ\textit{ขอบเขตตัดสินใจ}.
\textit{ขอบเขตตัดสินใจ}สำหรับกลุ่มบวก แสดงด้วยสีเขียวอ่อน.
\textit{ขอบเขตตัดสินใจ}สำหรับกลุ่มลบ แสดงด้วยสีชมพู.
	}
	\label{fig: csvm gaussian kernels}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics[width=0.3\textwidth]{04Classic/svm/linearC10.png}
	\end{center}
	\caption[ซัพพอร์ตเวกเตอร์แมชชีน ด้วยเคอร์เนลเชิงเส้น เพื่อเปรียบเทียบกับเกาส์เซียน]{การทำงานของซัพพอร์ตเวกเตอร์แมชชีน ด้วยเคอร์เนลเชิงเส้น เพื่อเปรียบเทียบกับเกาส์เซียนในรูป~\ref{fig: csvm gaussian kernels}.}
	\label{fig: csvm linear kernel on wave datapoints}
\end{figure}





%LATER
%\section{การวิเคราะห์องค์ประกอบหลัก}
%\label{sec: PCA}
%
%\subsection{หน้าไอเกน}
%\label{sec: Eigenface}


%\section{วิธีการวิเคราะห์ส่วนประกอบหลัก}
%\label{sec: principal component analysis}
%
%
%\subsection{หน้าไอเกน}
%\label{sec: eigenface}

%\section{วิธีการแบ่งกลุ่มแบบเคมีนส์}
%\label{sec: K-means}
%
%\subsection{เวกเตอร์ควอนไทเซชั่น}
%\label{sec: vector quantization}

%LATER
%\section{ระบบแนะนำสินค้า}

%LATER
%\section{การเรียนรู้แบบเสริมกำลัง}


%\section{Glossary}
\section{อภิธานศัพท์}

\begin{description}
	
	\item[การตรวจหาวัตถุ (object detection):] 
	ภารกิจการหาตำแหน่งของวัตถุในภาพ หากภาพมีวัตถุอยู่.
	\index{english}{object detection}
	\index{thai}{การตรวจหาวัตถุ}

	\item[วิธีหน้าต่างเลื่อน (sliding window):] 
วิธีการเลือกส่วนภาพขนาดที่กำหนดจากข้อมูลภาพใหญ่
โดยการเลือกส่วนภาพ จะเลือกทั่วถึงจากทุกบริเวณในภาพใหญ่
ซึ่งอาจเริ่มจากมุมซ้ายบนของภาพใหญ่
เลือกส่วนภาพออกมา แล้วขยับไปทางขวา และทำเช่นนี้ไปจนสุดปลายด้านขวา
แล้วจึงขยับลงล่างและไปเริ่มจากซ้ายสุด 
และทำลักษณะเช่นนี้อีก จนครอบคลุมบริเวณทั้งภาพใหญ่.
\index{english}{sliding window}
\index{thai}{วิธีหน้าต่างเลื่อน}

	\item[ขนาดขยับเลื่อน (stride):] 
ขนาดของการเลื่อนหน้าต่างแต่ละครั้ง.
\index{english}{stride}
\index{thai}{ขนาดขยับเลื่อน}
			
	\item[การสกัดลักษณะสำคัญ (feature extraction):] 
การแปลงอินพุตดั้งเดิม ให้อยู่ในรูปแบบใหม่ โดยที่รูปแบบใหม่นี้จะช่วยให้ภาระกิจที่ต้องการดำเนินการได้สะดวกขึ้น.
\index{thai}{การสกัดลักษณะสำคัญ}
\index{english}{feature extraction}

	\item[แบบจำลองแบ่งแยก (discriminative model):] 
แบบจำลองการจำแนกกลุ่ม ที่อาศัยหรือตีความได้ว่าใช้\textit{ความน่าจะเป็นภายหลัง} $\mathrm{Pr}(y|x)$ เมื่อ $y$ เป็นค่ากลุ่มที่ต้องการทำนาย และ $x$ เป็นอินพุตหรือตัวแปรต้น.
ตัวอย่างเช่น โครงข่ายประสาทเทียม.
\index{thai}{แบบจำลองแบ่งแยก}
\index{english}{discriminative model}

	\item[แบบจำลองสร้างกำเนิด (generative model):] 
แบบจำลองการจำแนกกลุ่ม 
ที่อาศัยความน่าจะเป็น $\mathrm{Pr}(x|y)$ ทางตรงหรือทางอ้อม 
เมื่อ $x$ เป็นอินพุตหรือตัวแปรต้น และ $y$ เป็นค่ากลุ่ม.
โดยทั่วไปแล้ว $x$ จะอยู่ในปริภูมิที่มีขนาดใหญ่กว่า $y$ มาก ๆ
เช่น ปัญหาการจำแนกภาพคน โดยเป็นภาพสเกลเทาขนาด $H \times W$
ตัวแปร $\bm{x}$ อยู่ในปริภูมิ $\mathbb{R}^{H \times W}$ 
ในขณะที่ตัวแปร $\bm{y}$ อยู่ในปริภูมิ $\{+1, -1\}$.
%ปัจจุบัน แบบจำลองสร้างกำเนิดที่มีประสิทธิภาพทำได้ยากมาก 
%และยังคงเป็นหัวข้อวิจัยที่ได้รับความสนใจอย่างกว้างขวาง
%ศักยภาพของแบบจำลองสร้างกำเนิดที่มีประสิทธิภาพ จะ 
\index{thai}{แบบจำลองสร้างกำเนิด}
\index{english}{generative model}

\item[ฟังก์ชันแบ่งแยก (discriminant function):] 
แบบจำลองการจำแนกกลุ่ม ทำการคำนวณค่าเพื่อจำแนกกลุ่มโดยตรง 
ไม่อาศัยและไม่สามารถตีความในเชิงความน่าจะเป็น.
ตัวอย่างเช่น ซัพพอร์ตเวกเตอร์แมชชีน.
\index{thai}{ฟังก์ชันแบ่งแยก}
\index{english}{discriminant function}

\item[กล่องขอบเขต (bounding box):]
บริเวณสี่เหลี่ยมที่เป็นขอบเขตภายในภาพ ซึ่งแต่ละกล่องขอบเขตสามารถระบุได้ด้วยสี่ค่า 
เช่น พิกัด $(x,y)$ มุมซ้ายบน และขนาดความกว้างกับความสูงของกล่อง $(w,h)$.
\index{thai}{กล่องขอบเขต}
\index{english}{bounding box}

\item[การกำจัดการระบุซ้ำซ้อน (redundancy removal):]
กลไกที่สำคัญสำหรับการตรวจจับภาพวัตถุ 
เพื่อกำจัดการระบุการตรวจพบตั้งแต่สองอันขึ้นไป ที่จริง ๆ  แล้วระบุถึงวัตถุเดียวกัน.
\index{thai}{การกำจัดการระบุซ้ำซ้อน}
\index{english}{redundancy removal}

\item[วิธีระงับค่าไม่มากสุดท้องถิ่น (non-local-maximum suppression):]
วิธีหนึ่งใน\textit{การกำจัดการระบุซ้ำซ้อน}
ที่ดำเนินการด้วยการตัดทิ้ง\textit{กล่องขอบเขต}ที่มี\textit{ค่าความเหมาะสม}ไม่มากที่สุด เมื่อเปรียบเทียบกับ\textit{กล่องขอบเขต}อื่น ๆ ที่อยู่รอบ ๆ กล่องนั้น
โดย \textit{ค่าความเหมาะสม}
คือค่าที่ใช้วัดความมั่่นใจว่า\textit{กล่องขอบเขต}นั้นมีวัตถุที่ค้นหาอยู่.
\index{thai}{วิธีระงับค่าไม่มากสุดท้องถิ่น}
\index{english}{non-local-maximum suppression}

\item[ไอโอยู (IoU หรือ intersection of union):]
ปริมาณวัด 
ที่วัดจากสัดส่วนพื้นที่ซ้อนทับกันของ\textit{กล่องขอบเขต}สองกล่อง ต่อพื้นที่รวม 
เพื่อบอกความใกล้เคียงของตำแหน่งการตรวจจับ
อาจใช้ในกลไกของการตรวจจับ หรือใช้ในกระบวนการประเมินผล.
\index{thai}{ไอโอยู}
\index{english}{IoU}

\item[วิธีการประมาณความหนาแน่นแก่น (kernel density estimation):]
วิธีหนึ่งในการประมาณความหนาแน่นความน่าจะเป็นของข้อมูล.
\index{thai}{วิธีการประมาณความหนาแน่นแก่น}
\index{english}{kernel density estimation}

\item[ค่าเฉลี่ยค่าประมาณความเที่ยงตรง (mean Average Precision หรือ mAP):]
วิธีการวัดความสามารถของระบบตรวจจับภาพวัตถุ
ที่คำนวณจากค่าประมาณพื้นที่ใต้กราฟของค่าความเที่ยงตรงและค่าระลึกกลับของวัตถุชนิดต่าง ๆ
แล้วนำมาเฉลี่ยกัน.
\index{thai}{ค่าเฉลี่ยค่าประมาณความเที่ยงตรง}
\index{english}{mean Average Precision}
\index{english}{mAP}

\item[ปริภูมิลักษณะสำคัญ (feature space):]
ปริภูมิหรือเซตของของค่าต่าง ๆ ที่เป็นไปได้ทั้งหมดของข้อมูล ในรูปแบบที่น่าจะช่วยให้ภาระกิจที่ต้องการทำได้ง่ายขึ้น.
\index{thai}{ปริภูมิลักษณะสำคัญ}
\index{english}{feature space}

\item[ซัพพอร์ตเวกเตอร์แมชชีน (support vector machine):]
แบบจำลองจำแนกค่าทวิภาค 
ที่จำแนกข้อมูล ด้วยการใช้อภิระนาบในปริภูมิลักษณะสำคัญ.
อภิระนาบที่ใช้ ถูกเลือกมาจากอภิระนาบที่สามารถแบ่งข้อมูลตัวอย่างได้\textit{ขอบเขตของการแบ่ง}กว้างที่สุด.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน}
\index{english}{Support Vector Machine}

\item[อภิระนาบ (hyperplane):]
ระนาบในปริภูมิหลายมิติ.
สำหรับปริภูมิสองมิติ {อภิระนาบ}คือเส้นตรง.
สำหรับปริภูมิสามมิติ {อภิระนาบ}คือแผ่นตรงเรียบ.
ปริภูมิหลายมิติใด ๆ (กี่มิติก็ตาม) {อภิระนาบ}สามารถบรรยายได้ด้วยสมการ $\bm{w}^T \bm{x} + b = 0$ 
เมื่อ $\bm{x}$ เป็นจุดใด ๆ ในปริภูมิ และ $\bm{w}$ กับ $b$ เป็นพารามิเตอร์ของอภิระนาบ.
\index{thai}{อภิระนาบ}
\index{english}{hyperplane}
	
\item[ขอบเขตของการแบ่ง (margin of separation):]
ความห่างที่แบ่งกลุ่มข้อมูลสองกลุ่มออกจากกัน (ซึ่งอาจแบ่งได้สมบูรณ์ หรือไม่สมบูรณ์ก็ตาม).
\index{thai}{ขอบเขตของการแบ่ง}
\index{english}{margin of separation}

\item[ขอบเขตตัดสินใจ (decision boundary):]
บริเวณในปริภูมิข้อมูล ที่จุดข้อมูลต่าง ๆ หากอยู่ภายในบริเวณจะถูกจำแนกชนิดตามชนิดของ\textit{ขอบเขตตัดสินใจ}.
\index{english}{decision boundary}
\index{thai}{ขอบเขตตัดสินใจ}


\item[ซัพพอร์ตเวกเตอร์ (support vectors):]
จุดข้อมูลที่สำคัญต่อการกำหนดอภิระนาบ.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ซัพพอร์ตเวกเตอร์}
\index{english}{support vector machine!support vectors}
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน}
\index{english}{support vector machine}

\item[ลูกเล่นเคอร์เนล (kernel tricks):]
การอาศัยรูปแบบการคำนวณของซัพพอร์ตเวกเตอร์แมชชีน
ที่สามารถกำหนดฟังก์ชันเคอร์เนลได้โดยตรง และไม่ต้องกำหนดฟังก์ชันลักษณะสำคัญ.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ลูกเล่นเคอร์เนล}
\index{english}{support vector machine!kernel tricks}
\index{thai}{ลูกเล่นเคอร์เนล}
\index{english}{kernel tricks}

\item[ฟังก์ชันเคอร์เนล (kernel function):]
ฟังก์ชันคำนวณค่าสเกล่าร์ ที่บรรยายความสัมพันธ์ระหว่างจุดข้อมูลสองจุด. 
ในปริภูมิลักษณะสำคัญ.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ฟังก์ชันเคอร์เนล}
\index{english}{support vector machine!kernel function}
\index{thai}{ฟังก์ชันเคอร์เนล}
\index{english}{kernel function}

\item[ฟังก์ชันเคอร์เนลเชิงเส้น (linear kernel):]
ฟังก์ชันเคอร์เนล $k(\bm{x}, \bm{x}) = \bm{x}^T \bm{x}'$.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ฟังก์ชันเคอร์เนล!ฟังก์ชันเคอร์เนลเชิงเส้น}
\index{english}{support vector machine!kernel function!linear kernel}
\index{thai}{ฟังก์ชันเคอร์เนล!ฟังก์ชันเคอร์เนลเชิงเส้น}
\index{english}{kernel function!linear kernel}
\index{english}{linear kernel}
\index{thai}{ฟังก์ชันเคอร์เนลเชิงเส้น}

\item[ฟังก์ชันเคอร์เนลเกาส์เซียน (gaussian kernel):]
ฟังก์ชันเคอร์เนล $k(\bm{x}, \bm{x}') = \exp\left( -\frac{\|\bm{x} - \bm{x}'\|^2}{2 \sigma^2}\right)$.
\index{thai}{ซัพพอร์ตเวกเตอร์แมชชีน!ฟังก์ชันเคอร์เนล!ฟังก์ชันเคอร์เนลเกาส์เซียน}
\index{english}{support vector machine!kernel function!gaussian kernel}
\index{thai}{ฟังก์ชันเคอร์เนล!ฟังก์ชันเคอร์เนลเกาส์เซียน}
\index{english}{kernel function!gaussian kernel}
\index{english}{gaussian kernel}
\index{thai}{ฟังก์ชันเคอร์เนลเกาส์เซียน}
	
	
\end{description}


