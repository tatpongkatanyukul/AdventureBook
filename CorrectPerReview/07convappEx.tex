\section{แบบฝึกหัด}
\label{section: conv app exercises}


\begin{Parallel}[c]{0.52\textwidth}{0.38\textwidth}
	\selectlanguage{english}
	\ParallelLText{
		``A single act of kindness throws out roots in all directions, and the roots spring up and make new trees.''
		\begin{flushright}
			---Amelia Earhart
		\end{flushright}
	}
	\selectlanguage{thai}
	\ParallelRText{
		``ความเมตตาปราณีเพียงครั้งก็หยั่งรากไปทุกทิศทาง และรากก็จะงอกงามออกมาเป็นต้นใหม่.''
		\begin{flushright}
			---เอมีเลีย แอร์ฮาร์ต
		\end{flushright}
	}
\end{Parallel}
\index{english}{words of wisdom!Amelia Earhart}
\index{english}{quote!kindness}


\begin{Exercise}
	\label{ex: conv app literature overview}
จงเลือกการประยุกต์ใช้โครงข่ายคอนโวลูชั่นที่สนใจ แล้วศึกษาวรรณกรรมที่เกี่ยวข้อง
โดยให้เลือกบทความวิจัยที่เกี่ยวข้องไม่น้อยกว่า $20$ บทความ 
แล้วสำหรับแต่ละบทความ ให้อภิปรายถึง 
จุดประสงค์ ความคาดหมาย ปัญหาที่ต้องการแก้ ความท้าทายที่เกี่ยวข้อง วิธีการที่นำเสนอ และผลลัพธ์ 
รวมถึงอภิปรายจุดเด่น และประเด็นอื่น ๆ ที่เห็นว่าน่าสนใจในบทความ.

นอกจากนั้น จงอภิปรายความสัมพันธ์กับการประยุกต์แบบอื่นที่มีลักษณะใกล้เคียงกัน (อาจต้องทำการศึกษาวรรณกรรมเพิ่มเติม ให้ฝึกการศึกษาวรรณกรรมอย่างกว้างขวาง).
ตัวอย่างเช่น หากเลือก\textit{การรู้จำใบหน้า} (face recognition)
อาจอภิปรายความสัมพันธ์ ความเหมือน ความต่าง กับการประยุกต์ใช้สำหรับ 
\textit{การจำแนกชนิดวัตถุในภาพ} (image classification)
หรือ\textit{การพิสูจน์ยืนยันใบหน้า} (face verification)
หรือ\textit{การรู้จำอารมณ์จากใบหน้า} (facial expression recognition)
เป็นต้น.
	
\end{Exercise}

\begin{Exercise}
	\label{ex: conv app literature}

จงเลือกบทความวิจัยในแบบฝึกหัด~\ref{ex: conv app literature overview}
มา $5$ บทความ แล้วสำหรับแต่ละบทความ 
(นอกจากประเด็นในแบบฝึกหัดที่~\ref{ex: conv app literature overview})
ให้อภิปรายถึง 
ข้อมูล วิธีการปฏิบัติ การทดลอง และวิธีการประเมินผล.
	
\end{Exercise}

\begin{Exercise}
	\label{ex: conv app hands-on}
	จากแบบฝึกหัดที่~\ref{ex: conv app literature}
	จงศึกษาวิธีการประยุกต์ใช้ และจงลงมือปฏิบัติ ทดลอง และเปรียบเทียบผลที่ได้กับผลที่รายงานในวรรณกรรม.
%	
	ในการลงมือปฏิบัติ อาจปรับลดความยากของปัญหาลงได้ตามความเหมาะสม 
	รวมถึงอาจศึกษาวิธีการปฎิบัติและโปรแกรมจากอินเตอร์เนต 
	
	ตัวอย่าง หากเลือก\textit{การรู้จำใบหน้า} และสนใจ FaceNet\cite{SchroffEtAl2015}
	อาจใช้คำค้นหา เช่น ``facenet code''
	และอาจเลือกชุดข้อมูลที่ง่ายขึ้น หรือเลือกข้อมูลขนาดเล็กลง หรือใช้แบบจำลองที่เล็กลง เพื่อให้การฝึกทำได้รวดเร็วขึ้น.
		
\end{Exercise}

\begin{Exercise}
	\label{ex: conv app GAN joint prob}
	จงทบทวนเรื่อง\textit{โครงข่ายปรปักษ์เชิงสร้าง}
	(และอาจศึกษาเพิ่มเติม ถ้าจำเป็น)
	และอภิปรายถึงแนวทาง วิธี หรือกลไก เพื่อจะอนุมาน\textit{การแจกแจงร่วม} $p(\bm{X}, \bm{C})$ เมื่อ $\bm{X}$ คือข้อมูลต้น เช่น ภาพ และ $\bm{C}$ คือข้อมูลตาม เช่น ประเภทของวัตถุในภาพ
	โดยอาศัยแนวทางของ\textit{โครงข่ายปรปักษ์เชิงสร้าง}.
	การอภิปราย อาจเริ่มจากข้อคิดเห็นหรือคำถาม 
	เช่น 
	หาก\textit{โครงข่ายปรปักษ์เชิงสร้าง} สามารถเรียนรู้การแจกแจง $p(\bm{X}|\bm{C}, \bm{z})$ ได้แล้ว 
	และในเมื่อ $\bm{z}$ ก็สุ่มสร้างขึ้นมาเอง (อาจจะจากสุ่มจากการแจกแจงเอกรูป หรือการแจกแจงเกาส์เซียน)
	ส่วนเงื่อนไขหรือข้อมูลตาม $\bm{C}$ ซึ่งมักอยู่ในปริภูมิที่มีจำนวนมิติไม่มาก ก็อาจสามารถประมาณการแจกแจงจากข้อมูลที่มีได้ไม่ยากนัก
	ดังนั้น จากการแจกแจง $p(\bm{X}|\bm{C}, \bm{z})$ เราก็น่าจะสามารถใช้\textit{ทฤษฎีของเบส์}
	เพื่ออนุมานการแจกแจงร่วม $p(\bm{X}, \bm{C})$ ได้.
	ทำไมการหาอนุมานการแจกแจงร่วม $p(\bm{X}, \bm{C})$ หรือแม้แต่การหา $p(\bm{X})$ เมื่อ $\bm{X}$ เป็นภาพ เช่น ภาพถ่ายทิวทัศน์ทั่วไป ถึงเป็นปัญหาที่ยากมาก%
	\footnote{
		คำใบ้ คำกล่าวว่าแบบจำลองเรียนรู้การแจกแจง $p(\bm{X}|\bm{C}, \bm{z})$
		กับการที่แบบจำลองสามารถให้ค่า $p(\bm{X}|\bm{C}, \bm{z})$ ออกมาได้ นั้นต่างกัน.
		สิ่งที่\textit{โครงข่ายปรปักษ์เชิงสร้าง}ให้ออกมาจริง ๆ คืออะไร?
		การแจกแจง (?) ความน่าจะเป็น (?) หรือเพียง\textit{ค่าคาดหมาย} $E[\bm{X}|\bm{C}, \bm{z}]$ หรืออะไร?
	}
 หากเป็นไปได้?
	\index{thai}{กฎของเบส์}
	\index{english}{Bayes' rule}
	\index{english}{Bayes' theorem} 
	
	ตั้งกลุ่ม ถามคำถามและอภิปรายข้อคิดเห็นลักษณะเช่นนี้ ความท้าทาย ความเสี่ยง แนวทางและกลไกที่จะลดหรือบรรเทาปัญหาและความเสี่ยงต่าง ๆ.
	ยกตัวอย่าง หรือหากเหมาะสม อาจจะลองออกแบบการทดลองเล็ก ๆ ง่าย ๆ เพื่อพิสูจน์ ยืนยัน หรือหักล้าง.
		
\end{Exercise}


\begin{Exercise}
\label{ex: label smoothing}
\index{english}{label smoothing}
\index{thai}{การทำฉลากราบรื่น}

พิจารณาข้ออภิปรายถึงวิธี\textit{การทำฉลากราบรื่น} ดังนี้
วิธี\textit{การทำฉลากราบรื่น}\cite{SzegedyEtAl2016}
ปรับค่าเป้าหมายของฉลากเป็น
$q_k = (1 - \epsilon) y_k + \epsilon p(k)$
เมื่อ $y_k$ คือค่าฉลากเฉลยในรูป\textit{หนึ่งร้อน}ของประเภท $k^{th}$  
และ $p(k)$ คือการแจกแจงของข้อมูลชนิด $k^{th}$. 
สังเกตว่า วิธี\textit{การทำฉลากราบรื่น}
ปรับที่ค่าเป้าหมายของฉลากเฉย ไม่ได้แก้ไขการคำนวณฟังก์ชันกระตุ้นในแบบจำลอง.
หากพิจารณาประเด็นนี้ร่วมกับฟังก์ชันสูญเสียสำหรับภาระกิจการจำแนกประเภทแบบหลายกลุ่ม
ซึ่งคือ $\mathrm{Loss} = - \sum_k y_k \log \hat{y}_k$ เมื่อ $y_k$ คือค่าเป้าหมายเฉลย และ $\hat{y}_k$ คือค่าทำนาย
จะพบว่า 
กรณีไม่ทำฉลากราบรื่น (กรณีดั่งเดิม) ฟังก์ชันสูญเสียสามารถคำนวณโดย
$\mathrm{Loss} = -\log \hat{y}_{k^\ast}$ เมื่อ $k^\ast$ คือฉลากของประเภทที่เฉลย
เพราะ $y_{k^\ast} = 1$ และ $y_{k \neq k^\ast} = 0$.

แต่กรณีทำฉลากราบรื่น ฟังก์ชันสูญเสียไม่สามารถย่อรูปดังข้างต้นได้
และหากทำการคำนวณ
$\mathrm{Loss} = - \sum_k y_k \log \hat{y}_k$ 
โดยตรง ซึ่งอาจเขียนเป็น 
$\mathrm{Loss} = - (1 - \epsilon + \frac{\epsilon}{K}) \log \hat{y}_{k^\ast} - \sum_{k \neq {k^\ast}} \frac{\epsilon}{K} \log \hat{y}_k$
เมื่อ $K$ คำจำนวนของประเภททั้งหมด
แล้วอาจเกิดปัญหาการคำนวณเชิงเลขได้.
ดังเช่น กรณีที่ $\hat{y}_k$ ตัวใดตัวหนึ่งมีค่าใกล้กับศูนย์มาก ๆ ($\log 0 \rightarrow -\infty$) ซึ่งอาจจะทำให้การคำนวณไม่มีเสถียรภาพ.
ปัญหานี้ แม้จะเกิดยากเนื่องจากแบบจำลองมีแนวโน้มที่จะถูกฝึกให้ $\hat{y}_k$ ปรับเข้าหาเป้าหมาย เช่น $\hat{y}_k \rightarrow \frac{\epsilon}{K}$ แล้วค่า $\epsilon$ ไม่เล็กจนเกินไป.
แต่การปรับเปลี่ยนนี้ ก็เพิ่มความเสี่ยงขึ้นมากจากกรณีดั่งเดิม (ที่มีแต่ $\log \hat{y}_{k^\ast}$ ซึ่ง $\hat{y}_{k^\ast} \rightarrow (1 - \epsilon + \frac{\epsilon}{K})$ ที่มีค่ามากใกล้ ๆ หนึ่ง).

นอกจากนั้น อีกประเด็นหนึ่งสำหรับการใช้วิธี\textit{การทำฉลากราบรื่น}ในทางปฏิบัติ
หาก\textit{การทำฉลากราบรื่น}ถูกนำไปเขียนโปรแกรมโดยไม่ระวัง เช่น อาจอาศัยโปรแกรมหรือโครงสร้างเดิมจากฟังก์สูญเสีย 
ซึ่งคำนวณ $\mathrm{Loss} = -\log \hat{y}_{k^\ast}$ แทน $\mathrm{Loss} = - \sum_k y_k \log \hat{y}_k$
แล้ว
\textit{การทำฉลากราบรื่น} อาจผิดเพี้ยนจากแนวคิดดั่งเดิมได้ 
เช่น
แทนที่จะคำนวณ 
$\mathrm{Loss} = - (1 - \epsilon + \frac{\epsilon}{K}) \log \hat{y}_{k^\ast} - \sum_{k \neq {k^\ast}} \frac{\epsilon}{K} \log \hat{y}_k$ 
แต่ด้วยการใช้โปรแกรมเดิม อาจทำให้สิ่งที่คำนวณจริงเป็น
$\mathrm{Loss} = -(1 - \epsilon + \frac{\epsilon}{K}) \log \hat{y}_{k^\ast}$
ซึ่งผิดเพี้ยนไปจากแนวคิดของ\textit{การทำฉลากราบรื่น}ที่เซเจดีและคณะ\cite{SzegedyEtAl2016}เสนอ
(แต่อาจจะทำงานได้ และอาจจะไปคล้ายกับแนวคิดของ\textit{การทำฉลากราบรื่นทางเดียว})


จงศึกษาโปรแกรม\textit{การทำฉลากราบรื่น}ที่ค้นหาได้จากอินเตอร์เนต ทดลองใช้และสังเกตการทำงานของวิธี\textit{การทำฉลากราบรื่น}
วิเคราะห์ และให้ข้อคิดเห็นเพิ่มเติมจากข้ออภิปรายข้างต้น (อาจเห็นด้วย เห็นแย้ง หรือเห็นต่าง) พร้อมให้เหตุผล และอาจยกตัวอย่างประกอบ เพื่อสนับสนุน
รวมถึงอภิปรายสถานการณ์ต่าง ๆ ว่าหากเกิดขึ้นจริง จะมีผลดี ผลเสียอย่างไรบ้าง และสำหรับผลเสียจะมีวิธีจัดการ แก้หรือบรรเทาปัญหาอย่างไรบ้าง

\end{Exercise}

\begin{Exercise}
	\label{ex: label smoothing binary classification}
	\index{english}{label smoothing}
	\index{thai}{การทำฉลากราบรื่น}
	
	จากแบบฝึกหัด~\ref{ex: label smoothing} ที่อภิปรายกรณีการจำแนกกลุ่ม
	จงอภิปรายประเด็นข้อดี ข้อเสีย โอกาส และความเสี่ยง ในทางปฏิบัติเมื่อนำวิธี\textit{การทำฉลากราบรื่น}ไปใช้ในกรณีการจำแนกค่าทวิภาค (binary classification)
	รวมถึงศึกษางานของคณะของแซลลิมันส์\cite{SalimansEtAl2016} สำหรับเหตุผลที่เลือกใช้\textit{การทำฉลากราบรื่นทางเดียว}
	ทั้งเหตุผล ข้อดี ข้อเสีย โอกาส และความเสี่ยง สำหรับการฝึกโครงข่ายปรปักษ์เชิงสร้าง และการนำแนวคิดไปใช้ในกรณีทั่วไป.

\end{Exercise}

%\begin{Exercise}
%\label{ex: object detection}
%\url{https://github.com/Garima13a/YOLO-Object-Detection}
%
%\end{Exercise}
%
%\begin{Exercise}
%	\label{ex: object tracking}
%	
%	
%\end{Exercise}
%
%\begin{Exercise}
%	\label{ex: face recognition}
%	
%	
%\end{Exercise}
%
%
%\begin{Exercise}
%	\label{ex: pose estimation}
%	
%	
%\end{Exercise}
%
%
%\begin{Exercise}
%	\label{ex: semantic segmentation}
%	
%	
%\end{Exercise}
%
%\begin{Exercise}
%	\label{ex: scene description}
%	
%	
%\end{Exercise}


% LATER
%\begin{Exercise}
%	\label{ex: deconv}
%	\index{deconvolution}
%	\index{ดีคอนโวลูชั่น}

% Check if deconvolution (หัวข้อ~\ref{sec: convapp fix enhance gen images}) is this ConvTranspose
% https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html
% It seems like transpose is a different thing: see https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
%\end{Exercise}