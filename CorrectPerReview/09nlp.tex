%\chapter{การประยุกต์ใช้ในการรู้จำรูปแบบภาษาธรรมชาติ}
\chapter{การรู้จำรูปแบบเชิงลำดับในโลกการประมวลผลภาษาธรรมชาติ}
\label{chapter: NLP}

\begin{Parallel}[c]{0.48\textwidth}{0.42\textwidth}
\selectlanguage{english}
\ParallelLText{
``Yet the truely unique feature of our language is not its ability to transmit information about men and lions. Rather, it's the ability to transmit information about things that do not exist at all.''
\begin{flushright}
---Yuval Noah Harari %, Sapiens.
\end{flushright}
}
\selectlanguage{thai}
\ParallelRText{
	``ลักษณะเด่นจริง ๆ ของภาษามนุษย์ ไม่ได้อยู่ที่ความสามารถในการแปลงสารสนเทศ เกี่ยวกับ คนกับสิงโต. 
แต่มันคือ ความสามารถในการแปลงสารสนเทศเกี่ยวสิ่งที่ไม่ได้มีอยู่จริงเลย.''
\begin{flushright}
---ยูวาล โนอาห์ ฮารารี
\end{flushright}
}
\end{Parallel}
\index{english}{words of wisdom!Yuval Noah Harari}
\index{english}{quote!language}
\vspace{0.5cm}


ภาษา เป็นแบบจำลองคร่าว ๆ ของความคิด และเป็นตัวแทนที่หยาบมาก ๆ สำหรับบรรยายโลกและอธิบายความเป็นจริง.
เพียงแต่ มันยังคงเป็นเครื่องมือที่ดีที่สุดอย่างหนึ่งเท่าที่เรามี สำหรับการถ่ายทอดความคิดและสื่อสารเรื่องราว.
แฮร์มันน์ เฮสเซอ กล่าวว่า ``ทุก ๆ อย่างที่คิด และแสดงออกมาเป็นคำพูด จะลำเอียงไปข้างเดียว ครึ่งเดียวของความจริง ขาดความครบถ้วน ขาดความสมบูรณ์ ขาดเอกภาพ.'' (Hermann Hesse: ``Everything that is thought and expressed in words is one-sided, only half the truth; it all lacks totality, completeness, unity.'')
%คำพูดไม่ได้ถ่ายทอดความคิดออกมาดีมากหรอก.
%มันออกมาต่างไปนิดเสมอทันทีที่ออกมาจากปาก เพี้ยนไปนิด โง่ไปนิด.
%(Hermann Hesse: ``Words do not express thoughts very well. They always become a little different immediately after they are expressed, a little distorted, a little foolish.'')


\section{การประมวลผลภาษาธรรมชาติ}

ภาษา หรือในบริษทของคอมพิวเตอร์ จะเรียกว่า \textbf{ภาษาธรรมชาติ} (natural language) เพื่อเน้นความแตกต่างจาก\textbf{ภาษาโปรแกรม} (programming language) ที่ใช้สำหรับเขียนโปรแกรมให้กับคอมพิวเตอร์.
\textit{ภาษาธรรมชาติ} เป็นภาษาที่คนใช้พูดสื่อสารกัน เช่น ภาษาไทย ภาษาจีน ภาษาสเปน ภาษาอังกฤษ.
\index{thai}{ภาษาธรรมชาติ}\index{english}{natural language}
ภาษาธรรมชาติ%
\footnote{คำอธิบายในส่วนนี้ ได้รับอิทธิพลหลัก ๆ จาก \cite{ThinkPython}}
มีการเกิด การพัฒนา การวิวัฒนาการตามธรรมชาติ.
วิวัฒนการของภาษาเกิดจากคนจำนวนมาก และผ่านผู้คนหลายรุ่น
แม้ว่าอาจมีบางครั้งที่ได้รับการควบคุม ปรับปรุง ผ่านกลุ่มคนจำนวนน้อย ๆ ที่มีอำนาจหรือที่ได้รับมอบหมายบ้าง.
ส่วน\textit{ภาษาโปรแกรม} 
เป็นภาษาที่ออกแบบจากคนหรือกลุ่มคน (จำนวนไม่มาก) เพื่อใช้สั่งงานคอมพิวเตอร์.
ตัวอย่างภาษาโปรแกรม เช่น ภาษาซี ภาษาซีพลัสพลัส ภาษาจาวา ภาษาอาร์ ภาษาไพธอน.
ภาษาโปรแกรม จะมีไวยากรณ์ที่ตายตัว ใช้ควบคุมโครงสร้างของคำสั่งต่าง ๆ.
ขณะที่ไวยากรณ์ของภาษาธรรมชาติ มักจะยืดหยุ่น และมีข้อยกเว้นอยู่มาก.

\textbf{ไวยากรณ์} (syntax) คือกฎเกณฑ์ที่เกี่ยวกับ\textit{โทเค็น} และโครงสร้าง.
\index{english}{syntax}\index{thai}{ไวยากรณ์}
\textbf{โทเค็น} (token) เป็นหน่วยพื้นฐานของภาษาที่มีความหมาย 
เช่น ในภาษาธรรมชาติ โทเค็น หมายถึง คำ.
ในภาษาโปรแกรม โทเค็น หมายถึง คำ, ตัวแปร, ค่าตัวแปร, ค่าคงที่, นิพจน์, ฟังก์ชัน, ออปเจ็ค, เมท็อด เป็นต้น.
โครงสร้างไวยากรณ์ คือการนำโทเค็นไปประกอบกันเพื่อสื่อความหมาย.
ความสัมพันธ์ระหว่างโทเค็นและโครงสร้างไวยากรณ์ อาจแสดงได้ด้วยตัวอย่าง เช่น
ในภาษาอังกฤษ ``This 1s @ท Englisก s3nteทce.'' มีการใช้\textit{โทเค็น}ที่ไม่ถูกต้อง.
ส่วน ``is.sentence ThisEnglish an'' แม้จะใช้\textit{โทเค็น}ที่ถูกต้องทั้งหมด แต่เป็นการประกอบกันที่ไม่ถูกต้องตามไวยากรณ์ภาษาอังกฤษ.
ประโยค ``This is an English sentence.'' ถูกไวยากรณ์ภาษาอังกฤษ (\textit{โทเค็น}ถูกต้องทั้งหมด และประกอบกันเป็นโครงสร้างที่ถูกต้อง). 
การวิเคราะห์โครงสร้างไวยากรณ์ของข้อความหรือประโยค จะเรียกว่า \textbf{การแจกส่วน} (parsing).
\index{thai}{การแจกส่วน}\index{english}{parsing}
เวลาที่เราอ่านข้อความต่าง ๆ เราทำการแจกส่วน เพื่อเข้าใจรูปประโยค ประกอบการทำเข้าใจความหมายของข้อความ.

นอกจากที่ภาษาโปรแกรมมีไวยากรณ์ที่ตายตัวมากกว่าภาษาธรรมชาติแล้ว
ยังมีประเด็นที่แตกต่างกันดังนี้
(1) \textit{ความกำกวม} (ambiguity) 
ที่ภาษาธรรมชาติมักอาศัยบริบทและสามัญสำนึกประกอบในการทำความเข้าใจข้อความ
ในขณะที่ภาษาโปรแกรม ถูกออกแบบให้มีความชัดเจนโดยสมบูรณ์ ตีความได้อย่างเดียว ไม่มีความกำกวมเลย,
(2) \textit{ความซ้ำซ้อน} (redundancy)
ที่พบได้บ่อย ๆ ในภาษาธรรมชาติ
แต่ภาษาโปรแกรมจะกระชับและไม่ซ้ำซ้อน,
(3)  \textit{ความตรงตามตัวอักษร} (literalness)
ที่ภาษาโปรแกรมบอกความหมายที่เจาะจง ตรงตามตัวอักษร
ในขณะที่ภาษาธรรมชาติ มีการใช้สำนวน โวหาร คำเปรียบเปรย.
\index{english}{natural language!issues}

ด้วยความแตกต่างระหว่างภาษาธรรมชาติและภาษาโปรแกรม
ทำให้การประมวลผลภาษาธรรมชาติต้องการเครื่องมือ แนวทาง และกลไกเฉพาะ
ที่นอกเหนือไปจากการยืมมาจากวิธีการต่าง ๆ ในการประมวลผลโปรแกรม.

\textbf{การประมวลผลภาษาธรรมชาติ} (Natural Language Processing คำย่อ NLP)
เป็นศาสตร์ที่ใช้วิธีการต่าง ๆ เพื่อให้คอมพิวเตอร์สามารถนำข้อความในภาษาธรรมชาติไปประมวลผล และให้ผลลัพธ์ตามจุดประสงค์ของภาระกิจที่ต้องการ.
\index{thai}{การประมวลผลภาษาธรรมชาติ}\index{english}{Natural Language Processing}\index{english}{NLP}

ภาระกิจของการประมวลผลภาษาธรรมชาตินั้นมีหลากหลายมาก 
เช่น 
ระบบตรวจสอบภาษา (spelling and grammar correction),
ระบบช่วยจบคำอัตโนมัติ (autocompletion),
ระบบตอบคำถามอัตโนมัติ (question answering system),
การสรุปข้อความ (text summarization),
แชทบอต (chatbot),
การจำแนกอารมณ์ (sentiment classification),
ระบบแปลภาษาอัตโนมัติ (machine translation),
การค้นหาเนื้อหาในเอกสาร (content searching),
การตรวจสอบการลอกเลียนวรรณกรรม (plagiarism detection)
และการสร้างข้อความอัตโนมัติ (text generation).
ลักษณะเฉพาะของภาษาเอง มีส่วนอย่างมากต่อความต้องการและความจำเป็นของภาระกิจต่าง ๆ
เช่น
ภาษาอังกฤษมีขอบเขตคำและขอบเขตประโยคที่ชัดเจน.
การตัดคำ (word segmentation) และตัดประโยค (sentence segmentation) ในภาษาอังกฤษทำได้ง่ายมาก
เมื่อเทียบกับภาษาไทย.
ดังนั้นในขณะที่ ระบบอัตโนมัติสำหรับการตัดคำและการตัดประโยคในภาษาอังกฤษมีความสมบูรณ์เต็มที่และพร้อมใช้งาน
ความสามารถของการตัดคำและการตัดประโยคอัตโนมัติในภาษาไทย กลับอยู่ในระดับเริ่มต้น และยังต้องการการพัฒนาอีกมาก.
การตัดคำและการตัดประโยค นอกจากจะใช้ประกอบการจัดแสดงหน้าเอกสาร (ในการตัดคำขึ้นบรรทัดใหม่)
การตัดคำและการตัดประโยค จัดเป็นภาระกิจพื้นฐานของการประมวลผลภาษาธรรมชาติ ที่จะช่วยให้งานที่มีความซับซ้อนอื่น ๆ สามารถประมวลผลต่อไปได้อย่างมีประสิทธิภาพ.
\index{english}{natural language!applications}

ภาพรวมของการประมวลผลภาษาธรรมชาติ
โดยเฉพาะภาษาอังกฤษ%
\footnote{%
ภาษาไทยมีลักษณะเฉพาะหลายอย่าง
โดยเฉพาะความคลุมเครือของขอบเขตคำและขอบเขตประโยค
ทำให้อาจต้องการความคิดสร้างสรรค์ และกรอบวิธีคิดใหม่ ที่ต่างจากภาพรวมที่อภิปรายนี้.
}
คือ
อินพุตที่ข้อความ
จะถูกแปลงเป็นชุดลำดับของ\textit{โทเค็น} ซึ่งแต่ละ\textit{โทเค็น}เป็นคำ.
จากนั้นแต่ละ\textit{โทเค็น} จะถูกแปลงเป็นเวกเตอร์ลักษณะสำคัญ ซึ่งเป็นเวกเตอร์ของค่าต่าง ๆ ที่เป็นตัวเลข
ก่อนจะเข้ากระบวนการประมวลผลตามแต่ภาระกิจ.
รูป~\ref{fig: nlp process overview} แสดงแนวทางการประมวลผลภาษาธรรมชาติ
โดยทั่วไป ที่แปลงข้อความภาษาธรรมชาติ ไปเป็นชุดลำดับต่าง ๆ ของค่าเวกเตอร์ลักษณะสำคัญ ก่อนจะเข้าประมวลผล.
แนวทางเช่นนี้
ทำให้สามารถใช้แบบจำลองเชิงลำดับต่าง ๆ ที่ทำงานกับข้อมูลที่มีค่าเป็นตัวเลข มาช่วยการประมวลผลตามแต่ภาระกิจได้.

%
\begin{figure}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{09nlp/nlp.png}	
		\caption[ภาพรวมของการประมวลผลภาษาธรรมชาติ]{ภาพรวมของการประมวลผลภาษาธรรมชาติ.
		กล่องแสดงตัวอย่างลักษณะของข้อมูล และลูกศรแทนกระบวนการแปลงข้อมูล.
		ข้อความภาษาธรรมชาติ จะผ่านขั้นตอนต่าง ๆ เพื่อแปลงไปเป็นชุดลำดับของค่าเวกเตอร์ลักษณะสำคัญ ก่อนจะเข้าประมวลผลตามภาระกิจที่ต้องการ.
		}
		\label{fig: nlp process overview}
	\end{center}
\end{figure}
%

รูป~\ref{fig: nlp POS tagging}
แสดงลักษณะภารกิจ\textbf{การระบุหมวดคำ} (Part-Of-Speech Tagging)
\index{english}{Part-Of-Speech Tagging}
\index{thai}{การระบุหมวดคำ}
ที่รับอินพุตเป็นข้อความ (ลำดับของคำ)
แล้วให้เอาต์พุต ออกมาเป็นลำดับของหมวดคำ โดยลำดับของเอาต์พุตจะสอดคล้องกับลำดับของอินพุต.
ในภาพ ข้อความอินพุตถูกแบ่งออกเป็น\textit{โทเค็น} ซึ่ง ณ ที่นี้ แต่ละ\textit{โทเค็น}คือ คำ
และเอาต์พุต ก็เป็นชุดลำดับข้อมูล ที่แต่ละจุดข้อมูลจะสอดคล้องกับแต่ละ\textit{โทเค็น}.

%
\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{09nlp/POStagging.png}	
		\caption[ตัวอย่างอินพุตเอาต์พุตของภารกิจการระบุหมวดคำ]{ตัวอย่างอินพุตเอาต์พุตของภารกิจการระบุหมวดคำ.
		อินพุตเป็น คำพูดของบรูซลี และเอาต์พุต เป็นตัวอย่างผลลัพธ์จากระบบการระบุหมวดคำ.
		บรรทัดสุดท้าย แสดงเฉลย.
		}
		\label{fig: nlp POS tagging}
	\end{center}
\end{figure}
%

%
%\begin{table}[hbtp]
%	{\small
%	\caption{ค่าพารามิเตอร์ของแบบจำลองพหุนาม กับการทำค่าน้ำหนักเสื่อมที่ลากรานจ์ค่าต่าง ๆ}
%	\begin{center}
%		\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
%			\hline 
%			Mistakes & are & always & forgivable &, & if & one & has & the & courage & to &admit them &. \\
%			noun & verb & adverb & adjective & & proposition & \textit{number} () & verb
%			& determiner & noun & () & verb & pronoun & \\ 
%			\hline 
%		\end{tabular} 
%	\end{center}
%	\label{tbl: nlp POS tagging example}
%}%
%\end{table}



%noun, verb, article, adjective, preposition, pronoun, adverb, conjunction และ interjection 


\section{โครงข่ายประสาทเวียนกลับ}
\label{sec: RNN}

\textbf{โครงข่ายประสาทเวียนกลับ} (Recurrent Neural Network คำย่อ RNN)
\index{english}{Recurrent Neural Network}
\index{thai}{โครงข่ายประสาทเวียนกลับ}
เป็นแบบจำลองโครงข่ายประสาทเทียม ที่อินพุตของแต่ละหน่วยย่อย นอกจากจะเป็นค่าของเอาต์พุตจากหน่วยย่อยในชั้นคำนวณก่อนหน้าแล้ว
ยังสามารถเป็นค่าของเอาต์พุตของหน่วยย่อยในชั้นคำนวณเดียวกัน สำหรับจุดข้อมูลลำดับก่อนหน้าได้.
เช่นเดียวกับการวิเคราะห์การคำนวณของโครงข่ายเป็นชั้นคำนวณ ดังอภิปรายในบทที่~\ref{chapter: Convolution}
โครงข่ายประสาทเวียนกลับ ก็สามารถมองเป็นการประกอบกันของ\textbf{ชั้นคำนวณเวียนกลับ} (recurrent layer) ได้.

การคำนวณของหน่วยย่อยในชั้นคำนวณเวียนกลับที่ $q^{th}$ อาจเขียนได้ดังนี้
\begin{eqnarray}
a_j^{(q)}(t) 
&=& \sum_{i=1}^D w_{ji}^{(q)} \cdot z_{i}^{(q-1)}(t) + \sum_{m=1}^M v_{jm}^{(q)} \cdot z_m^{(q)}(t-1) + b_j^{(q)}
\label{eq: RNN feedforward a} \\
z_j^{(q)}(t) &=& h(a_j^{(q)}(t))
\label{eq: RNN feedforward z}
\end{eqnarray}
เมื่อ $a_j^{(q)}(t)$ คือค่าตัวกระตุ้นของหน่วยย่อยที่ $j^{th}$ สำหรับจุดข้อมูลลำดับที่ $t^{th}$.
ตัวแปร $z_j^{(q)}(t)$ คือ ผลการ\atom{กระตุ้น}
หรือบางครั้งอาจเรียก 
%$z_j^{(q)}(t)$ 
ว่าเป็น \textbf{สถานะซ่อน}\index{thai}{โครงข่ายประสาทเวียนกลับ!สถานะซ่อน}\index{english}{Recurrent Neural Network!hidden state}
ของหน่วยที่ $j^{th}$ ในชั้นคำนวณ $q^{th}$ สำหรับจุดข้อมูลลำดับเวลา $t^{th}$
%ตัวแปร $z_{i}^{(q-1)}(t)$ คืออินพุตที่ $i^{th}$ ของชั้นคำนวณ $q^{th}$ ซึ่งก็คือ เอาต์พุตจากหน่วยย่อยที่ $i^{th}$ ของชั้นคำนวณ $(q-1)^{th}$
%สำหรับจุดข้อมูลลำดับที่ $t^{th}$
โดย $D$ คือจำนวนหน่วยย่อยในชั้น $(q-1)^{th}$
%ตัวแปร $z_m^{(q)}(t-1)$ คือผลการกระตุ้นของหน่วยที่ $m^{th}$ สำหรับจุดข้อมูลลำดับที่ $(t-1)^{th}$
และ $M$ คือจำนวนหน่วยย่อยในชั้น $q^{th}$.
ตัวแปร $w_{jd}^{(q)}$ เป็นค่าน้ำหนักของการเชื่อมต่อระหว่างหน่วยที่ $d^{th}$ ของชั้น $(q-1)^{th}$ กับหน่วยที่ $j^{th}$ ของชั้นคำนวณ $q^{th}$.
ตัวแปร $v_{jm}^{(q)}$ เป็นค่าน้ำหนักของการเชื่อมต่อของหน่วยที่ $m^{th}$ เวียนกลับมาเข้าหน่วยที่ $j^{th}$ ของชั้นคำนวณเดียวกัน.
ส่วน $b_j^{(q)}$ คือค่าไบอัสของหน่วยที่ $j^{th}$
และ $h(\cdot)$ คือฟังก์ชันกระตุ้น.

เมื่อเปรียบเทียบสมการ~\ref{eq: RNN feedforward a}
กับสมการ~\ref{eq: mlp feedforward a} ซึ่งเป็นการคำนวณของ\textit{โครงข่ายแพร่กระจายไปข้างหน้า}
จะเห็นว่าจุดต่างที่สำคัญ คือ พจน์ $\sum_{m=1}^M v_{jm}^{(q)} \cdot z_m^{(q)}(t-1)$
ซึ่งเป็นการนำผลการกระตุ้นที่ลำดับเวลาก่อน เข้ามาคำนวณด้วย.
รูป~\ref{fig: rnn} แสดงตัวอย่างโครงสร้างการเชื่อมต่อของโครงข่ายประสาทเวียนกลับ ที่อินพุตมีสี่มิติ 
และเอาต์พุตมีสองมิติ
โดยชั้นคำนวณที่สอง ซึ่งเป็นชั้นเวียนกลับ มีหน่วยย่อยสองหน่วย.
%สังเกตการเวียนกลับ จะส่งผ่านค่าผลการกระตุ้นของลำดับเวลาก่อนหน้า กระจายไปทุก ๆ หน่วย.
รูป~\ref{fig: rnn example} แสดงตัวอย่างโครงข่ายประสาทเวียนกลับ พร้อมตัวอย่างชุดข้อมูลลำดับ และตัวแปรที่สำคัญ.


%
\begin{figure}
	\begin{center}
		\includegraphics[width=0.4\columnwidth]{08seq/RNN/rnn_structure.png}	
		
		\caption[ตัวอย่างโครงข่ายประสาทเวียนกลับ]{ตัวอย่างโครงข่ายประสาทเวียนกลับ โดยเน้นเส้นทางข้อมูลป้อนเวียนกลับ.
โครงข่ายประกอบด้วยสามชั้นคำนวณ 
โดยชั้นอินพุต (อยู่บนสุด) มีสี่หน่วย (รับอินพุตสี่มิติ ได้แก่ $x_1$ ถึง $x_4$)
ชั้นที่สอง เป็นชั้นเวียนกลับ มีสองหน่วย (แสดงด้วยวงกลมสีฟ้าเขียว)
และชั้นที่สาม (อยู่ล่างสุด).
เส้นทางการส่งข้อมูลเวียนกลับ แสดงด้วย เส้นสีแดง (ค่าเวียนกลับจากหน่วยแรก) และเส้นสีส้ม (ค่าเวียนกลับจากหน่วยที่สอง).
	}
		\label{fig: rnn}
	\end{center}
\end{figure}
%

\begin{figure}
	\begin{center}

		\includegraphics[width=0.5\columnwidth]{08seq/RNN/rnn_example.png}	
		\caption[ตัวอย่างโครงข่ายประสาทเวียนกลับ พร้อมตัวอย่างชุดข้อมูลลำดับ]{ตัวอย่างโครงข่ายประสาทเวียนกลับ พร้อมตัวอย่างชุดข้อมูลลำดับ โดยเน้นตัวแปรที่สำคัญ.
ในภาพ แสดงการคำนวณ ณ จุดข้อมูลลำดับที่ $t=2$
ซึ่ง $a_1(t) = w_{11} (0.13) + w_{12} (0.45) + w_{13} (0.56) + w_{14} (0.79)$
$+ v_{11} z_1(t-1) + v_{12} z_2(t-1) + b_1$
และ $a_2(t) = w_{21} (0.13) + w_{22} (0.45) + w_{23} (0.56) + w_{24} (0.79)$
$+ v_{21} z_1(t-1) + v_{22} z_2(t-1) + b_2$.}
\label{fig: rnn example}
\end{center}
\end{figure}
%

รูป~\ref{fig: rnn}
แสดงโครงข่ายประสาทเวียนกลับ โดยเน้นการแสดงโครงสร้าง.
อย่างไรก็ตาม หากชั้นเวียนกลับมีจำนวนหน่วยมาก ๆ 
การเขียนแผนภาพเช่นนี้ จะดูยุ่งเหยิงมาก
(แต่ละหน่วยส่งค่าเวียนกลับไปให้ทุก ๆ หน่วยในชั้น).
บ่อยครั้ง แผนภาพโครงข่ายประสาทเวียนกลับ
จึงม้ักถูกแสดงโดยใช้วงกลมแค่หนึ่งวงแทนชั้นคำนวณทั้งชั้น (ไม่ว่าภายในชั้นจะใช้จำนวนหน่วยเท่าใด)
ดังแสดงในรูป~\ref{fig: rnn simplified diagram}.
นอกจากนั้น ในบางสถานการณ์
การใช้\textbf{แผนภาพคลี่ลำดับ} (unfolding diagram) \index{english}{unfolding diagram}\index{thai}{แผนภาพคลี่ลำดับ}
ที่แสดงข้อมูลการเวียนกลับ
ด้วยการกระจายออกตามลำดับเวลา
อาจช่วยให้เข้าใจแนวคิดได้ดีกว่า.
\textit{แผนภาพคลี่ลำดับ}
อาจแสดงดังรูป~\ref{fig: rnn unfolding diagram}.

จากแผนภาพคลี่ลำดับ ในรูป~\ref{fig: rnn unfolding diagram} สังเกต
(1) ทุก ๆ ลำดับเวลา การคำนวณใช้ค่าน้ำหนักชุดเดียวกัน 
(ที่เวลา $t$ ต่าง ๆ ใช้ค่า $\bm{w}^{(1)}$,
$\bm{w}^{(2)}$ และ $\bm{v}$ เหมือนกัน)
(2) ผลการกระตุ้นของจุดข้อมูลลำดับเวลาใด ๆ $\bm{z}(t)$ จะส่งผลต่อเอาต์พุตผ่านหลายเส้นทาง 
(เส้นทางตรง ส่งผลต่อ $\bm{y}(t)$ 
และเส้นทางเวียนกลับเอาต์พุตอื่น ๆ หลังจากลำดับเวลานั้น ๆ ผ่านเส้นทางการเวียนกลับ). 




\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/RNN_simplified1.png}	
		\caption[แผนภาพโครงสร้างโดยรวมของโครงข่ายประสาทเวียนกลับ]{แผนภาพโครงสร้างโดยรวมของโครงข่ายประสาทเวียนกลับ
		โดยวงกลมแทนชั้นคำนวณทั้งชั้น (โดยไม่ระบุจำนวนหน่วยคำนวณภายในชั้น).
		เส้นทางข้อมูล ระบุ $\bm{w}^{(1)}$, $\bm{w}^{(2)}$ และ $\bm{v}$
		สำหรับค่าน้ำหนักการแพร่กระจายไปข้างหน้า ของชั้นคำนวณที่หนึ่ง กับของชั้นคำนวณที่สอง 
		และค่าน้ำหนักเวียนกลับ (ของชั้นคำนวณที่หนึ่ง แต่ตัวยกถูกละไว้ เพื่อความกระชับ).
}
\label{fig: rnn simplified diagram}
\end{center}
\end{figure}
%

\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/unfolding1.png}	
		\caption[แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับ]{แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับ.		
		}
		\label{fig: rnn unfolding diagram}
	\end{center}
\end{figure}
%

\paragraph{เกรเดียนต์ของชั้นเวียนกลับ.}
ในลักษณะเดียวกับ\textit{โครงข่ายแพร่กระจายไปข้างหน้า}และ\textit{โครงข่ายคอนโวลูชั่น}
การฝึก\textit{โครงข่ายประสาทเวียนกลับ}
สามารถทำได้โดยปรับค่าน้ำหนักต่าง ๆ โดยอาศัยการแพร่กระจายย้อนกลับ
ซึ่งทำการคำนวณค่าเกรเดียนต์เป็นชั้น ๆ.

การแพร่กระจายย้อนกลับ สำหรับชั้นคำนวณเวียนกลับ
สามารถทำได้อย่างมีประสิทธิภาพ ด้วยขั้นตอนวิธี หลาย ๆ วิธี\cite{Graves2012} 
ไม่ว่าจะเป็น
\textit{การเรียนรู้เวียนกลับเวลาจริง} (real time recurrent learning\cite{RobinsonFallside1987})
หรือ\textbf{การแพร่กระจายย้อนกลับผ่านเวลา} (backpropagation through time\cite{WilliamsZipser1995, Werbos1990} คำย่อ BPTT)
\index{thai}{การแพร่กระจายย้อนกลับผ่านเวลา}
\index{english}{backpropagation through time}
โดย เกรฟซ์\cite{Graves2012}  ให้ความเห็นว่า \textit{การแพร่กระจายย้อนกลับผ่านเวลา} เข้าใจได้ง่ายกว่า และสามารถคำนวณได้อย่างมีประสิทธิภาพมากกว่า.
เกรเดียนต์ของชั้นเวียนกลับ ดังที่จะอภิปรายต่อไปนี้ ใช้แนวทางของ\textit{การแพร่กระจายย้อนกลับผ่านเวลา} เช่นเดียวกับการอธิบายของเกรฟซ์\cite{Graves2012}.

ทำนองเดียวกัน กำหนดให้ $E$ เป็นฟังก์ชันค่าผิดพลาด
และ
\begin{eqnarray}
\delta_j^{(q)}(t) \equiv \frac{\partial E}{\partial a_j^{(q)}(t)}
\label{eq: RNN derivative delta} .
\end{eqnarray}

จากการที่
ค่าการกระตุ้น $a_j^{(q)}(t)$ ส่งผลต่อ $E$ ผ่านผลการกระตุ้น $z_j^{(q)}(t)$
และ\textit{กฎลูกโซ่} 
เราจะได้
\begin{eqnarray}
\frac{\partial E}{\partial a_j^{(q)}(t)}
&=& \frac{\partial E}{\partial z_j^{(q)}(t)} 
\cdot \frac{\partial z_j^{(q)}(t)}{\partial a_j^{(q)}(t)}
\nonumber \\
&=& \frac{\partial E}{\partial z_j^{(q)}(t)} 
\cdot h'(a_j^{(q)}(t))
\label{eq: BPTT dE/da = dE/dZ h'(a)} .
\end{eqnarray}

เมื่อพิจารณา เราจะเห็นว่า
ผลการกระตุ้น $z_j^{(q)}(t)$ ส่งอิทธิพลต่อ $E$ ผ่านสองเส้นทาง คือ
(1) ผ่านการแพร่กระจายไปข้างหน้า (ผ่านชั้นคำนวณต่อไป) และ (2) ผ่านการเวียนกลับ (ผ่านชั้นคำนวณเดิม แต่สำหรับลำดับเวลาถัดไป).
ดังนั้น ด้วย\textit{กฎลูกโซ่} เราก็จะได้
\begin{eqnarray}
\frac{\partial E}{\partial z_j^{(q)}(t)}
&=& \sum_k \frac{\partial L}{\partial a_k^{(q+1)}(t)} \cdot \frac{\partial a_k^{(q+1)}(t)}{\partial z_j^{(q)}(t)} 
+ \sum_m  \frac{\partial L}{\partial a_m^{(q)}(t+1)} \cdot \frac{\partial a_m^{(q)}(t+1)}{\partial z_j^{(q)}(t)}
\label{eq: RNN derivative dE/dz} \\
&=& \sum_k \delta_k^{(q+1)}(t) \cdot w_{kj}^{(q+1)} 
+ \sum_m  \delta_m^{(q)}(t+1) \cdot v_{mj}^{(q)}
\label{eq: RNN derivative dE/dz refined} .
\end{eqnarray}

จากสมการ~\ref{eq: BPTT dE/da = dE/dZ h'(a)} และ~\ref{eq: RNN derivative dE/dz refined}
เราจะได้
\begin{eqnarray}
\delta_j^{(q)}(t) &=& h'(a_j^{(q)}(t))
\cdot \left(\sum_k \delta_k^{(q+1)}(t) \cdot w_{kj}^{(q+1)} 
+ \sum_m  \delta_m^{(q)}(t+1) \cdot v_{mj}^{(q)}\right)
\label{eq: RNN delta} .
\end{eqnarray}

สุดท้าย เมื่อพิจารณาเกรเดียนต์ต่อค่าน้ำหนักต่าง ๆ 
ซึ่งค่าน้ำหนักต่าง จะถูกใช้คำนวณสำหรับทุก ๆ ลำดับเวลาเหมือนกัน
ดังนั้น
\begin{eqnarray}
\frac{\partial E}{\partial w_{ji}^{(q)}}
&=& \sum_t \frac{\partial E}{\partial a_j^{(q)}(t)} \cdot \frac{\partial a_j(t)^{(q)}}{\partial w_{ji}^{(q)}}
\label{eq: RNN derivative dE/dw} \\
&=& \sum_t \delta_j^{(q)}(t) \cdot z_i^{(q-1)}(t)
\label{eq: RNN derivative dE/dw refined}
\end{eqnarray}
และ
\begin{eqnarray}
\frac{\partial E}{\partial v_{jm}^{(q)}}
&=& \sum_t \frac{\partial E}{\partial a_j^{(q)}(t)} \cdot \frac{\partial a_j(t)^{(q)}}{\partial v_{jm}^{(q)}}
\label{eq: RNN derivative dE/dv} \\
&=& \sum_t \delta_j^{(q)}(t) \cdot z_m^{(q)}(t-1)
\label{eq: RNN derivative dE/dv refined}
\end{eqnarray}

เช่นเดียวกับค่าน้ำหนัก ค่าไบอัส $b_j^{(q)}$ สามารถคำนวณได้จาก
$\frac{\partial E}{\partial b_j^{(q)}}$
$=\sum_t \frac{\partial E}{\partial a_j^{(q)}(t)} \cdot \frac{\partial a_j(t)^{(q)}}{\partial b_j^{(q)}}$
ซึ่งจะได้ว่า
\begin{eqnarray}
\frac{\partial E}{\partial b_j^{(q)}}
&=&
\sum_t \delta_j^{(q)}(t) 
\label{eq: RNN derivative dE/db} .
\end{eqnarray}

การวิเคราะห์ค่าที่ใช้ในการเริ่มต้นการคำนวณ
สามารถทำได้ในลักษณะเดียวกับที่ทำกับโครงข่ายแพร่กระจายไปข้างหน้า.
นั่นคือ ฟังก์ชันค่าผิดพลาด อาจนิยามเป็น 
\begin{eqnarray}
E = \frac{1}{T} \sum_t \sum_k E_k(t)
\label{eq: RNN objective}
\end{eqnarray}
เมื่อ $T$ เป็นจำนวนลำดับ %(และดัชนีลำดับสุดท้าย)
โดย $E_k(t)$ เป็นค่าผิดพลาดของมิติ $k^{th}$ ที่ลำดับเวลา $t^{th}$.
หากลักษณะภาระกิจถูกตีกรอบเป็นการหาค่าถดถอย
เราอาจกำหนด 
\begin{eqnarray}
E_k(t) = \frac{\mathcal{M}(t)}{2} \cdot (\hat{y}_k(t) - y_k(t))^2
\label{eq: RNN SE objective}
\end{eqnarray}
โดย $y_k(t)$ เป็นค่าเฉลยของมิติ $k^{th}$ ที่ลำดับเวลา $t^{th}$ 
และ $\hat{y}_k(t)$ เป็นค่าที่ทำนาย.
ส่วน $\mathcal{M}(t) \in \{0, 1\}$
เป็นเสมือน\textit{หน้ากาก} (mask) ที่ใช้ควบคุมว่า ณ ลำดับเวลา $t^{th}$ เราต้องการคิดผลของการทำนายหรือไม่.

การใช้กลไก\textit{หน้ากาก} แม้จะสามารถใช้ได้ทั่วไป
แต่สำหรับบริบทของการอนุมานข้อมูลเชิงลำดับ กลไกนี้มีความสำคัญอย่างมาก.
ภาระกิจการอนุมานข้อมูลเชิงลำดับ มีหลากหลายประเภท (หัวข้อ~\ref{sec: seq data}).
ภาระกิจบางประเภท อาจต้องการการทำนายค่าสำหรับทุก ๆ ลำดับเวลา (เช่น ระบบตรวจสอบการสะกดคำ ที่ต้องให้ค่าทำนายสะกดถูกหรือผิดออกมาสำหรับทุกดัชนีลำดับ)
ภาระกิจบางประเภท อาจต้องการการทำนายค่า แค่บางลำดับเวลา (เช่น การจำแนกอารมณ์ ที่อาจจะให้ค่าทำนายออกมาเฉพาะที่ดัชนีลำดับสุดท้ายเท่านั้น)
%
การใช้กลไก\textit{หน้ากาก} ช่วยกำหนดดัชนีลำดับที่มีผลจริง ๆ 
($\mathcal{M}(t) = 1$ เฉพาะดัชนีลำดับ $t$ ที่มีค่าเฉลย ส่วนนอกนั้นให้ $\mathcal{M}(t) = 0$)
จะช่วยให้การทำงานกับข้อมูลลำดับยืดหยุ่นและสะดวกมากขึ้น.

เมื่อพิจารณาเกรเดียนต์ ด้วยสมการ~\ref{eq: RNN objective} และ~\ref{eq: RNN SE objective}
สำหรับ กรณีการหาค่าถดถอย ซึ่งมักกำหนดให้ $a^{(L)}_k(t) = z^{(L)}_k(t) = \hat{y}_k(t)$
เราจะเห็นว่า
\begin{eqnarray}
\delta_k^{(L)} (t) = \frac{\partial E}{\partial a^{(L)}_k(t)}
&=& \frac{\partial E}{\partial \hat{y}_k(t)}
= \frac{1}{T} \sum_{\tau} \sum_j \mathcal{M}(\tau) \cdot \left(\hat{y}_j(\tau) - y_j(\tau)\right) 
\cdot \frac{\partial \hat{y}_j(\tau)}{\partial \hat{y}_k(t)}
\nonumber \\
&=& \frac{1}{T} \mathcal{M}(t) \cdot \left(\hat{y}_k(t) - y_k(t)\right) 
\label{eq: RNN delta L}
\end{eqnarray}
สำหรับ $t = 1, \ldots, T$.



{\small
	\begin{shaded}
		\paragraph{\small เกร็ดความรู้ เมตตา.}
		\index{english}{self}\index{thai}{อัตตา}
		\index{thai}{ความผิดปกติทางบุคลิกภาพแบบหลงตัวเอง}\index{thai}{โรคหลงตัวเอง}
		\index{english}{Narcissistic Personality Disorder}
		\index{thai}{เมตตา}\index{english}{compassion}\index{english}{loving kindness}
		\index{english}{side story}
		\index{english}{side story!compassion}
		\index{thai}{เกร็ดความรู้}
		\index{thai}{เกร็ด!เมตตา}
				
		\begin{Parallel}[c]{0.48\textwidth}{0.38\textwidth}
			\selectlanguage{english}
			\ParallelLText{
				``Wisdom can be learned. \\
				But it cannot be taught .''
				\begin{flushright}
					---Anthony de Mello
				\end{flushright}
			}
			\selectlanguage{thai}
			\ParallelRText{
				``ปัญญาสามารถเรียนรู้ได้ \\
				แต่มันสอนกันไม่ได้.''
				\begin{flushright}
					---แอนโธนี เดอ เมลโล
				\end{flushright}
			}
		\end{Parallel}
		\index{english}{words of wisdom!Leo Tolstoy}
		\index{english}{quote!pre-condition}
		\vspace{0.5cm}
						
%		\paragraph{\small ปัญญาและเมตตา}
		ปัญญาและเมตตาเป็นคุณค่าสูงสุดของมนุษย์.
		\textbf{ปัญญา} คือ ความรู้ในเรื่องราวตามความเป็นจริง ครอบคลุมถึงความสามารถในการคิด วิเคราะห์ สังเคราะห์ แก้ปัญหา พัฒนา ตระหนักรู้ โดยใช้ความรู้, ประสบการณ์, ความเข้าใจ, สามัญสำนึก และมุมมองที่หลากหลายครบถ้วน.
		แฮร์มันน์ เฮสเซอ กล่าวว่า ``ความรู้สามารถสื่อสารกันได้ แต่ไม่ใช่ปัญญา. 
		เราหาปัญญาได้ เราใช้ชีวิตอยู่กับปัญญาได้ เราป้องกันตัวเองจากภยันตรายด้วยปัญญาได้
		เราทำสิ่งมหัศจรรย์ด้วยปัญญาได้ แต่เราสื่อสารปัญญาออกไปไม่ได้ เราสอนปัญญาไม่ได้.''
		(Hermann Hesse: ``Knowledge can be communicated, but not wisdom. One can find it, live it, 
		be fortified by it, do wonders through it, but one cannot communicate and teach it.'')
		ผู้คนและสังคมชื่นชมและยกย่องปัญญา แม้หลายครั้งอาจจะสับสนระหว่างปัญญา ความรู้ และความฉลาด.
		\index{english}{wisdom}\index{thai}{ปัญญา}
		
		\textbf{เมตตา} คือ ความปรารถนาให้ชีวิตต่าง ๆ เป็นสุข ซึ่งรวมทั้งชีวิตสัตว์ ชีวิตคนอื่น และชีวิตของตัวเราเองด้วย. ในความหมายกว้าง ๆ แล้ว ความหมายของเมตตา ยังครอบคลุมไปถึงความปรารถนาให้ชีวิตพ้นทุกข์ (กรุณา), ความยินดีเมื่อชีวิตเป็นสุข (มุทิตา) และในบางครั้งก็อาจหมายรวมถึงการปล่อยวาง (อุเบกขา) ด้วย.
		สำหรับเมตตาแล้ว แม้จะเป็นหนึ่งในสองคุณค่าสูงสุดคู่กับปัญญา 
		แต่สังคมดูเหมือนจะชื่นชมและยกย่องเมตตาน้อยเกินไป โดยเฉพาะเมื่อเปรียบเทียบกับระดับการยกย่องปัญญา. 
		(ดูจากปรัชญาของโรงเรียนและมหาวิทยาลัยต่าง ๆ ทั้งในและต่างประเทศ เป็นตัวอย่าง.)
		\index{thai}{เมตตา}\index{english}{compassion}
		
		หมายเหตุ
\textbf{คุณธรรม} นั้นอ้างถึงความดี ซึ่งครอบคลุมความหมายกว้าง ๆ และบ่อยครั้งที่ถูกตีความผ่านค่านิยมของสังคมหรือกลุ่มคน.
แม้บางครั้งอาจมองว่า คุณธรรมครอบคลุมถึงความเมตตาด้วย 
แต่เนื่องจากคุณธรรมถูกตีความผ่านค่านิยมของสังคม 
ความหมายของคุณธรรมจึงขึ้นกับบริบทเป็นอย่างมาก. 
\index{thai}{คุณธรรม}\index{english}{virtues}
%และลำดับความสำคัญของแง่มุมต่าง ๆ 
ตัวอย่างเช่น\cite{Wikipedia, VIA} 
คุณธรรมตามค่านิยมของกรีกโบราณ ตามแนวคิดของเพลโต คือ ความรอบคอบ, ความกล้าหาญ, การรู้จักระงับยับยั้งใจ และความยุติธรรม.
คุณธรรมตามค่านิยมของซามูไร (บูชิโด) คือ ความซื่อสัตย์และยุติธรรม, ความกล้าหาญ, ความเมตตา, ความเคารพให้เกียรติกันและกัน, สัจจะวาจา, เกียรติและศักดิ์ศรี, หน้าที่และความภักดี และการระงับอารมณ์ การควบคุมตัวเอง.
%คุณธรรมตามค่านิยมของศาสนาคริสต์ คือ ความอ่อนน้อมถ่อมตน, ความใจดีมีเมตตา, การรู้จักระงับควบคุมตัวเอง, การถือพรหมจรรย์, ความอดทนอดกลั้น, ความมีน้ำใจแบ่งปันให้ทาน และความขยันหมั่นเพียร.
%
%คุณธรรมหลักตามค่านิยมของศาสนาอิสลาม คือ การให้ทานช่วยเหลือเพื่อนมนุษย์, การให้อภัย, การยอมรับความต่างและอยู่ร่วมกันอย่างสันติ, ความซื่อสัตย์, ความใจดีอ่อนโยน, การปฏิบัติต่อสัตว์อย่างเมตตา, ความยุติธรรม, การรักษาสัญญา, ความอ่อนน้อมถ่อมตน, ปิยวาจา, การเป็นที่ไว้ใจได้, ความอดทน, ความซื่อสัตย์, การควบคุมความโกรธ, ความจริงใจ และการเคารพผู้ใหญ่.
%Charity and philanthropy, 
%Forgiveness,
%Tolerance,
%Honesty,
%Kindness and leniency,
%Kind treatment to animals,
%Justice,
%Fulfillment of promise
%Modesty and humility
%Decent speech
%Trustworthiness
%Patience
%Truthfulness
%Anger management
%Sincerity
%Respecting the elders
%
% Hindu: Dhriti (courage), Kshama (forgiveness), Dama (temperance), Asteya (Non-covetousness/Non-stealing), Saucha (inner purity), Indriyani-graha (control of senses), dhi (reflective prudence), vidya (wisdom), satyam (truthfulness), akrodha (freedom from anger)
%
%Friedrich Nietzsche: courage, insight, sympathy, solitude
%
คุณธรรมตามค่านิยมจีนดั่งเดิม คือ 
ความเมตตา, ความประหยัดมัธยัสถ์, ความอ่อนน้อมถ่อมตน และความกตัญญู.
%
คุณธรรมแก่นตามแนวคิดจิตวิทยายุคใหม่
คือ ปัญญาและความรู้ (ความอยากรู้อยากเห็น, ความคิดสร้างสรรค์, การเปิดกว้างทางความคิด, การรักที่จะเรียนรู้ และการมีมุมมองที่หลากหลาย),
ความกล้าหาญ (ความอาจหาญในการเผชิญความเสี่ยงหรืออันตราย, ความมุมานะอุตสาหะ, ความซื่อสัตย์มั่นคง และความกระตือรือร้น),
มนุษยธรรม (ความรัก, เมตตา และความฉลาดทางสังคม),
ความเป็นธรรม (ความรับผิดชอบทางสังคม, ความยุติธรรม และความเป็นผู้นำ),
การควบคุมอารมณ์ (การให้อภัย, ความอ่อนน้อมถ่อมตน, ความรอบคอบ และการควบคุมตนเอง)
และอุตรภาพ (การชื่นชมในความงามของสิ่งรอบตัวและความดีของผู้คน, ความสำนึกเห็นค่าและรู้คุณ, ความหวัง, อารมณ์ขันและความขี้เล่น
และศรัทธาหรือความแกร่งทางจิตวิญญาณ)

%https://en.wikipedia.org/wiki/Virtue
%https://en.wikipedia.org/wiki/Bushido
%https://en.wikipedia.org/wiki/Morality_in_Islam
		 
%		หลาย ๆ ที่ระบุปัญญาในปรัชญาอย่างเด่นชัด แต่มีกี่ที่ ที่กล่าวถึงเมตตาหรือคุณภาพอื่นที่ใกล้เคียง.%
%		ปรัชญาของมหาวิทยาลัยขอนแก่น ``วิทยา คือ ความรู้ดี จริยา คือ ความประพฤติดี ปัญญา คือ ความฉลาด เกิดแต่การเรียนดี และคิดดี''%
%		)
		การจะพัฒนาปัญญาเองนั้น ถ้าหากขาดเมตตาแล้ว ปัญญาจะพัฒนาไปได้อย่างจำกัดมาก (หากจะยังพัฒนาต่อไปได้)
		เพราะความรู้ในเรื่องราวตามความเป็นจริง จะสมบูรณ์ได้อย่างไร หากขาดความเห็นใจเข้าใจชีวิตอื่น.
%		แต่เมตตากลับเป็นคุณค่าหนึ่งที่มักถูกละเลย หรือไม่ได้ถูกให้ความสำคัญมากนัก.
		นอกจากนั้น 
%		เมตตายังเป็นคุณภาพที่ช่วยลดขนาดอัตตา และป้องกันไม่ให้อัตตาโตมากจนเกินไป.
เช่นเดียวกับที่ \textit{ผลป้อนกลับลบ} (negative feedback) จะช่วยให้ระบบทางวิศวกรรมมีเสถียรภาพที่ดี และทนทานต่อสภาพการใช้งานที่หลากหลายมากกว่า
เมตตาเป็นเสมือนกับกลไกผลป้อนกลับของชีวิต.
ลองจินตนาการดูว่า หากเราเป็นผู้น้อยอ่อนประสบการณ์ ผู้คนสามารถว่ากล่าวตักเตือน ให้คำแนะนำกับเราได้. 
แต่หากเราเป็นผู้ยิ่งใหญ่ที่สูงด้วยวัยวุฒิ ด้วยคุณวุฒิ ด้วยชื่อเสียง ด้วยเงินทอง ด้วยอำนาจ โดยไม่มีเกณฑ์ใดที่บังคับให้เราต้องฟังใคร และเราก็ไม่มีความจำเป็นต้องฟังใคร 
จะมีอะไรที่ทำให้เราต้องฟังคนอื่น? 
ณ ตอนนั้น มีเพียงความเมตตาความเห็นอกเห็นใจเท่านั้น ที่จะเป็นเสมือนช่องทางที่ยังจะเปิดรับฟังอยู่เสมอ ไม่ว่าช่องทางอื่น ๆ อาจจะถูกปิดไปแล้ว ปิดไปด้วยความสูงส่งของอำนาจ เกียรติยศ ศักดิ์ศรี ชื่อเสียง เงินทอง.
เราต้องการกลไก\textit{ผลป้อนกลับลบ} เพื่อเสถียรภาพที่ดีของสังคมและของตัวเราเอง
เพื่อที่จะยังสามารถรับฟังคำตักเตือน คำแนะนำ ความเห็นต่าง ๆ ได้อยู่เสมอ.
บ่อยครั้งที่เมตตาอาจช่วยให้ เราสามารถสังเกตและรับรู้ถึงความรู้สึกของผู้คนได้ ก่อนที่เขาจะต้องเอ่ยปากด้วยซ้ำ.

		\vspace{0.5cm}		
\begin{Parallel}[c]{0.45\textwidth}{0.4\textwidth}
	\selectlanguage{english}
	\ParallelLText{
		``Kindness in words creates confidence.\\
		Kindness in thinking creates profoundness.\\
		Kindness in giving creates love.''
		\begin{flushright}
			---Lao Tzu
		\end{flushright}
	}
	\selectlanguage{thai}
	\ParallelRText{
		``ความเมตตาในคำพูด สร้างความมั่นใจ.\\
		ความเมตตาในความคิด สร้างความลึกซึ้ง.\\
		ความเมตตาในการให้ สร้างความรัก''
		\begin{flushright}
			---เล่าจื๊อ
		\end{flushright}
	}
\end{Parallel}
\index{english}{words of wisdom!Lao Tzu}
\index{english}{quote!kindness}
\vspace{0.5cm}
		
การขาดเมตตานั้น ไม่ได้มีผลเฉพาะแค่ต่อการจำกัดปัญญา, ต่อการขาดระบบป้อนกลับ และต่อการลดประสิทธิภาพในการสื่อสารเท่านั้น.
เมตตาเป็นกลไกสำคัญในการลดและควบคุมอัตตา.
\textbf{อัตตา} (ego) หรือ \textit{มโนคติของตัวตน} (concept of self)
เป็นแนวโน้มและพฤติกรรม%
%การยึดตัวตนเป็นศูนย์กลาง (หรือพฤติกรรม
การยึดติดกับสิ่งที่จิตใช้เป็นตัวแทนของตัวตน
เป็นการยึดติดในตัวตน 
เป็นการยึดติดในความรู้สึกเป็นเจ้าของ.
อาจกล่าวโดยรวมได้ว่า เราทุกคนมีอัตตาอยู่ (ยกเว้นบุคคล เช่น อริยบุคคล ซึ่งเป็นผู้ไม่มีอัตตา)
เพียงแต่ว่า โดยส่วนใหญ่แล้ว ขนาดของอัตตาของเราไม่ได้ใหญ่จนรบกวนการดำเนินชีวิตมากจนเกินไป.
%(ข้อยกเว้นอาจมีได้ เช่น อริยบุคคล ซึ่งมีสี่ประเภท ได้แก่ โสดาบัน, สกทาคามี, อนาคามี และอรหันต์ เป็นผู้ที่ไม่มีอัตตาอยู่เลย.)
อย่างไรก็ตาม คนบางคนอาจมีอัตตาที่ใหญ่มาก ๆ 
และอาจใหญ่มากจนเข้าข่ายของ\textit{โรคหลงตัวเอง}.

\textbf{โรคหลงตัวเอง} (ความผิดปกติทางบุคลิกภาพแบบหลงตัวเอง ซึ่งภาษาอังกฤษคือ Narcissistic Personality Disorder คำย่อ NPD. เนื้อหาหลัก ๆ ในส่วนนี้ เรียบเรียงจาก \cite{MayoClinic})
คือ
%https://www.mayoclinic.org/diseases-conditions/narcissistic-personality-disorder/symptoms-causes/syc-20366662
%is a mental condition in which people have an inflated sense of their own importance, a deep need for excessive attention and admiration, troubled relationships, and a lack of empathy for others. But behind this mask of extreme confidence lies a fragile self-esteem that's vulnerable to the slightest criticism.
สภาพจิต ที่ผู้ป่วยรู้สึกว่าตัวเองเป็นคนสำคัญมาก, ชอบให้คนมาสนใจและชื่นชมมาก ๆ,
มีปัญหาความสัมพันธ์กับคนในครอบครัว และขาดความเห็นอกเห็นใจผู้อื่น.
ภายนอก ผู้ป่วยอาจดูเป็นคนที่มีความมั่นใจในตัวเองสูงมาก แต่ภายในแล้ว ผู้ป่วยมีความนับถือตัวเองในระดับที่เปราะบางมาก และทนไม่ได้กับการถูกวิพากษ์วิจารณ์.
\index{thai}{ความผิดปกติทางบุคลิกภาพแบบหลงตัวเอง}\index{thai}{โรคหลงตัวเอง}
\index{english}{Narcissistic Personality Disorder}

สัญญาณและอาการของโรค ได้แก่
คิดว่าตัวเองสำคัญมาก (มากเกินกว่าความเป็นจริง),
คิดว่าตัวเองสมควรจะถูกยกย่อง
และต้องการถูกชื่นชมอยู่ตลอดเวลา,
คิดว่าตัวเองต้องถูกยอมรับว่าเหนือกว่าคนอื่น ๆ โดยไม่ได้มีหลักฐานรูปธรรมรองรับ,
โอ้อวดความสำเร็จ พรสวรรค์ และความสามารถ,
หมกมุ่นและฝันเฟื้องกับการประสบความสำเร็จ อำนาจ ความเฉลียวฉลาด ความสวย หรือคู่ครองที่สมบูรณ์แบบ,
เชื่อว่าตัวเองดีกว่าคนอื่น ๆ และควรจะได้คบหาสมาคมกับคนพิเศษในระดับเดียวกัน,
จองพูดอยู่คนเดียวในวงสนทนา และการดูถูกคนอื่นที่คิดว่าต่ำต้อยกว่า,
เอาเปรียบคนอื่น เพื่อให้ได้สิ่งที่ตนต้องการ,
ไม่สามารถหรือไม่ยอมที่จะรับรู้ถึงความต้องการหรือความรู้สึกของคนอื่น,
อิจฉาคนอื่น หรือคิดว่าคนอื่น ๆ อิจฉาตัวเอง,
ก้าวร้าว หรือหยิ่งยโส ดูไม่จริงใจ ขี้โม้ และเสแสร้ง,
ยืนกรานที่จะได้สิ่งที่ดีที่สุด เช่น รถที่ดีที่สุด ที่ทำงานที่ดีที่สุด,
ไม่สามารถยอมรับการถูกวิพากษ์วิจารณ์ได้,
หงุดหงิดหรือโกรธ หากไม่ได้รับการต้อนรับปฏิบัติเป็นพิเศษ,
มีปัญหาการควบคุมอารมณ์,
มีปัญหาการจัดการกับความเครียด,
มีปัญหาการปรับตัวกับการเปลี่ยนแปลง,
รู้สึกเศร้าและไม่สบอารมณ์ เวลาไม่ได้ดั่งใจ
และแอบรู้สึกว่าไม่มั่นคง อ่อนแอ อาย อดสูขายหน้า.

ผู้ป่วยโรคหลงตัวเอง นอกจากจะสร้างความทุกข์ให้กับคนอื่นแล้ว
โรคอาจส่งผลกระทบกับตัวผู้ป่วยเอง ได้แก่
ปัญหาความสัมพันธ์ในครอบครัว,
ปัญหาที่โรงเรียน หรือที่ทำงาน,
ปัญหาภาวะซึมเศร้าและวิตกกังวล,
ปัญหาสุขภาพทางกาย,
ปัญหาการใช้ยาเสพติด หรือการดื่มสุรา
และพฤติกรรมการฆ่าตัวตาย.
คำแนะนำจากเมโยคลินิก สำหรับผู้ป่วยโรคหลงตัวเอง คือ การเข้าพบแพทย์. 
แต่โดยส่วนใหญ่แล้ว
ผู้มีความผิดปกติทางบุคลิกภาพ 
รวมถึงผู้ป่วยโรคหลงตัวเอง มักไม่คิดว่าตัวเองป่วย และมักไม่ยอมเข้ารับการรักษา.
%สิ่งที่น่าสนใจเกี่ยวกับการรักษาผู้ป่วยจิตเวชในประเทศไทย คือ
%บุคคลากรทางด้านจิตเวช (จากประสบการณ์ของผู้เขียน ) ไม่ยอมรับรู้ปัญหาของการไม่ยอมเข้ารับการรักษาของตัวผู้ป่วยเอง ซึ่งเป็นลักษณะพิเศษที่แตกต่างจากผู้ป่วยจากอาการทางกายภาพ
%และดูเหมือนจะพอใจกับสถานการณ์ที่เป็นอยู่.
%ถ้าอยากจะช่วยบรรเทาปัญหาสุขภาพจิตจริง ๆ ควรรับรู้ปัญหาของการไม่ยอมเข้ารับการรักษาของตัวผู้ป่วยจิตเวช และหาทางแก้ไขหรือบรรเทา
%แต่ถ้าแค่ทำตามหน้าที่ การที่ผู้ป่วยไม่เข้าโรงพยาบาลก็ย่อมอยู่นอกเหนือหน้าที่อยู่แล้ว
%คำถามคือ ต้องการจะช่วยบรรเทาปัญหาจริง ๆ หรือแค่ทำตามหน้าที่?

\paragraph{การลดอัตตา.}
ผู้ที่ป่วยแล้ว การเข้าพบแพทย์น่าจะดีที่สุด
แต่สำหรับ คนทั่วไป ที่อาจต้องการลดหรือควบคุมอัตตา
อาจทำได้ด้วยการพัฒนาเมตตาขึ้น.
การพัฒนาเมตตา อาจทำโดย
ฝึกให้อภัยคนอื่น ให้อภัยตัวเอง และปล่อยวางบ้าง,
ฝึกยอมรับความจริง ฝึกพูดความจริง และฝึกที่จะเปิดใจกว้างยอมรับความคิดความเห็นที่หลากหลาย,
ฝึกยอมรับความผิดของตัวเอง,
ฝึกลดหรือละความรู้สึกที่จะควบคุมทุกสิ่งทุกอย่างลง,
หาเวลาอยู่เงียบ ๆ สงบ ๆ คนเดียวบ้าง,
ฝึกชื่นชมความงามของสิ่งรอบตัว และมองเห็นความดีของคนอื่น ๆ,
ฝึกระลึกถึงบุญคุณหรือสิ่งดี ๆ ที่คนอื่น ๆ ทำให้เรา,
ฝึกช่วยเหลือคนอื่นบ้าง,
ฝึกทำดีกับคนแปลกหน้าบ้าง,
ลองเป็นจิตอาสาบ้าง,
ฝึกพูดสิ่งดี ๆ ให้กำลังใจคนอื่น,
ลด ละ เลิกการวิจารณ์คนอื่นและการเปรียบเทียบคน,
ฝึกที่จะไม่บ่น ไม่เสียดสี ไม่ประชดประชัน,
ฝึกมองโลกในแง่ดี,
ฝึกทักทายผู้คนอย่างยิ้มแย้มแจ่มใส,
ฝึกที่จะช่วยคนที่เดือดร้อนบ้าง หากมีโอกาส,
ฝึกที่จะถ่อมตัว,
ฝึกที่จะไม่พูดโอ้อวด รวมถึงลดหรือเลิกการโอ้อวด ผ่านสื่อสังคมออนไลน์,
ฝึกที่จะปล่อยให้คนอื่นได้รับความสนใจ ได้รับการชื่นชม,
ฝึกสมาธิอย่างสม่ำเสมอ,
แผ่เมตตาหรืออวยพรให้สรรพชีวิตอย่างสม่ำเสมอ,
แผ่เมตตาให้กับคนที่เราไม่ชอบหรือคนที่เราโกรธ,
พยายามมีสติรู้ถึงอารมณ์ที่เข้ามาในใจ,
พยายามควบคุมอารมณ์
และศึกษาพัฒนาตนทางด้านจิตวิญญาณบ้าง.

อัตตา มีลักษณะที่แปลก.
นั่นคือ ถ้าเราชอบคิดว่า เราดีกว่าคนอื่น นี่คืออัตตาสูง
และถ้าเราชอบคิดว่า เราแย่กว่าคนอื่น นี่ก็คืออัตตาสูง.
ตราบที่เรายังหมกมุ่นกับตัวเราเป็นสำคัญ นั่นคืออัตตาสูง.
สิ่งที่จะลดอัตตาได้ คือเมตตา (ภาพของสรรพชีวิตมีความสุข เราอาจจะยังอยู่ในภาพ แต่ไม่ได้เด่นอีกต่อไปแล้ว).

\paragraph{ไม่ได้รักษาความผิดปกติ แต่ดูแลส่วนที่ปกติ.}
สำหรับการรักษาผู้ป่วยอาการจิตเวช มีเรื่องเล่าที่น่าสนใจจากอาจารย์พรหม (Ajahn Brahm) ซึ่งเป็นพระนักเทศน์ นักบรรยาย และนักเขียนที่ได้รับการยอมรับนับถืออย่างกว้างขวาง 
ที่ท่านเคยถามเจ้าหน้าที่ในโรงพยาบาลจิตเวชแห่งหนึ่งว่า
เขารักษาความผิดปกติทางจิตอย่างไร
เจ้าหน้าที่ตอบว่า เขาไม่ได้รักษาส่วนที่ผิดปกติ เขารักษาส่วนที่ดี.

ผู้ป่วยจิตเวช ไม่ได้แสดงอาการผิดปกติออกมาตลอดเวลา.
ผู้ป่วยหลายคน ส่วนใหญ่ก็ปกติดี เพียงแค่มีช่วงเวลาที่เกิดอาการผิดปกติทางจิตขึ้นมาเท่านั้น.
สิ่งที่เจ้าหน้าจิตเวชทำ คือ พยายามรักษา ส่งเสริม ดูแล ให้ช่วงเวลาที่ดีอยู่ได้ยาวนานขึ้น ดูแลให้ส่วนที่ปกติเติบโตขึ้น
แล้วช่วงเวลาที่ผู้ป่วยเป็นปกติ จะยาวนานขึ้น และทำให้ช่วงเวลาผิดปกติสั้นลงไปเอง.
ความปกติถูกดูแล ถูกให้ความสำคัญ จนมันอยู่ได้นานขึ้น แข็งแรงมากขึ้น
ส่วนความผิดปกติจะเกิดน้อยลงและเบาลงเอง.
แนวทางนี้ไม่ใช่ใช้ได้เฉพาะกับผู้ป่วยจิตเวชหรอก 
ในตัวคนเรา ในชุมชน หรือในสังคมก็เช่นกัน ที่มีทั้งส่วนที่ดี และส่วนที่ไม่ดี
ถ้าเรารักษา ดูแล ส่งเสริมให้ส่วนที่ดีเติบโตขึ้นแข็งแรงขึ้น
ส่วนที่ไม่ดีมันจะน้อยลง เบาลงเอง.

		\vspace{0.5cm}		
		\begin{Parallel}[c]{0.5\textwidth}{0.45\textwidth}
			\selectlanguage{english}
			\ParallelLText{
				``When life is good do not take it for granted as it will pass. Be mindful, be compassionate and nurture the circumstances that find you in this good time so it will last longer. When life falls apart always remember that this too will pass. Life will have its unexpected turns.''
				\begin{flushright}
					---Ajahn Brahm
				\end{flushright}
			}
			\selectlanguage{thai}
			\ParallelRText{
				``ตอนที่ชีวิตดี ใส่ใจกับมัน เพราะมันจะผ่านไป.
				มีสติรับรู้ มีเมตตา และทะนุถนอมสิ่งต่าง ๆ ที่ช่วยให้เราได้มีช่วงเวลาที่ดี เพื่อให้เวลาดี ๆ มีได้นานขึ้น.
				ตอนที่ชีวิตแตกเป็นเสี่ยง ๆ จำไว้เสมอว่า เวลานั้นมันก็จะผ่านไปเหมือนกัน.
				ชีวิตจะมีการเปลี่ยนแปลงที่คาดไม่ถึงเสมอ.''
				\begin{flushright}
					---อาจารย์พรหม
				\end{flushright}
			}
		\end{Parallel}
		\index{english}{words of wisdom!Lao Tzu}
		\index{english}{quote!kindness}
		
		
	\end{shaded}
}%small

\paragraph{ข้อดีข้อเสียของโครงข่ายประสาทเวียนกลับ.}
โครงข่ายประสาทเวียนกลับ
สามารถประมวลผลชุดข้อมูลลำดับได้โดยไม่จำกัดความยาวของลำดับ
โดยที่ความซับซ้อนของแบบจำลอง ไม่ขึ้นกับความยาวของลำดับ (ดูแบบฝึกหัด~\ref{ex: seq RNN cf ANN} และ~\ref{ex: seq RNN cf CNN} ประกอบ)
และที่สำคัญ คือ การใช้ค่าน้ำหนักร่วม สำหรับทุก ๆ ลำดับเวลา.
อย่างไรก็ตาม ข้อเสียของโครงข่ายประสาทเวียนกลับ 
คือ การคำนวณใช้เวลามาก (การประมวลผลแบบขนานทำได้ลำบาก)
และ
โครงข่ายประสาทเวียนกลับ 
ยังถูกรายงานบ่อย ๆ ว่าจำลองความสัมพันธ์ระยะยาวระหว่างจุดข้อมูลได้ไม่ดี
และไม่สามารถจำลองความสัมพันธ์กับจุดข้อมูลลำดับข้างหน้า หรือลำดับเวลาในอนาคต (ดูหัวข้อ~\ref{sec: bi-RNN} ประกอบ).

นอกจากการฝึก\textit{โครงข่ายประสาทเวียนกลับ}ที่ใช้เวลามากแล้ว
การฝึก\textit{โครงข่ายประสาทเวียนกลับ} 
ยังมี\textit{ปัญหาการเลือนหายของเกรเดียนต์}
และ\textit{ปัญหาการระเบิดของเกรเดียนต์}.
การเวียนกลับย้อนลำดับเวลา ให้ผลคล้ายการแพร่กระจายย้อนกลับผ่านชั้นคำนวณต่าง ๆ ของโครงข่ายประสาทเชิงลึก
(ดูแผนภาพคลี่ลำดับ เช่น รูป~\ref{fig: rnn unfolding diagram} ประกอบ)
แต่จุดต่างที่สำคัญคือ
เมื่อย้อนกลับผ่านชั้นคำนวณ ค่าน้ำหนักของชั้นคำนวณแต่ละชั้น เป็นอิสระต่อกัน
แต่เมื่อย้อนกลับผ่านลำดับเวลา ค่าน้ำหนักที่ลำดับเวลาต่าง ๆ เป็นชุดเดียวกัน.
%
กลไกการเวียนกลับ ส่งผลต่อเสถียรภาพของการคำนวณค่าเกรเดียนต์
ซึ่งบางครั้งเกิดปัญหาในลักษณะการเลือนหายของเกรเดียนต์
\index{thai}{ปัญหาการเลือนหายของเกรเดียนต์}
\index{english}{vanishing gradient problem}
ที่เกรเดียนต์มีค่าลดลงอย่างมาก เมื่อเวียนกลับย้อนลำดับเวลา จนไม่สามารถเชื่อมโยงความสัมพันธ์ระยะยาวได้.
%ที่ลำดับทำการฝึกได้อย่างมีประสิทธิผลได้.
แต่บางครั้ง
การฝึก\textit{โครงข่ายประสาทเวียนกลับ}
อาจเห็นปัญหาในลักษณะของการระเบิดของเกรเดียนต์.
\textbf{ปัญหาการระเบิดของเกรเดียนต์} (exploding gradient problem)
\index{english}{exploding gradient problem}\index{thai}{ปัญหาการระเบิดของเกรเดียนต์}
ที่พบกับการฝึก\textit{โครงข่ายประสาทเวียนกลับ}
คือ
การที่เกรเดียนต์มีค่าเพิ่มขั้นอย่างมาก เมื่อเวียนกลับย้อนลำดับเวลา
จนทำให้การคำนวณเสียเสถียรภาพ และการฝึกล้มเหลวในที่สุด.

\textit{ปัญหาการเลือนหายของเกรเดียนต์}
ในโครงข่ายประสาทเวียนกลับ
สามารถบรรเทาลงได้ด้วยกลไกต่าง ๆ 
เช่นที่เป็นส่วนประกอบของ\textit{แบบจำลองความจำระยะสั้นที่ยาว} (หัวข้อ~\ref{sec: RNN lstm}).
ส่วน\textit{ปัญหาการระเบิดของเกรเดียนต์}
สามารถบรรเทาลงได้ง่าย ๆ ด้วย\textit{การเล็มเกรเดียนต์}.

\textbf{การเล็มเกรเดียนต์} (gradient clipping)
\index{english}{gradient clipping}
\index{thai}{การเล็มเกรเดียนต์}
เป็น กลไกง่ายในการลดขนาดเกรเดียนต์ลง ให้อยู่ในระดับที่การคำนวณจะยังสามารถทำต่อไปได้โดยมีเสถียรภาพ.
พาสคานูและคณะ\cite{PascanuEtAl2013} ปรับขนาดของเกรเดียนต์ลงให้ไม่เกินค่าที่กำหนด
โดย
%นั่นคือ
%
\begin{eqnarray}
\mbox{ถ้า} & \| \bm{g} \| > \tau & \mbox{แล้ว}
\nonumber \\
          & \bm{g} \leftarrow \frac{\bm{g} \cdot \tau}{ \| \bm{g} \|} &
\label{eq: gradient clipping} 
\end{eqnarray}
เมื่อ $\bm{g}$ คือ เกรเดียนต์ นั่นคือ $\bm{g} \equiv \nabla_{\bm{\theta}} E$
และ $\| \bm{g} \|$ คือ ขนาดของเกรเดียนต์.
ส่วนสเกล่าร์ $\tau$ คือ ค่าขีดแบ่งที่กำหนด.
ค่าขีดแบ่ง $\tau$ สามารถเลือกได้ง่าย ๆ เพียงเป็นค่าที่ไม่มากเกินไปที่จะทำให้ระบบเสียเสถียรภาพเท่านั้น
เช่น อาจเลือกให้ $\tau = 1$ เหมือนที่พาสคานูและคณะใช้ในการทดลองก็ได้.

พาสคานูและคณะ ใช้วิธีปรับลงขนาดของเกรเดียนต์ทั้งเวคเตอร์ ทำให้แม้ลดขนาดของเวคเตอร์ลง แต่ทิศทางของเกรเดียนต์ยังคงเดิม.


\section{โครงข่ายประสาทเวียนกลับสองทาง}
\label{sec: bi-RNN}

โครงข่ายประสาทเวียนกลับ นำจุดข้อมูลลำดับก่อนหน้ามาร่วมพิจารณาผลการทำนายที่ลำดับเวลาปัจจุบัน
ช่วยให้เราสามารถสร้างแบบจำลองความสัมพันธ์ของจุดข้อมูลลำดับปัจจุบัน กับจุดข้อมูลต่าง ๆ ในลำดับก่อนหน้าได้.
อย่างไรก็ตาม ภารกิจกับข้อมูลเชิงลำดับหลายอย่าง อาจต้องการจำลองความสัมพันธ์ระหว่างจุดข้อมูลลำดับปัจจุบันกับจุดข้อมูลในลำดับหลัง ๆ
เช่น กรณีภารกิจการระบุหมวดคำ ในรูป~\ref{fig: nlp POS tagging} 
การระบุหมวดคำของ\textit{โทเค็น} \texttt{one} ที่ถูกต้อง ต้องการรู้\textit{โทเค็น}ต่าง ๆ ที่ตามมาในภายหลัง
นั่นคือ
สำหรับ ``... if \underline{one} has the courage to admit them.''
คำว่า ``one'' ทำหน้าที่เป็นสรรพนาม
แต่ถ้าสำหรับ 
``... if \underline{one} day you can let it go.''
คำว่า ``one'' ทำหน้าที่เป็นตัวเลข.
โครงข่ายประสาทเวียนกลับ ที่อาศัยเฉพาะแต่ความสัมพันธ์กับลำดับที่ผ่านมา ไม่อาจแก้ปัญหาลักษณะนี้ได้.

วิธีบรรเทาปัญหาลักษณะนี้อย่างง่าย ๆ ก็คือ การใช้กลไก\textit{หน้าต่างเวลา} (time-window)
ที่จับกลุ่ม\textit{โทเค็น}หลาย ๆ โทเค็นรวมกันเป็นจุดข้อมูลแต่ละจุด สำหรับโครงข่ายประสาทเวียนกลับ.
อย่างไรก็ตาม แนวทางการใช้กลไก\textit{หน้าต่างเวลา}นี้
อาศัยกรอบ\textit{หน้าต่างเวลา} ที่มีความยาวคงที่ ทำให้จำกัดความสัมพันธ์ระยะยาวระหว่างจุดข้อมูล.
อีกแนวทางง่าย ๆ ก็คือ การหน่วงเวลาระหว่างจุดข้อมูลลำดับของอินพุต กับจุดข้อมูลลำดับของเอาต์พุต
แต่แนวทางนี้ ก็ยังต้องอาศัยการเลือกระยะเวลาหน่วงที่เหมาะสม.

แนวทางหนึ่ง ที่ถูกออกแบบและพบว่า\cite{Graves2012} ใช้ได้ดี
สำหรับกรณีเช่นนี้
คือ \textbf{โครงข่ายประสาทเวียนกลับสองทาง} (bidirectional recurrent neural networks\cite{SchusterPaliwal1997}).
\index{thai}{โครงข่ายประสาทเวียนกลับสองทาง}
\index{english}{bidirectional recurrent neural networks}
กลไกที่สำคัญของ\textit{โครงข่ายประสาทเวียนกลับสองทาง}
คือ เพิ่มชั้นคำนวณเวียนกลับที่รับชุดลำดับที่เรียงกลับหลัง.
การคำนวณเอาต์พุตสุดท้ายของโครงข่าย จะรอจนกว่า\textit{ชั้นคำนวณเวียนกลับ} (ทั้งชั้นที่รับชุดลำดับเรียงหน้าไปหลัง และชั้นที่รับชุดลำดับเรียงหลังไปหน้า) 
จะได้ประมวลผลครบทุกจุดข้อมูลในชุดลำดับก่อน.

\begin{figure}
	\begin{center}		
		\includegraphics[width=0.5\textwidth]{08seq/RNN/bidirectionalRNN.png}	
		\caption[แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับสองทาง]{แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับสองทาง.
		}
		\label{fig: rnn unfolding diagram BRNN}
	\end{center}
\end{figure}
%

รูป~\ref{fig: rnn unfolding diagram BRNN} แสดงแผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับสองทาง.
การคำนวณของหน่วยย่อยในชั้นคำนวณเวียนกลับทิศทางย้อนกลับที่ $q^{th}$ อาจเขียนได้ดังนี้
\begin{eqnarray}
\tilde{a}_j^{(q)}(t) 
&=& \sum_{i=1}^{\tilde{D}} \tilde{w}_{ji}^{(q)} \cdot \tilde{z}_{i}^{(q-1)}(t) + \sum_{m=1}^{\tilde{M}} \tilde{v}_{jm}^{(q)} \cdot \tilde{z}_m^{(q)}(t+1) + \tilde{b}_j^{(q)}
\label{eq: BRNN backward direction layer a} \\
\tilde{z}_j^{(q)}(t) &=& h(\tilde{a}_j^{(q)}(t))
\label{eq: BRNN backward direction layer z}
\end{eqnarray}
เมื่อ $\tilde{a}_j^{(q)}(t)$
และ $\tilde{z}_j^{(q)}(t)$
คือ
ค่าตัวกระตุ้นและผลการกระตุ้น
ของหน่วยย่อยที่ $j^{th}$ 
ในชั้นคำนวณ $q^{th}$
สำหรับจุดข้อมูลลำดับที่ $t^{th}$
โดย $\tilde{D}$ คือจำนวนหน่วยย่อยในชั้น $(q-1)^{th}$
และ $\tilde{M}$ คือจำนวนหน่วยย่อยในชั้น $q^{th}$.
ตัวแปร $\tilde{w}_{jd}^{(q)}$ เป็นค่าน้ำหนักของการเชื่อมต่อระหว่างหน่วยที่ $d^{th}$ ของชั้น $(q-1)^{th}$ กับหน่วยที่ $j^{th}$ ของชั้นคำนวณ $q^{th}$.
ตัวแปร $\tilde{v}_{jm}^{(q)}$ เป็นค่าน้ำหนักของการเชื่อมต่อของหน่วยที่ $m^{th}$ เวียนกลับมาเข้าหน่วยที่ $j^{th}$ ของชั้นคำนวณเดียวกัน,
$\tilde{b}_j^{(q)}$ คือค่าไบอัสของหน่วยที่ $j^{th}$
และ $h(\cdot)$ คือฟังก์ชันกระตุ้น.

สังเกตสมการ~\ref{eq: BRNN backward direction layer a}
เปรียบเทียบกับสมการ~\ref{eq: RNN feedforward a} 
ซึ่งเป็นการคำนวณ\textit{ค่าตัวกระตุ้น} ในชั้นคำนวณเวียนกลับ ทิศทางปกติ (ทิศทางไปข้างหน้า)
จะเห็นว่า
จุดสำคัญคือ
การที่\textit{ค่าตัวกระตุ้น} ในชั้นคำนวณเวียนกลับ ทิศทางย้อนกลับ ได้รับอิทธิพลจาก ผลการกระตุ้นของลำดับเวลาอนาคต $\tilde{z}_m^{(q)}(t+1)$.

การคำนวณของหน่วยย่อยในชั้นรวมผลของทั้งสองทิศทาง (เช่น ชั้นเอาต์พุต ที่คำนวณค่า $\bm{y}$ ในรูป~\ref{fig: rnn unfolding diagram BRNN})
ก็สามารถดำเนินการได้ เช่นเดียวกับการคำนวณหน่วยย่อยใน\textit{ชั้นคำนวณเชื่อมต่อเต็มที่}ทั่ว ๆ ไป
นั่นคือ
\begin{eqnarray}
\hat{a}_j^{(q)}(t) 
&=& 
\sum_{i=1}^D w_{ji}^{(q)} \cdot z_{i}^{(q-1)}(t)
+ \sum_{i=1}^{\tilde{D}} \tilde{w}_{ji}^{(q)} \cdot \tilde{z}_{i}^{(q-1)}(t)
+ \hat{b}_j^{(q)}
\label{eq: BRNN combine layer a} \\
\hat{z}_j^{(q)}(t) &=& h(\hat{a}_j^{(q)}(t))
\label{eq: BRNN combine layer z}
\end{eqnarray}
เมื่อ
$\hat{a}_j^{(q)}(t)$
และ
$\hat{z}_j^{(q)}(t)$
คือ
ค่าตัวกระตุ้นและผลการกระตุ้น ของชั้นที่รวมผลจากการคำนวณเวียนกลับทั้งสองทิศทาง
โดย
$z_{i}^{(q-1)}(t)$
และ
$\tilde{z}_{i}^{(q-1)}(t)$
คือผลการกระตุ้น
จากชั้นเวียนกลับทิศทางไปข้างหน้า และทิศทางย้อนกลับ ตามลำดับ.
ส่วน $w_{ji}^{(q)}$, 
$\tilde{w}_{ji}^{(q)}$
และ $\hat{b}_j^{(q)}$
คือพารามิเตอร์ของชั้นคำนวณ.

เพื่อให้การคำนวณค่าเอาต์พุตของโครงข่ายประสาทเวียนกลับสองทาง
เป็นไปโดยเรียบร้อย
การคำนวณ (การแพร่กระจายไปข้างหน้า) ดำเนินการตามลำดับดังนี้
%
\begin{itemize}
	\item คำนวณค่าผลการกระตุ้น จากชั้นเวียนกลับทิศทางไปข้างหน้า 
	โดยคำนวณตามลำดับเวลาจาก $t = 1$ ไป $t = T$.
	นั่นคือ คำนวณค่า $z_{i}^{(q-1)}(t)$ สำหรับ $t = 1, \ldots, T$ ตามลำดับ.
	\item คำนวณค่าผลการกระตุ้น จากชั้นเวียนกลับทิศทางย้อนกลับ 
	โดยคำนวณตามลำดับเวลาจาก $t = T$ ไป $t = 1$.
	นั่นคือ คำนวณค่า $\tilde{z}_{i}^{(q-1)}(t)$ สำหรับ $t = T, \ldots, 1$ ตามลำดับ.
	\item คำนวณชั้นที่รวมผลจากสองทิศทาง.
	นั่นคือ คำนวณค่า $\hat{z}_j^{(q)}(t)$ สำหรับทุก ๆ ค่าของ $t$ (ลำดับใดก็ได้).
%	โดยใช้ผลจาก  $z_{i}^{(q-1)}(t)$ และ $\tilde{z}_{i}^{(q-1)}(t)$
\end{itemize}

การฝึกโครงข่ายประสาทเวียนกลับสองทาง
ก็สามารถทำได้ในลักษณะเดียวกับการฝึกโครงข่ายประสาทเวียนกลับ
เพียงมีความซับซ้อนเพิ่มขั้น
เนื่องจาก
(1) การเวียนกลับมีสองทิศทาง
และ (2) การเวียนกลับทั้งสองทิศทาง เปรียบเสมือนส่วนประกอบในชั้นคำนวณเดียวกัน
เพราะรับอินพุตจากชั้นเดียวกัน และให้เอาต์พุตออกไปที่ชั้นเดียวกัน.

การคำนวณในการฝึกชั้นเวียนกลับสองทาง (ชั้น $q^{th}$)  สรุปได้ดังนี้
\begin{itemize}
\item (1) คำนวณการแพร่กระจายไปข้างหน้า
	\begin{itemize}	
	\item (1.0) คำนวนชั้น $(q-1)^{th}$ \\
	ได้ $\hat{z}_i^{(q-1)}(t)$
	สำหรับทุก ๆ $i$ และทุก ๆ $t$. \\
	ถ้าชั้น $(q-1)^{th}$ เป็นชั้นอินพุต 
	$\hat{z}_i^{(q-1)}(t) = x_i(t)$.
	
	\item (1.1) คำนวณการเวียนกลับทิศทางไปข้างหน้า ($t = 1, \ldots, T$ ตามลำดับ)\\	
	ได้ $a_j^{(q)}(t)$ 
	%	$= \sum_{i=1}^D w_{ji}^{(q)} \cdot z_{i}^{(q-1)}(t) + \sum_{m=1}^M v_{jm}^{(q)} \cdot z_m^{(q)}(t-1) + b_j^{(q)}$
	และ
	$z_j^{(q)}(t)$ สำหรับทุก ๆ $j$
	%	$= h(a_j^{(q)}(t))$
	(สมการ~\ref{eq: RNN feedforward a}	
	และ~\ref{eq: RNN feedforward z})

	\item (1.2) คำนวณการเวียนกลับทิศทางกลับหลัง ($t = T, \ldots, 1$ ตามลำดับ)\\
	ได้ $\tilde{a}_j^{(q)}(t)$
	และ
	$\tilde{z}_j^{(q)}(t)$ สำหรับทุก ๆ $j$
	(สมการ~\ref{eq: BRNN backward direction layer a}
	และ~\ref{eq: BRNN backward direction layer z})

	\item (1.3) คำนวณชั้น $(q+1)^{th}$ \\		
	ได้ $\hat{a}_k^{(q+1)}(t)$
	และ
	$\hat{z}_k^{(q+1)}(t)$ สำหรับทุก ๆ $k$ และทุก ๆ $t$. \\
	(สมการ~\ref{eq: BRNN combine layer a}
	และ~\ref{eq: BRNN combine layer z} 
	ถ้าชั้น $(q+1)^{th}$ เป็นชั้นเชื่อมต่อเต็มที่)

	\item คำนวณชั้นต่อ ๆ ไป จนได้เอาต์พุตสุดท้าย 		
	\end{itemize}

\item (2) คำนวณการแพร่กระจายย้อนกลับ
	\begin{itemize}
	\item (2.0) คำนวณการแพร่กระจายย้อนกลับจนถึงชั้น $(q+1)^{th}$\\
	ได้ $\hat{\delta}_k^{(q+1)}(t) \equiv \frac{\partial E}{\partial \hat{a}_k^{(q+1)}(t)}$
	สำหรับทุก ๆ $k$ และทุก ๆ $t$.
	
	\item (2.1) คำนวณการแพร่กระจายย้อนกลับสำหรับทิศทางไปข้างหน้า
	(แต่การคำนวณต้องทำจาก $t=T$ ไป $t=1$)\\
	ได้ $\delta_j^{(q)}(t)$,
	$\frac{\partial E}{\partial w_{ji}^{(q)}}$,
	$\frac{\partial E}{\partial v_{jm}^{(q)}}$ และ
	$\frac{\partial E}{\partial b_j^{(q)}}$
	สำหรับทุก ๆ $i$, $j$ และ $m$
	(สมการ~\ref{eq: RNN delta},
	\ref{eq: RNN derivative dE/dw refined},
	\ref{eq: RNN derivative dE/dv refined} และ
	\ref{eq: RNN derivative dE/db})
		
	\item (2.2) คำนวณการแพร่กระจายย้อนกลับสำหรับทิศทางกลับหลัง
	(การคำนวณต้องทำจาก $t=1$ ไป $t=T$)\\
	\begin{eqnarray}
	\tilde{\delta}_j^{(q)}(t) 
	&\equiv& \frac{\partial E}{\partial \tilde{a}_j^{(q)}(t)}
	\nonumber \\
	&=& h'(\tilde{a}_j^{(q)}(t))
	\cdot \left(\sum_k \hat{\delta}_k^{(q+1)}(t) \cdot \tilde{w}_{kj}^{(q+1)} 
	+ \sum_m  \tilde{\delta}_m^{(q)}(t-1) \cdot \tilde{v}_{mj}^{(q)}\right)
	\nonumber \\
	\label{eq: BRNN backprop delta} \\
	%
	\frac{\partial E}{\partial \tilde{w}_{ji}^{(q)}}
	&=& \sum_t \tilde{\delta}_j^{(q)}(t) \cdot \hat{z}_i^{(q-1)}(t)
	\label{eq: BRNN backprop dEw} \\
	%
	\frac{\partial E}{\partial \tilde{v}_{jm}^{(q)}}
	&=& \sum_t \tilde{\delta}_j^{(q)}(t) \cdot \tilde{z}_m^{(q)}(t-1)
	\label{eq: BRNN backprop dEv} \\
	\frac{\partial E}{\partial \tilde{b}_j^{(q)}}
	&=&
	\sum_t \tilde{\delta}_j^{(q)}(t) 
	\label{eq: BRNN backprop dEb}
	\end{eqnarray}
	สำหรับทุก ๆ $i$, $j$ และ $m$
	
	\item แพร่กระจายย้อนกลับต่อไปจนครบทุกชั้น\\
	ถ้าชั้น $(q-1)^{th}$ เป็นชั้นเชื่อมต่อเต็มที่
	แล้ว
	\begin{eqnarray}
	\hat{\delta}_i^{(q-1)}(t) 
	&\equiv& \frac{\partial E}{\partial \hat{a}_i^{(q-1)}(t)}
	\nonumber \\
	&=& h'(\hat{a}_i^{(q-1)}(t))
	\cdot \left(\sum_j \delta_j^{(q)}(t) \cdot w_{ji}^{(q)}
	+ \sum_j \tilde{\delta}_j^{(q)}(t) \cdot \tilde{w}_{ji}^{(q)} 
	\right)
	\nonumber \\
	\label{eq: BRNN backprop previous delta}.
	\end{eqnarray}	
	\end{itemize}

\item (3) ปรับค่าน้ำหนักตามเกรเดียนต์ที่คำนวณได้
\end{itemize}

โครงข่ายประสาทเวียนกลับสองทาง
มีความสามารถในการเชื่อมความสัมพันธ์ ทั้งความสัมพันธ์กับลำดับในอดีต และลำดับในอนาคต.
อย่างไรก็ตาม
การใช้งานโครงข่ายประสาทเวียนกลับสองทาง
เหมาะกับ
(1) ลักษณะชุดข้อมูลที่ต้องการจำลองความสัมพันธ์กับลำดับในอนาคต
และ
(2) ภารกิจที่ต้องการเอาต์พุต หลังจากได้เห็นชุดข้อมูลครบทุกลำดับแล้ว.
หากลักษณะชุดข้อมูลไม่ได้ต้องการความสัมพันธ์กับลำดับในอนาคต
การคำนวณที่เพิ่มขึ้นของโครงข่ายประสาทเวียนกลับสองทาง
จะกลายเป็นภาระที่ไม่จำเป็น.
%
หากภารกิจต้องการเอาต์พุตก่อนที่แบบจำลองจะได้เห็นข้อมูลครบลำดับ
โครงข่ายประสาทเวียนกลับสองทาง (ในรูปแบบดั้งเดิมนี้)
จะไม่เหมาะสมกับภารกิจนั้น
และอาจพิจารณาทางเลือกอื่น เช่น กลไกหน้าต่างเวลา หรือ การหน่วงเวลาระหว่างอินพุตและเอาต์พุต.

\section{แบบจำลองความจำระยะสั้นที่ยาว}
\label{sec: RNN lstm}
\index{english}{lstm}
\index{thai}{แบบจำลองความระยะสั้นที่ยาว}

ดังข้อดีข้อเสียของโครงข่ายประสาทเวียนกลับ
ที่ได้อภิปรายไปว่า
\textit{โครงข่ายประสาทเวียนกลับ}
มีปัญหาการเลือนหายของเกรเดียนต์
ที่ทำให้โครงข่ายประสาทเวียนกลับ
ไม่สามารถเชื่อมโยงความสัมพันธ์ระยะยาว (ความสัมพันธ์ระหว่างจุดข้อมูลที่ลำดับต่างกันมาก).

ปัญหาเรื่องนี้
นำไปสู่การพัฒนาวิธีแก้ต่าง ๆ มากมาย
และหนึ่งในนั้น
คือ
\textit{แบบจำลองความจำระยะสั้นที่ยาว}
ที่ปัจจุบัน ได้รับการยอมรับอย่างกว้างขวาง.
%แบบจำลองโครงข่ายประสาทเทียม
%ที่ออกแบบมาเพื่อรู้จำความสัมพันธ์ระยะยาวได้ดีขึ้น
%และแบบจำลองที่สามารถจัดการปัญหานี้ได้ดี คือ 

\textbf{แบบจำลองความจำระยะสั้นที่ยาว} 
(long short-term memory model\cite{HochreiterEtAl2001a, HochreiterSchmidhuber1997, GersEtAl1999, GersEtAl2002, GreffEtAl2017} ที่มักย่อ LSTM)
\index{english}{lstm}
\index{thai}{แบบจำลองความระยะสั้นที่ยาว}
คือ 
โครงข่ายประสาทเวียนกลับ
ที่เพิ่มกลไกควบคุมความจำค่าใหม่ ควบคุมการลืมค่าเก่า และควบคุมการระลึกความจำไปใช้ อย่างชัดเจน.
รูป~\ref{fig: lstm block}
แสดงโครงสร้างของ\textit{แบบจำลองความจำระยะสั้นที่ยาว}.
\textit{แบบจำลองความจำระยะสั้นที่ยาว}
มีกลไกภายในหน่วยคำนวณย่อยที่ซับซ้อนกว่า
\textit{หน่วยย่อย}ของ\textit{โครงข่ายประสาทเทียม}ทั่วไปอยู่มาก
ดังนั้น เพื่อกันการสับสน
หน่วยคำนวณย่อยที่ครอบคลุมแนวคิด\textit{แบบจำลองความจำระยะสั้นที่ยาว}
จะเรียกว่า \textit{บล็อกความจำ} (LSTM block).
\index{english}{lstm block}
\index{english}{lstm!block}
\index{thai}{บล็อกความจำ}
\index{thai}{แบบจำลองความจำระยะสั้นที่ยาว!บล็อกความจำ}
ภายใน\textit{บล็อกความจำ}
มีหน่วยความจำ เรียก 
\textit{เซลล์} (cell)
หรือ\textit{เซลล์ความจำ} (memory cell).
\index{english}{lstm cell}
\index{english}{lstm!cell}
\index{thai}{เซลล์ความจำ}
\index{thai}{แบบจำลองความจำระยะสั้นที่ยาว!เซลล์}
กลไกการเก็บความจำของเซลล์นี้ คือ จุดเด่นของ\textit{แบบจำลองความจำระยะสั้นที่ยาว}
ซึ่งบรรยายดังสมการ~\ref{eq: LSTM forget gate} ถึง~\ref{eq: LSTM cell output}.

\begin{figure}
	\begin{center}		
	\includegraphics[height=2.5in]{08seq/LSTM/LSTM_control.png}	
		\caption[แผนภาพโครงสร้างบล็อกความจำของแบบจำลองความจำระยะสั้นที่ยาว]{แผนภาพโครงสร้างบล็อกความจำของแบบจำลองความจำระยะสั้นที่ยาว.
		วงกลม แทนหน่วยคำนวณ ดังเช่นแผนภาพโครงข่ายประสาทเทียมอื่น ๆ.
		สามเหลี่ยม แทนกลไกของประตูควบคุม ที่ควบคุมการไหลของสารสนเทศ 
		โดยการปิดเปิดประตู ขึ้นกับสัญญาณจากอินพุตเวลาปัจจุบัน และผลลัพธ์ที่ผ่านมา.
		ดูแผนภาพคลี่ลำดับ (รูป~\ref{fig: lstm unfolding diagram}) ประกอบ.
		}
		\label{fig: lstm block}
	\end{center}
\end{figure}
%

เมื่อ ชุดลำดับอินพุต คือ $[\bm{x}(1), \ldots, \bm{x}(T)]$
การคำนวณของ\textit{บล็อกความจำ} ดำเนินการดังนี้
\begin{eqnarray}
\bm{f}(t) &=& \sigma(\bm{W}_f \cdot [\bm{z}(t-1); \bm{x}(t)] + \bm{b}_f) 
\label{eq: LSTM forget gate} \\
\bm{u}(t) &=& \sigma(\bm{W}_u \cdot [\bm{z}(t-1); \bm{x}(t)] + \bm{b}_u) 
\label{eq: LSTM input gate} \\
\bm{o}(t) &=& \sigma(\bm{W}_o \cdot [\bm{z}(t-1); \bm{x}(t)] + \bm{b}_o) 
\label{eq: LSTM output gate} \\
\tilde{\bm{c}}(t) &=& \mathrm{tanh}(\bm{W}_c \cdot [\bm{z}(t-1); \bm{x}(t)] + \bm{b}_c) 
\label{eq: LSTM new cell} \\
\bm{c}(t) &=& \bm{f}(t) \odot \bm{c}(t-1) + \bm{u}(t) \odot \tilde{c}(t) 
\label{eq: LSTM update cell} \\
\bm{z}(t) &=& \bm{o}_t \odot \mathrm{tanh}(\bm{c}(t)) 
\label{eq: LSTM cell output} 
\end{eqnarray}
เมื่อ 
ตัวดำเนินการ $\odot$ หมายถึง \textit{การคูณแบบตัวต่อตัว} (element-wise product)
\index{english}{element-wise product}
\index{thai}{การคูณแบบตัวต่อตัว}
และตัวดำเนินการ $[\;\cdot\; ; \;\cdot\;]$ หมายถึงการนำค่าเวคเตอร์ต่อกัน
นั่นคือ
ถ้า $\bm{v}^{(1)} = [v^{(1)}_1, \ldots, v^{(1)}_M]^T$
และ $\bm{v}^{(2)} = [v^{(2)}_1, \ldots, v^{(2)}_N]^T$
แล้ว
$[\bm{v}^{(1)}; \bm{v}^{(2)}]$
$= [v^{(1)}_1, \ldots, v^{(1)}_M, v^{(2)}_1, \ldots, v^{(2)}_N]^T$.
%ถ้า $\bm{v}^{(3)} = [\bm{v}^{(1)}; \bm{v}^{(2)}]$
%แล้ว $v_i^{(3)}$ ซึ่งคือ ส่วนประกอบที่ $i^{th}$ ของเวคเตอร์ $\bm{v}_3$
%จะเป็น
%
ตัวแปร $\bm{z}(t)$ เป็นผลลัพธ์ของบล็อค (สำหรับลำดับเวลา $t$).

ตัวแปร $\bm{c}(t)$ ทำหน้าที่เป็นค่าความจำของเซลล์ ที่ลำดับเวลา $t$.
ส่วนตัวแปร $\tilde{\bm{c}}(t)$ เป็นสารสนเทศใหม่ ที่ลำดับเวลา $t$.
ตัวแปร $\bm{f}(t)$, $\bm{u}(t)$ และ $\bm{o}(t)$
ทำหน้าที่เป็นเสมือนประตูควบคุมการไหลของสารสนเทศ.
ประตูลืม (forget gate) และประตูรับค่าใหม่ (update gate) 
ซึ่งคือ $\bm{f}(t)$ และ $\bm{u}(t)$ ตามลำดับ
ควบคุมว่า จะให้เซลล์รับจำสารสนเทศใหม่ $\tilde{\bm{c}}(t)$
หรือคงความจำเดิม $\bm{c}(t-1)$.
ประตูผลลัพธ์ (output gate) $\bm{o}(t)$
ควบคุมว่า ควรจะระลึกความจำออกมาหรือไม่.
ค่าอินพุต $\bm{x}(t)$ และค่าผลลัพธ์ที่ผ่านมา $\bm{z}(t-1)$ ควบคุมการทำงานของประตูต่าง ๆ.
รูป~\ref{fig: lstm unfolding diagram}
แสดงแผนภาพคลี่ลำดับของบล็อกความจำ.

\begin{figure}
	\begin{center}		
		\includegraphics[height=2.5in]{08seq/LSTM/LSTM_unfolded.png}	
		\caption[แผนภาพคลี่ลำดับของแบบจำลองความจำระยะสั้นที่ยาว]{แผนภาพคลี่ลำดับของแบบจำลองความจำระยะสั้นที่ยาว สำหรับสองลำดับเวลา.
		}
		\label{fig: lstm unfolding diagram}
	\end{center}
\end{figure}
%

นอกจากนั้น
เพืื่อปรับการควบคุมประตูได้แม่นยำยิ่งขึ้น
โครงสร้างของบล็อกความจำ อาจมีกลไก\textbf{ช่องแอบมอง}
(peephole connections\cite{GersEtAl2002}) เพิ่มเข้ามาด้วย.
\index{thai}{ช่องแอบมอง}
\index{english}{peephole}
\index{thai}{แบบจำลองความจำระยะสั้นที่ยาว!ช่องแอบมอง}
\index{english}{lstm!peephole}
กลไกของ\textit{ช่องแอบมอง} จะเพิ่มค่าของความจำที่ผ่านมา เข้ามาเป็นส่วนในการควบคุมประตูต่าง ๆ ด้วย
ดังสมการ~\ref{eq: LSTM forget gate peephole} ถึง~\ref{eq: LSTM output gate peephole}.
%
\begin{eqnarray}
\bm{f}(t) & = \sigma(\bm{W}_f \cdot [\bm{z}(t-1); \bm{x}(t); \bm{c}(t-1)] + \bm{b}_f) 
\label{eq: LSTM forget gate peephole} \\
\bm{u}(t) & = \sigma(\bm{W}_u \cdot [\bm{z}(t-1); \bm{x}(t); \bm{c}(t-1)] + \bm{b}_u) 
\label{eq: LSTM input gate peephole} \\
\bm{o}(t) & = \sigma(\bm{W}_o \cdot [\bm{z}(t-1); \bm{x}(t); \bm{c}(t-1)] + \bm{b}_o) 
\label{eq: LSTM output gate peephole}.
\end{eqnarray}


%BREAK HERE!!!


%LATER
%\section{โครงข่ายเวียนกลับมีประตู}
%โครงข่ายเวียนกลับมีประตู หรือ
%หน่วยเวียนกลับมีประตู (Gated Recurrent Unit\cite{ChungEtAl2014a} คำย่อ GRU)
%\index{หน่วยเวียนกลับมีประตู}
%\index{โครงข่ายเวียนกลับมีประตู}
%\index{gru}



%
%
%\begin{eqnarray}
%\frac{\partial L}{\partial h_t} &=& \frac{\partial L}{\partial v_t} 
%\cdot \frac{\partial v_t}{\partial h_t} 
%= \frac{\partial L}{\partial v_t} 
%\cdot W_v
%\end{eqnarray}
%
%

%ค่าเกรเดียนต์ของพารามิเตอร์ต่างๆ
%\begin{eqnarray}
%\frac{\partial L}{\partial W_v} &=& \frac{\partial L}{\partial v_t} \cdot \frac{\partial v_t}{\partial W_v}
%=  \frac{\partial L}{\partial v_t} \cdot h_t
%\label{eq: RNN lstm W_v grad} \\
%\frac{\partial L}{\partial b_v} &=& \frac{\partial L}{\partial v_t}
%\label{eq: RNN lstm b_v grad} \\
%%
%\frac{\partial L}{\partial W_o} &=& \frac{\partial L}{\partial o_t} \cdot \frac{\partial o_t}{\partial W_o}
%\label{eq: RNN lstm W_o grad} \\
%\frac{\partial L}{\partial b_o} &=& \frac{\partial L}{\partial o_t} \cdot \frac{\partial o_t}{\partial b_o}
%\label{eq: RNN lstm b_o grad} \\
%%
%\frac{\partial L}{\partial W_C} &=& \frac{\partial L}{\partial C_t} \cdot \frac{\partial C_t}{\partial W_C}
%\label{eq: RNN lstm W_C grad} \\
%\frac{\partial L}{\partial b_C} &=& \frac{\partial L}{\partial C_t} \cdot \frac{\partial C_t}{\partial b_C}
%\label{eq: RNN lstm b_C grad} \\
%%
%\frac{\partial L}{\partial W_f} &=& \frac{\partial L}{\partial f_t} \cdot \frac{\partial f_t}{\partial W_f}
%\label{eq: RNN lstm W_f grad} \\
%\frac{\partial L}{\partial b_f} &=& \frac{\partial L}{\partial f_t} \cdot \frac{\partial f_t}{\partial b_f}
%\label{eq: RNN lstm b_f grad} \\
%%
%\frac{\partial L}{\partial W_i} &=& \frac{\partial L}{\partial i_t} \cdot \frac{\partial i_t}{\partial W_i}
%\label{eq: RNN lstm W_i grad} \\
%\frac{\partial L}{\partial b_i} &=& \frac{\partial L}{\partial i_t} \cdot \frac{\partial i_t}{\partial b_i}
%\label{eq: RNN lstm b_i grad} \\
%%
%\end{eqnarray}
%
%
%กลไก\textit{ความใส่ใจ}
%
%(Youtube, Leo Dirac, LSTM is dead. Long live Transformer!)
%"LSTM is dead. Love live Transformers", Leo Dirac on 12 Nov 2019. Sea-ADL.org
%Seattle Applied Deep Learning
%
%how to represent text aka how to turn text into numeric data
%* bag of word
%** con: loss of word order
%* N-gram
%** con: bi-gram: O(N^2), N-gram: O(N^N)
%* RNN
%** vanishing and exploding gradient
%* LSTM
%** limitations: 
%*** difficult to train, e.g., LSTM on 100-word doc ~100-layer net
%*** transfer learning never really works.
%*** needs labet set for every task.
%* Transformers & Muppets
%** "Attention is all you need" paper
%** translation: encoder-decoder
%** query -> key
%** multi-headed attention: Q, K, V
%** position encoding
%** all-to-all comparison -> parallel
%** N^2 but extra parallel can be free.
%** does not require sigmoid/tanh
%
%Transformer --> Transformer (Attention is all you need), BERT, GPT-3
%and reformer (hardware-efficient transformer)
%
%
% causal reasoning, symbolic rule formation, rapid abstraction, and commonsense representations of events in terms of objects, agents and their interactions.
%
%ADAM, SGD, SWA
%GELU
%https://github.com/huggingface/transformers
%** re-usable
%*** Megatron LM
%*** XLM-Roberta
%** key advantages
%*** easier to train
%*** transfer learning works
%**** pre-trained model can be fine tuned.
%
%
%# from Break into NLP by Deeplearning AI
%@18:35 Andrew Ng introductory talk
%* NLP (today)
%- search
%- summarization
%- autocomplete
%- anti-spam
%- machine translation
%- smart speakers/assistants
%- chatbots
%* NLP (future)
%- education (autograding, feedback)
%- email tools
%- synthesis academic papers
%- flexible robotic process automation
%
%Unsupervised learning
%* Self-taught learning
%--> learn from unlabeled data > transfer to other task
%
%
%
%
%word CNN
%- good for bi-gram - tri-gram
%
%transformer
%
%lstm is still good when
%* seq length long or infinite
%** Transformers are N^2.
%* E.g., real-time control for Robotics
%* cannot pre-train 
%* on large corpus
%
%\section{โมเดลความจำระยะสั้นที่ยาว}
%\label{sec: RNN lstm}
%\index{lstm}
%\index{โมเดลความระยะสั้นที่ยาว}
%
%การคำนวณ
%
%*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*
%
%เพื่อความสะดวกต่อ $h_{t-1}$ และ $x_t$
%\begin{eqnarray}
%z & = [h_{t-1}, x_t] \\
%\end{eqnarray}
%
%การคำนวณ LSTM functions
%\begin{eqnarray}
%f_t & = \sigma(W_f \cdot z + b_f) \\
%i_t & = \sigma(W_i \cdot z + b_i) \\
%\bar{C}_t & = tanh(W_C \cdot z + b_C) \\
%C_t & = f_t * C_{t-1} + i_t * \bar{C}_t \\
%o_t & = \sigma(W_o \cdot z + b_t) \\
%h_t &= o_t * tanh(C_t) \\
%\end{eqnarray}
%
%นำค่าสถานะซ่อนไปคำนวณเอาต์พุต
%% Logits
%\begin{eqnarray}
%v_t &= W_v \cdot h_t + b_v \\
%\end{eqnarray}
%
%ทำซอฟต์แมกซ์
%% Softmax
%\begin{eqnarray}
%\hat{y_t} &= \mathrm{softmax}(v_t) \label{eq: RNN lstm softmax}
%\end{eqnarray}
%
%$\hat{y_t}$ is `y` in code and $y_t$ is `targets`.
%
%การคำนวณหาเกรเดียนต์
%
%% Loss
%
%\begin{eqnarray}
%L_k &= -\sum_{t=k}^T\sum_j y_{t,j} log \hat{y_{t,j}} \\
%L &= L_1 \\
%\end{eqnarray}
%
%Total loss $L$ is an accumulated loss starting from period $t=1$, therefore $L = L_1$.
%
%ก่อนจะพิจารณาเกรเดียนต์ของพารามิเตอร์ต่างๆ 
%พิจารณาเกรเดียนต์ของสัญญาณต่างๆดังนี้ โดยเริ่มจากปลายท้ายสุด
%
%\begin{eqnarray}
%\frac{\partial L}{\partial v_t} &=& -\frac{\partial \sum_{\tau =1}^T \sum_j y_{\tau, j} \log \hat{y}_{\tau, j} 
%}{\partial v_t}
%\end{eqnarray}
%
%เนื่องจาก $\frac{\partial \hat{y}_{\tau,j}}{\partial v_t} = 0$ เมื่อ $\tau \neq ่t$ ดังนั้น
%
%\begin{equation}
%\frac{\partial L}{\partial v_t} 
%=
%-\frac{\partial 
%	\sum_j y_{t, j} \log \hat{y}_{t, j} 
%}{\partial v_t}
%\nonumber \\
%\end{equation}
%
%จากสมการ~\ref{eq: RNN lstm softmax} เมื่อแทนค่าและหาอนุพันธ์จะได้
%
%\begin{equation}
%\frac{\partial L}{\partial v_t} 
%=
%\hat{y}_t - y_t
%\label{eq: RNN lstm dv_t}
%\end{equation}
%
%BREAK HERE
%
%
%\begin{eqnarray}
%\frac{\partial L}{\partial h_t} &=& \frac{\partial L}{\partial v_t} 
%\cdot \frac{\partial v_t}{\partial h_t} 
%= \frac{\partial L}{\partial v_t} 
%\cdot W_v
%\end{eqnarray}
%
%
%
%ค่าเกรเดียนต์ของพารามิเตอร์ต่างๆ
%\begin{eqnarray}
%\frac{\partial L}{\partial W_v} &=& \frac{\partial L}{\partial v_t} \cdot \frac{\partial v_t}{\partial W_v}
%=  \frac{\partial L}{\partial v_t} \cdot h_t
%\label{eq: RNN lstm W_v grad} \\
%\frac{\partial L}{\partial b_v} &=& \frac{\partial L}{\partial v_t}
%\label{eq: RNN lstm b_v grad} \\
%%
%\frac{\partial L}{\partial W_o} &=& \frac{\partial L}{\partial o_t} \cdot \frac{\partial o_t}{\partial W_o}
%\label{eq: RNN lstm W_o grad} \\
%\frac{\partial L}{\partial b_o} &=& \frac{\partial L}{\partial o_t} \cdot \frac{\partial o_t}{\partial b_o}
%\label{eq: RNN lstm b_o grad} \\
%%
%\frac{\partial L}{\partial W_C} &=& \frac{\partial L}{\partial C_t} \cdot \frac{\partial C_t}{\partial W_C}
%\label{eq: RNN lstm W_C grad} \\
%\frac{\partial L}{\partial b_C} &=& \frac{\partial L}{\partial C_t} \cdot \frac{\partial C_t}{\partial b_C}
%\label{eq: RNN lstm b_C grad} \\
%%
%\frac{\partial L}{\partial W_f} &=& \frac{\partial L}{\partial f_t} \cdot \frac{\partial f_t}{\partial W_f}
%\label{eq: RNN lstm W_f grad} \\
%\frac{\partial L}{\partial b_f} &=& \frac{\partial L}{\partial f_t} \cdot \frac{\partial f_t}{\partial b_f}
%\label{eq: RNN lstm b_f grad} \\
%%
%\frac{\partial L}{\partial W_i} &=& \frac{\partial L}{\partial i_t} \cdot \frac{\partial i_t}{\partial W_i}
%\label{eq: RNN lstm W_i grad} \\
%\frac{\partial L}{\partial b_i} &=& \frac{\partial L}{\partial i_t} \cdot \frac{\partial i_t}{\partial b_i}
%\label{eq: RNN lstm b_i grad} \\
%%
%\end{eqnarray}
%
%\section{Word Representation}
%
%GloVe\cite{PSM2014a}
%
%\section{กลไกความใส่ใจ}
%\label{sec: attention}
%
%Translation
%\cite{BahdanauEtAl2015}
%
%Encoder-Decoder
%\cite{ChoEtAl2014a}
%
%
%
%Q and A
%
%\cite{reddy2019coqa}
%QnA Wikipedia \cite{ChenEtAl2017a}
%FusionNet\cite{HuangEtAl2017a}
%
%
%Story telling
%\cite{see2019massively}
%
%\section{ลักษณะพิเศษของภาษาไทยและความท้าทายในงานประมวลผลภาษาธรรมชาติ}
%\label{sec: Thai NLP Challenges}
%
%
%\cite{ThaiNLPTalk}

\section{การใช้งานโครงข่ายประสาทเวียนกลับ}
\label{sec: RNN configurations}
โครงข่ายประสาทเวียนกลับ%
\footnote{%
ณ ที่นี้	
โครงข่ายประสาทเวียนกลับ จะหมายรวมถึง
\textit{โครงข่ายประสาทเวียนกลับ}ทุก ๆ ชนิด
ซึ่งรวมถึง แบบจำลองความจำระยะสั้นที่ยาว และหน่วยเวียนกลับมีประตู ด้วย.
\index{english}{lstm}\index{thai}{แบบจำลองความจำระยะสั้นที่ยาว}
\index{thai}{หน่วยเวียนกลับมีประตู}\index{english}{gru}
}	
 สามารถใช้เป็นแบบจำลองในการรู้จำรูปแบบเชิงลำดับแบบต่าง ๆ ดังอภิปรายในหัวข้อ~\ref{sec: seq data}.
ตัวอย่างเช่น
กรณีที่ทั้งอินพุตและเอาต์พุตเป็นชุดลำดับ และมีจำนวนลำดับเท่ากัน เช่น การระบุหมวดคำ 
\index{english}{Part-Of-Speech Tagging}
\index{thai}{การระบุหมวดคำ}
(ดูแบบฝึกหัด~\ref{ex: nlp POS})
\textit{โครงข่ายประสาทเวียนกลับ} สามารถนำมาใช้ในกรณีนี้ได้อย่างตรงไปตรงมา.
กระบวนการฝึก อาจกำหนดให้ $\mathcal{M}(t) = 1$ สำหรับทุก ๆ ลำดับเวลา $t$.
แผนภาพคลี่ลำดับ สำหรับกรณีนี้ แสดงในรูป~\ref{fig: rnn unfolding diagram many to many}.

\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/unfolding_manyin_manyout.png}	
		\caption[แผนภาพคลี่ลำดับ กรณีที่ทั้งอินพุตและเอาต์พุตเป็นชุดลำดับ]{แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับ สำหรับกรณีที่ทั้งอินพุตและเอาต์พุตเป็นชุดลำดับ และมีจำนวนลำดับเท่ากัน.		
		}
		\label{fig: rnn unfolding diagram many to many}
	\end{center}
\end{figure}
%

กรณีการจำแนกลำดับ เช่น การจำแนกอารมณ์ (ดูแบบฝึกหัด~\ref{ex: nlp sentiment analysis})
ที่อินพุตเป็นข้อมูลชุดลำดับ แต่เอาต์พุตไม่ได้เป็นชุดลำดับ
นั่นคือ อินพุต $\bm{X} =\{\bm{x}(1), \ldots, \bm{x}(T)\}$ และเอาต์พุต $\bm{y}$.
กรณีนี้อาจดำเนินการโดยกำหนดให้ ค่าทำนายที่ลำดับเวลาสุดท้าย เป็นเอาต์พุต และค่าทำนายต่าง ๆ ที่ลำดับเวลาอื่น ๆ ไม่มีความสำคัญ.
กระบวนการฝึก ในกรณีการจำแนกลำดับ ก็สามารถทำได้สะดวก โดยการใช้กลไก\textit{หน้ากาก} ที่กำหนดให้ $\mathcal{M}(T) = 1$ และ $\mathcal{M}(t \neq T) = 0$. 
รูป~\ref{fig: rnn unfolding diagram seq classification} แสดงแผนภาพคลี่ลำดับ สำหรับกรณีการจำแนกลำดับ.

\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/unfolding_oneout.png}	
		\caption[แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับ กรณีการจำแนกลำดับ]{แผนภาพคลี่ลำดับ กรณีการจำแนกลำดับ.
			จุดสำคัญอยู่ที่ การเลือกเฉพาะเอาต์พุตที่ต้องการไปใช้ (ค่าที่ทำนาย $\hat{\bm{y}} = \bm{y}(T)$).
			แม้ค่าแบบจำลองจะยังคงให้เอาต์พุตอื่น ๆ $\bm{y}(t \neq T)$ ออกมาด้วย เพียงแต่เอาต์พุตเหล่านี้ไม่ได้ถูกนำไปใช้ทำอะไร (แผนภาพใช้สีจาง เพื่อสื่อถึงการปล่อยค่าเอาต์พุตเหล่านี้ทิ้ง).
		}
		\label{fig: rnn unfolding diagram seq classification}
	\end{center}
\end{figure}
%

กรณีที่เอาต์พุตเป็นชุดลำดับ แต่อินพุตไม่ใช่ เช่น ระบบแต่งเพลงอัตโนมัติ (ดูแบบฝึกหัด~\ref{ex: seq music generation})
โครงข่ายประสาทเวียนกลับ ก็อาจสามารถนำมาประยุกต์ใช้ได้โดยจัดโครงสร้างดังแสดงในรูป~\ref{fig: rnn unfolding diagram one to many}.
การจัดโครงสร้างที่นิยม สำหรับกรณีเช่นนี้ 
คือ ใช้อินพุตของระบบ เป็นอินพุตที่ลำดับเวลาแรกสุดของโครงข่ายประสาทเวียนกลับ 
และหลังจากนั้น ใช้เอาต์พุตที่คำนวณได้ มาเป็นอินพุตสำหรับลำดับเวลาถัดไป.	
การฝึกโครงข่ายประสาทเวียนกลับ สำหรับกรณีเช่นนี้ ซึ่งเป็นลักษณะของการใช้งานแบบจำลองก่อกำเนิด
ก็สามารถใช้แนวทางของงานแบบจำลองก่อกำเนิด เช่น แนวทางของ\textit{โครงข่ายปรปักษ์เชิงสร้าง}ได้ (ดูหัวข้อ~\ref{sec: convapp fix enhance gen images} ประกอบ). 	

\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/unfolding_onein.png}	
		\caption[แผนภาพคลี่ลำดับ กรณีที่อินพุตไม่ใช่ชุดลำดับ]{แผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับ ที่นิยมใช้สำหรับกรณีที่เอาต์พุตเป็นชุดลำดับ แต่อินพุตไม่ใช่.
			อินพุตจริงของระบบ $\tilde{\bm{x}}$ จะถูกนำเข้าเป็นจุดข้อมูลที่ลำดับเวลาแรก นั่นคือ $\bm{x}(1) = \tilde{\bm{x}}$
			และหลังจากลำดับเวลาแรก เอาต์พุตที่คำนวณได้ จะถูกนำเป็นอินพุตสำหรับลำดับเวลาถัดไป.	
			ภาพ	แสดง $\bm{x}(2)$ ถึง $\bm{x}(T)$ ด้วยสีจาง เพื่อสื่อถึงว่า ค่าอินพุตที่ลำดับเวลาเหล่านี้ ไม่ใช่อินพุตของระบบ แต่เป็นค่าที่ถูกสร้างขึ้นในกระบวนการ. 
		}
		\label{fig: rnn unfolding diagram one to many}
	\end{center}
\end{figure}
%

สุดท้าย
กรณีที่ทั้งอินพุตและเอาต์พุตเป็นชุดลำดับ แต่มีจำนวนลำดับอาจไม่เท่ากัน 
เช่น ภารกิจการแปลภาษาอัตโนมัติ
ที่จำนวนคำในประโยคของภาษาต้นทาง อาจไม่เท่ากับจำนวนคำในประโยคของภาษาเป้าหมาย.
กรณีนี้ บ่อยครั้งอาจถูกอ้างถึงเป็น ภารกิจ\textit{จำลองแบบชุดลำดับเป็นชุดลำดับ} (sequence-to-sequence modeling task)
เป็น สถานการณ์ที่ท้าทายอย่างมาก โดยเฉพาะ เมื่อการทำนายชุดลำดับของเอาต์พุต จำเป็นต้องเห็นอินพุตครบทุกลำดับก่อน.
\index{english}{sequence-to-sequence modeling}
\index{thai}{การจำลองแบบชุดลำดับเป็นชุดลำดับ}
แนวทางหนึ่ง คือการใช้\textbf{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส} (encoder-decoder architecture\cite{ChoEtAl2014a})
หรือบางครั้งอาจเรียก \textbf{สถาปัตยกรรมแปลงชุดลำดับไปชุดลำดับ} (sequence-to-sequence architecture\cite{SutskeverEtAl2014})
\index{thai}{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส}
\index{english}{encoder-decoder architecture}
\index{english}{sequence-to-sequence architecture}
ดังแสดงในรูป~\ref{fig: rnn unfolding diagram encoder-decoder coded hidden state}
(ดูแบบฝึกหัด~\ref{ex: seq encoder-decoder configuration} เพิ่มเติม).

\begin{figure}
	\begin{center}		
		\includegraphics[height=2in]{08seq/RNN/unfolding_MT3.png}	
		\caption[แผนภาพคลี่ลำดับของสถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส]{แผนภาพคลี่ลำดับของสถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส.
			สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส ใช้โครงข่ายประสาทเวียนกลับสองตัว ตัวหนึ่งทำหน้าที่เข้ารหัส (encoder แบบจำลองทางซ้ายในภาพ) อีกตัวหนึ่งทำหน้าที่ถอดรหัส (decoder แบบจำลองทางขวาในภาพ).
			ตัวเข้ารหัส สรุปเนื้อความของชุดลำดับอินพุต ไว้เป็น\textit{รหัสเนื้อความ} แล้วส่งรหัสเนื้อความนี้ไปให้ตัวถอดรหัส เพื่อถอดออกมาเป็นชุดลำดับเอาต์พุต.			
		}
		\label{fig: rnn unfolding diagram encoder-decoder coded hidden state}
	\end{center}
\end{figure}
%

\textit{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส} 
คล้ายการรวมแบบจำลองสองตัว.
ตัวแรก สำหรับชุดลำดับอินพุต (เรียก ตัวเข้ารหัส)
และตัวที่สอง สำหรับชุดลำดับเอาต์พุต (เรียก ตัวถอดรหัส).
\textit{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส} อาศัยกลไกของ\textit{รหัสเนื้อความ} (code) หรืออาจเรียก \textit{บริบท} (context) ในการสรุปความหมายของชุดลำดับอินพุตไว้
แล้วค่อยถอด\textit{รหัสเนื้อความ}ออกมาเป็นอีกชุดลำดับ.

โดยทั่วไป ผลการกระตุ้นลำดับสุดท้ายของตัวเข้ารหัส เช่น $\bm{z}(T_x)$ ในรูป~\ref{fig: rnn unfolding diagram encoder-decoder coded hidden state} จะถูกใช้เป็น\textit{รหัสเนื้อความ}.
ตัวถอดรหัส อาจรับ\textit{รหัสเนื้อความ} มาเป็นส่วนหนึ่งของผลการกระตุ้นเริ่มต้นของตัวถอดรหัส
เช่น $\tilde{\bm{z}}(0) = [\bm{z}(T_x); \bm{0}]$ โดย $\bm{0}$ อาจเป็นค่าเริ่มต้นที่เติมเข้าไป
เพื่อให้เต็มขนาด (ขนาดสถานะภายในของตัวถอดรหัส อาจใหญ่กว่าขนาดของ\textit{รหัสเนื้อความ}ได้).
อินพุตลำดับแรกของตัวถอดรหัส $\tilde{\bm{x}}(1)$ 
อาจกำหนดด้วยค่า $\bm{0}$.
%นิยมกำหนดด้วยค่าสุ่ม.
ดูแบบฝึกหัด~\ref{ex: seq encoder-decoder configuration} เพิ่มเติม สำหรับการอภิปรายโครงสร้างของสถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัสแบบอื่น.

โดยทั่วไป ขนาดสถานะภายในของตัวถอดรหัส ไม่เล็กกว่าขนาดสถานะภายในของตัวเข้ารหัส 
นั่นคือ $|\bm{z}(t)| \leq |\tilde{\bm{z}}(t)|$.
การกำหนดขนาดสถานะภายในของตัวถอดรหัส ให้ใหญ่กว่าขนาดรหัส ช่วยให้ตัวถอดรหัสเสมือนมีความจำเหลือพอที่จะใช้งานอื่น ๆ ได้ (เช่น อาจจะเก็บสถานะของการทำงาน ณ ลำดับเวลาปัจจุบัน).

แม้\textit{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส} จะสามารถช่วยให้ชุดลำดับของเอาต์พุตมีจำนวนลำดับที่เป็นอิสระจากจำนวนลำดับของอินพุต
และยังช่วยให้การทำนายชุดลำดับของเอาต์พุต ได้เห็นอินพุตครบทุกลำดับก่อน
แต่\textit{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส} ใช้กลไกของ\textit{รหัส} ในการส่งผ่านความหมายสรุปจากชุดลำดับอินพุต
ไปสร้างชุดลำดับเอาต์พุต.
ดังนั้น ขนาดของ\textit{รหัส} 
มีผลโดยตรงต่อความสามารถในการแทนความหมาย.
ในกรณีของระบบการแปลภาษาอัตโนมัติ 
ขนาดของ\textit{รหัส}ที่เล็กเกินไป จะส่งผลลบต่อคุณภาพการแปลอย่างชัดเจน โดยเฉพาะกับการแปลประโยคยาว ๆ\cite{ChoEtAl2014b, BahdanauEtAl2015} (ดูแบบฝึกหัด~\ref{ex: seq encoder-decoder code length} เพิ่มเติม.
หัวข้อ~\ref{sec:attention mechanism} อภิปราย\textit{กลไกความใส่ใจ} ซึ่งพัฒนามาเพื่อแก้ข้อจำกัดนี้).
\index{english}{attention mechanism}
\index{thai}{กลไกความใส่ใจ}

\paragraph{การจัดโครงสร้างเชิงลึกของโครงข่ายประสาทเวียนกลับ.}
โครงข่ายประสาทเวียนกลับ (สมการ~\ref{eq: RNN feedforward a} และ ~\ref{eq: RNN feedforward z})
สามารถถูกจัดโครงสร้างแบบลึกได้ (รูป~\ref{fig: deep RNN}).
อย่างไรก็ตาม
ผู้เชี่ยวชาญศาสตร์การเรียนรู้ของเครื่อง
แอนดรูว์ อึ้ง\cite{Ng2020a} ให้ข้อสังเกตว่า
แม้ปัจจุบัน โครงข่ายประเทียมเชิงลึกอาจมีจำนวนชั้นคำนวณเป็นหลักร้อยชั้นได้ (เช่น เรสเน็ต\cite{He_2016_CVPR})
แต่สำหรับโครงข่ายประสาทเวียนกลับ โดยเฉพาะชั้นคำนวณเวียนกลับ มักต่อกันไม่เกินสามชั้น%
\footnote{%
	ข้อสังเกตนี้ ตั้งขึ้นจากสภาพสิ่งแวดล้อม และความนิยมในปัจจุบัน.
	ประวัติและวิวัฒนาการของโครงข่ายประสาทเทียม 
	เช่น การใช้งาน\textit{\atom{เพอร์เซปตรอน}หลายชั้น} (บทที่~\ref{chapter: ANN})
	ในช่วงเวลาก่อนปี ค.ศ. 2012 (ที่โครงข่ายประสาทเทียมเชิงลึกเริ่มได้รับความสนใจอย่างกว้างขวาง)
	ก็มีความเชื่อว่า ไม่มีความจำเป็นที่จะต้องใช้โครงข่ายที่มีจำนวนชั้นคำนวณเกินกว่าสองชั้น.
	ดังนั้น ข้อสังเกตนี้ ผู้เขียนบันทึกไว้ เพื่อผู้อ่านจะได้พอเห็นภาพการนำไปใช้งาน 
	แต่หวังว่า ผู้อ่านจะไม่ยึดติดจนเกินไป
	%สำหรับการวิจัยและพัฒนา 
	ดังที่ประวัติศาสตร์สอนได้ไว้ตลอดมา.
	นวัตกรรม ความก้าวหน้า พัฒนาการที่สำคัญ (breakthrough) เกิดจากการปล่อยวาง มากกว่าการยึดติด.
}
เหตุผลส่วนหนึ่ง อาจเป็นพราะกลไกการเวียนกลับได้เพิ่มความสามารถของแบบจำลองขึ้นอย่างมาก
(รวมถึงเพิ่มความต้องการการคำนวณขึ้นอย่างมหาศาล โดยเฉพาะในกระบวนการฝึก).
แต่โครงสร้างเชิงลึกมักพบ คือการใช้\textit{โครงข่ายประสาทเวียนกลับ} ที่มีชั้นเวียนกลับ (หนึ่งถึงสามชั้น) แล้วต่อด้วยชั้นคำนวณ (ที่ไม่มีกลไกเวียนกลับ) หลาย ๆ ชั้น (รูป~\ref{fig: deep RNN with non-recurrent layers}).


\begin{figure}
	\begin{center}		
		\includegraphics[width=0.75\columnwidth]{08seq/RNN/deepRNN.png}	
		\caption[โครงข่ายประสาทเวียนกลับแบบลึก]{โครงสร้างและแผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับแบบลึก.
			ตัวอย่างนี้ แสดงการใช้ชั้นเวียนกลับสามชั้นต่อกัน.			
		}
		\label{fig: deep RNN}
	\end{center}
\end{figure}



\begin{figure}
	\begin{center}		
		\includegraphics[width=0.75\columnwidth]{08seq/RNN/deepstckedRNN.png}	
		\caption[โครงข่ายประสาทเวียนกลับแบบลึก ที่ใช้ชั้นคำนวณไม่เวียนกลับจำนวนมาก]{โครงสร้างและแผนภาพคลี่ลำดับของโครงข่ายประสาทเวียนกลับแบบลึก
			ที่ใช้ชั้นคำนวณไม่เวียนกลับจำนวนมาก.			
		}
		\label{fig: deep RNN with non-recurrent layers}
	\end{center}
\end{figure}

\section{กลไกความใส่ใจ}
\label{sec:attention mechanism} 

สำหรับภารกิจ\textit{จำลองแบบชุดลำดับเป็นชุดลำดับ}
เช่น การแปลภาษาอัตโนมัติ (machine translation), 
การสรุปข้อความ (text summarization),
แชทบอต (chatbot)
และระบบตอบคำถาม (question answering)
\textit{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส}
สามารถทำงานได้ดี 
แต่ความสามารถของระบบลดลงอย่างมาก เมื่อชุดลำดับอินพุตมีความยาวมาก ๆ.
คณะของบาห์ดาโน\cite{BahdanauEtAl2015}
เชื่อว่าข้อจำกัดนี้ เกิดจากปัญหาคอขวดที่การใช้\textit{รหัสเนื้อความ} ซึ่งมีความยาวจำกัด
และได้เสนอวิธีการแก้ด้วย\textit{กลไกความใส่ใจ}.

แนวคิดของ\textit{กลไกความใส่ใจ} พัฒนาจากภารกิจการแปลภาษาอัตโนมัติ
ซึ่งสังเกตว่า เวลาที่คนแปลภาษา แม้จะเป็นการแปลจากประโยคยาว ๆ ในภาษาต้นทาง
แต่เวลาแปล ถอดความออกมาเป็นคำ ๆ ในภาษาปลายทาง
ผู้แปล แม้จะเห็นทั้งประโยค แต่เวลาพิจารณาเลือกคำ แต่ละคำ ที่จะใช้สำหรับประโยคปลายทาง
ผู้แปลจะใส่ใจกับเฉพาะบางส่วนของประโยคต้นทางเท่านั้น.

ตัวอย่างเช่น
ประโยคต้นทาง (ภาษาอังกฤษ) คือ
``Knowing that \textit{what is} cannot be undone---because it already is --- you say yes to \textit{what is} or accept what isn't.''%
\footnote{จาก Eckhart Tolle, The Power of Now.}
%
\\
ประโยคปลายทาง (ภาษาไทย) คือ
``การรู้ว่า \textit{สิ่งที่เป็น} ไม่สามารถเปลี่ยนแปลงได้ (เพราะว่ามันเป็นไปแล้ว)
คุณยอมรับกับ \textit{สิ่งที่เป็น} หรือยอมรับ สิ่งที่ไม่ได้เป็น.''

เอาต์พุต ``การรู้ว่า'' อาจมาจากโทเค็น ``Knowing'' และโทเค็น ``that'' เป็นหลัก
โดยส่วนอื่น ๆ ของประโยคต้นทางมีความเกี่ยวข้องต่ำ.

%\paragraph{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส.}


\paragraph{การทำงานของกลไกความใส่ใจ.}
แทนที่จะอาศัยการส่งเนื้อความจากชุดลำดับอินพุต ไปชุดลำดับเอาต์พุต ผ่าน\textit{รหัสเนื้อความ}ที่มีความยาวจำกัด
และให้อิสระกับ\textit{ตัวถอดรหัส}ในการจัดเรียงเอาต์พุตเอง
\textbf{กลไกความใส่ใจ} (attention mechanism)
\index{english}{attention mechanism}
\index{thai}{กลไกความใส่ใจ}
กำหนด\textit{บริบท}สำหรับแต่ละลำดับ ให้กับ\textit{ตัวถอดรหัส}โดยตรง.

ด้วยชุดลำดับอินพุต $\bm{X} = [\bm{x}(1), \ldots, \bm{x}(T_x)]$
สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส คำนวณ\textit{รหัสเนื้อความ}
\begin{eqnarray}
\bm{c} = e'(\bm{z}(1), \ldots, \bm{z}(T_x))
\label{eq: encoder-decoder encoding context}
\end{eqnarray}
เมื่อ 
\begin{eqnarray}
\bm{z}(t) = e(\bm{z}(t-1), \bm{x}(t), \bm{z}(t+1))
\label{eq: encoder-decoder encoding hidden states}
\end{eqnarray}
โดย $e'(\cdot)$ และ $e(\cdot)$ เป็นฟังก์ชันของตัวเข้ารหัส
ที่ทำหน้าที่สรุป และจำลองแบบเชิงลำดับ.
ตัวอย่างเช่น
$e'(\bm{z}(1), \ldots, \bm{z}(T_x)) = \bm{z}(T_x)$
และ $e(\cdot)$ เป็นโครงข่ายประสาทเวียนกลับสองทาง. 
%ฟังก์ชัน $
%คณะของซุตส์เกเวอร์\cite{SutskeverEtAl2014} ใช้ 
%$e'(\bm{z}(1), \ldots, \bm{z}(T_x)) = \bm{z}(T_x)$
%และ $e(\cdot)$ เป็นแบบจำลองความจำระยะสั้นที่ยาว 
%(ดูรูป~\ref{fig: rnn unfolding diagram encoder-decoder coded hidden state}).

ในสถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัสแบบดั่งเดิม (รูป~\ref{fig: nlp encoder-decoder without attention})
ตัวถอดรหัสรับ\textit{รหัสเนื้อความ} $\bm{c}$
และระบบประมาณความน่าจะเป็นของเอาต์พุตที่ลำดับ $t$ 
ด้วย 
\begin{eqnarray}
p(\bm{y}(t) | \{\bm{y}(1), \ldots, \bm{y}(t-1)\}, \bm{X}) \approx \mathcal{D}(\bm{y}(t-1), \tilde{\bm{z}}(t), \bm{c})
\label{eq: encoder-decoder decoding context}
\end{eqnarray}
เมื่อ $\mathcal{D}(\cdot)$ เป็นฟังก์ชันที่อนุมานความน่าจะเป็นของเอาต์พุต%
\footnote{%
	นิพจน์นี้ (บรรยายตามคณะของบาห์ดาโน\cite{BahdanauEtAl2015}) ต้องการสื่อถึงความสัมพันธ์ระหว่าง\textit{รหัสเนื้อความ}กับการแทนใจความของลำดับอินพุต
	ในมุมมองความน่าจะเป็น.
	ในทางปฏิบัติ บ่อยครั้งที่ฟังก์ชันอนุมาน $\mathcal{D}(\cdot)$ รับสารสนเทศผ่านสถานะซ่อน
	ซึ่งหมายถึง $p(\bm{y}(t) | \{\bm{y}(1), \ldots, \bm{y}(t-1)\}, \bm{X}) \approx \mathcal{D}(\tilde{\bm{z}}(t))$ 
	โดย $\tilde{\bm{z}}(t)$ มักอนุมานได้จาก $\tilde{\bm{z}}(t) = d(\tilde{\bm{z}}(t-1), \bm{y}(t-1), \bm{c})$
	เมื่อ $d(\cdot)$ เป็นส่วนของตัวถอดรหัส.
}
และ $\tilde{\bm{z}}(t)$ เป็นสถานะซ่อน ของโครงข่ายประสาทเวียนกลับ ที่ลำดับเวลา $t$.

จากสมการ~\ref{eq: encoder-decoder decoding context}
เราจะเห็นว่า
ตัวถอดรหัสรับรู้ชุดลำดับอินพุต $\bm{X}$ ผ่านรหัสเนื้อความ $\bm{c}$
ตลอดการประมาณเอาต์พุตทุก ๆ ลำดับ.
ดังนั้น ชุดลำดับอินพุตที่มีความยาวมาก อาจจะยัดเยียดสารสนเทศจำนวนมากลงไปในรหัสเนื้อความ $\bm{c}$
ซึ่งแม้โดยแนวคิด รหัสเนื้อความไม่ได้ถูกจำกัดขนาด
แต่ในทางปฏิบัติ การนำแนวคิดสถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัสไปใช้งาน จะกำหนดขนาดของรหัสเนื้อความนี้
และขนาดของรหัสเนื้อความที่จำกัด อาจทำให้เกิดปัญหาคอขวดกับชุดลำดับยาว ๆ ได้.

กลไกความใส่ใจ เสนอใช้\textit{บริบท}ตามตำแหน่งลำดับ แทนที่จะใช้\textit{รหัสเนื้อความ}เดียวกันทุก ๆ ลำดับ.
นั่นคือ
ตัวถอดรหัส
%รับ\textit{รหัสเนื้อความ} $\bm{c}$
%และ
ประมาณความน่าจะเป็นของเอาต์พุตที่ลำดับ $t$ 
ด้วย 
\begin{eqnarray}
p\left(\bm{y}(t) | \{\bm{y}(1), \ldots, \bm{y}(t-1)\}, \bm{X}\right) \approx \mathcal{D}\left(\bm{y}(t-1), \tilde{\bm{z}}(t), \bm{c}_t\right)
\label{eq: attention decoding context}
\end{eqnarray}
โดย $\bm{c}_t$ เป็น\textit{บริบท}สำหรับเอาต์พุตที่ลำดับเวลา $t$.
%ตัวแปร $\tilde{\bm{z}}(t)$ เป็นสถานะซ่อนของโครงข่ายประสาทเวียนกลับ ที่ลำดับเวลา $t$
%ซึ่ง
ค่าสถานะซ่อนคำนวณได้จาก
\begin{eqnarray}
\tilde{\bm{z}}(t) &=& d(\tilde{\bm{z}}(t-1), \bm{y}(t-1), \bm{c}_t)
\label{eq: attention decoder z}
\end{eqnarray}
เมื่อ $d(\cdot)$ เป็นส่วนของตัวถอดรหัสที่ทำหน้าที่จำลองแบบเชิงลำดับ%
\footnote{%
เนื่องจาก $d(\cdot)$ ต้องให้ผลลัพธ์ออกมาทุก ๆ ลำดับ ก่อนที่จะจบลำดับ ซึ่งแม้แต่การจบลำดับก็อาจจะถูกกำหนดด้วยค่าของผลลัพธ์ที่ออกมา
ดังนั้น $d(\cdot)$ อาจเป็นโครงข่ายประสาทเวียนกลับ แต่ไม่ใช่โครงข่ายประสาทเวียนกลับสองทาง.
}%
.

\textit{บริบท} $\bm{c}_t$ ควรจะขึ้นกับชุดลำดับของเนื้อความย่อย $\{\bm{z}(1), \ldots, \bm{z}(T_x)\}$
ที่\textit{ตัวเข้ารหัส}ได้วิเคราะห์มาจากชุดลำดับอินพุต.
เนื่องจากตัวเข้ารหัสเป็นโครงข่ายประสาทเวียนกลับสองทาง
แต่ละเนื้อความย่อย $\bm{z}(t)$ 
จึงถูกสรุปมาจากอินพุตทั้งชุดลำดับ
โดยเน้นลำดับต่าง ๆ บริเวณรอบ ๆ ลำดับ $t$ (ของชุดลำดับอินพุต). 

คณะของบาห์ดาโน\cite{BahdanauEtAl2015}
กำหนดให้\textit{บริบท}ของเอาต์พุตที่ลำดับเวลา $t$
เป็นผลรวมตามน้ำหนักของเนื้อความย่อยต่าง ๆ.
นั่นคือ
\begin{eqnarray}
\bm{c}_t &=& \sum_{t'=1}^{T_x} \alpha_{t,t'} \cdot \bm{z}(t')
\label{eq: attention context}
\end{eqnarray}
โดย $\alpha_{t,t'}$ เรียกว่า \textit{ค่าน้ำหนักความใส่ใจ} (attention weight)
เป็นค่าน้ำหนักที่ระบุ\textit{การจัดตำแหน่งแบบอ่อน ๆ} (soft alignment)
ของลำดับเอาต์พุต เทียบกับลำดับอินพุต.
และเพื่อให้การคำนวณมีเสถียรภาพ 
ค่าน้ำหนักความใส่ใจ $\alpha_{t,t'}$
จะถูกควบคุมให้
$0 \leq \alpha_{t,t'} \leq 1$ 
สำหรับทุก ๆ $t$ และ $t'$
และ $\sum_{t'} \alpha_{t,t'} = 1$ ด้วยฟังก์ชันซอฟต์แมกซ์
\begin{eqnarray}
\alpha_{t,t'} &=& \frac{\exp(r_{t,t'})}{\sum_{\tau=1}^{T_x} \exp(r_{t,\tau})}
\label{eq: attention attention weights}
\end{eqnarray}
เมื่อ $r_{t,t'}$ เป็น\textit{แบบจำลองการจัดเรียงตำแหน่ง} (alignment model)
ที่ระบุคะแนนว่าอินพุตลำดับ $t'$ ควรมีผลกับเอาต์พุตลำดับ $t$ มากน้อยขนาดไหน.
\textit{แบบจำลองการจัดเรียงตำแหน่ง} ควรขึ้นอยู่กับสถานะซ่อนจากคู่ลำดับอินพุตและเอาต์พุต
ดังนั้น
\begin{eqnarray}
r_{t,t'} &=& f(\tilde{\bm{z}}(t-1), \bm{z}(t'))
\label{eq: attention attention scores}
\end{eqnarray}
โดย $f(\cdot)$ เป็นฟังก์ชันคำนวณคะแนนความสัมพันธ์ระหว่างลำดับ $t'$ ของอินพุต
กับลำดับ $t$ ของเอาต์พุต.
สมการ~\ref{eq: attention attention scores} ใช้ $\tilde{\bm{z}}(t-1)$
แทนที่จะเป็น $\tilde{\bm{z}}(t)$
เพราะว่า 
$\tilde{\bm{z}}(t-1)$ คือสถานะซ่อนล่าสุดของตัวถอดรหัส
(ดูลำดับการคำนวณประกอบ).
%
ฟังก์ชันคะแนน $f(\cdot)$ ก็สามารถทำได้จากโครงข่ายประสาทเทียม
โดยทำการฝึกโครงข่ายไปพร้อม ๆ กับแบบจำลองส่วนอื่น ๆ ของระบบ.

สังเกต ฟังก์ชัน $f(\cdot)$ คำนวณคะแนน โดยใช้ค่าเวคเตอร์จากสถานะซ่อน ไม่ได้ใช้ค่าดัชนี $t$ หรือ $t'$ โดยตรง.
ดังนั้น (1) ไม่ต้องกังวลเรื่องความยาวของลำดับ
และ (2) การเชื่อมโยงระหว่างลำดับอินพุต และลำดับเอาต์พุต ทำผ่านสถานะซ่อน
ไม่ได้ขึ้นกับตำแหน่งสัมบูรณ์.

รูป~\ref{fig: nlp encoder-decoder with attention}
แสดงโครงสร้าง เมื่อใช้กลไกความใส่ใจ.
สถานะซ่อนของตัวเข้ารหัสทุก ๆ ลำดับเวลา จะถูกนำไปประกอบเป็นบริบท
โดยสถานะซ่อนที่ลำดับเวลาใด จะถูกผสมเข้าไปมากน้อยเท่าไร ขึ้นกับน้ำหนักความใส่ใจ.
สถานะซ่อนของตัวเข้ารหัส $\bm{z}(t) = [\bm{z}'(t); \bm{z}''(t)]$
เมื่อ $\bm{z}'(t)$ และ $\bm{z}''(t)$ คือ ผลกระตุ้นในทิศทางไปข้างหน้า และกลับหลัง ตามลำดับ.


ด้วยกลไกความใส่ใจ
ไม่ว่าชุดลำดับอินพุตจะยาวเท่าไร
แต่ละลำดับเวลาของเอาต์พุต จะเสมือนมองเห็นทั้งชุดลำดับอินพุต เพียงแต่เน้นความหมายจากเฉพาะส่วนที่เกี่ยวข้องเท่านั้น
ซึ่งแม้ขนาดของ\textit{บริบท}ที่ลำดับ $\bm{c}_t$ 
จะไม่ได้ใหญ่ไปกว่าขนาดของ\textit{รหัสเนื้อความ} $\bm{c}$
แต่การที่ค่าของ\textit{บริบท}เปลี่ยนไปตามลำดับของเอาต์พุตได้
ช่วยแก้ปัญหาคอขวดของภารกิจ\textit{จำลองแบบชุดลำดับเป็นชุดลำดับ}ได้อย่างดี.
ปัจจุบัน
\textit{กลไกความใส่ใจ}
เป็นศาสตร์และศิลป์ของการเรียนรู้เชิงลึก โดยเฉพาะภารกิจเกี่ยวกับการประมวลผลภาษาธรรมชาติ
มีการประยุกต์ใช้อย่างกว้างขวาง (รวมถึงภารกิจนอกเหนือจากงานประมวลผลภาษาธรรมชาติทั่วไปด้วย เช่น \cite{XuEtAl2015})
และเป็นแรงบันดาลใจให้เกิดการพัฒนาอย่างต่อเนื่อง
จนเป็นแนวทางของ\textit{ตัวแปลง} (transformer\cite{VaswaniEtAl2017, clark2019what, brown2020language}).

\begin{figure}
	\begin{center}		
		\includegraphics[width=0.75\columnwidth]{09nlp/EncoderDecoder/MTA.png}	
		\caption[สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส ที่ใช้รหัสเนื้อความประกอบอินพุตของตัวถอดรหัส]{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส ที่ใช้รหัสเนื้อความประกอบอินพุตของตัวถอดรหัส.
		}
		\label{fig: nlp encoder-decoder without attention}
	\end{center}
\end{figure}


\begin{figure}
	\begin{center}		
		\includegraphics[width=0.75\columnwidth]{09nlp/Attention/Attention2.png}	
		\caption[แผนภาพแสดงโครงสร้าง เมื่อใช้กลไกความใส่ใจ]{แผนภาพแสดงโครงสร้าง เมื่อใช้กลไกความใส่ใจ.	
		ค่าน้ำหนักความใส่ใจ $\alpha_{t,t'}$ จะถูกคำนวณทุก ๆ รอบของลำดับเอาต์พุต $t$.
		แม้ว่า ค่าน้ำหนักความใส่ใจ ในภาพ อาจดูเหมือนเมทริกซ์ 
		แต่ $\alpha_{t,t'}$ ถูกคำนวณจากค่าคะแนน $r_{t,t'}$ ซึ่งเป็นฟังก์ชันของค่าสถานะซ้อน ไม่ได้ขึ้นกับลำดับ $t$ และ $t'$ โดยตรง.
		ค่าคะแนน $r_{t,t'}$ อาจได้จากโครงข่ายประสาทเทียม ดังแสดงในภาพ (ภายในกรอบสีเหลือง ด้านล่าง).
		}
		\label{fig: nlp encoder-decoder with attention}
	\end{center}
\end{figure}


%LATER
%\section{ตัวแปลง}
%ตัวแปลง (Transformer\cite{VaswaniEtAl2017})
%
%
%BERT
%\cite{clark2019what}


%\section{Glossary}
\section{อภิธานศัพท์}

\begin{description}
		
\item[โครงข่ายประสาทเวียนกลับ (Recurrent Neural Network คำย่อ RNN):]
\index{english}{Recurrent Neural Network}
\index{thai}{โครงข่ายประสาทเวียนกลับ}
โครงข่ายประสาทเทียม ที่โครงสร้างการคำนวณมีการเชื่อมต่อ ที่นำค่าที่คำนวณแล้วในลำดับเวลาก่อนกลับเข้ามาคำนวณในลำดับเวลาปัจจุบันด้วย.

\item[แผนภาพคลี่ลำดับ (unfolding diagram):] 
แผนภาพโครงสร้างของโครงข่ายประสาทเทียม ที่กระจายการแสดงการเวียนกลับเชิงลำดับเวลา
ออกเป็นลักษณะเดียวกับโครงสร้างตรรกะกายภาพ.
แผนภาพคลี่ลำดับ นิยมใช้แสดงการเชื่อมต่อของโครงข่ายประสาทเวียนกลับ.
\index{english}{unfolding diagram}\index{thai}{แผนภาพคลี่ลำดับ}

\item[การแพร่กระจายย้อนกลับผ่านเวลา (backpropagation through time คำย่อ BPTT):] 
ขั้นตอนวิธีการคำนวณเกรเดียนต์ สำหรับโครงข่ายประสาทเวียนกลับ.
\index{thai}{การแพร่กระจายย้อนกลับผ่านเวลา}
\index{english}{backpropagation through time}

\item[ปัญหาการระเบิดของเกรเดียนต์ (exploding gradient problem):]
ปัญหาที่อาจพบกับการฝึกโครงข่ายประสาทเวียนกลับ
ที่เกรเดียนต์มีค่าเพิ่มขั้นอย่างมาก เมื่อเวียนกลับย้อนลำดับเวลา.
\index{english}{exploding gradient problem}\index{thai}{ปัญหาการระเบิดของเกรเดียนต์}

\item[การเล็มเกรเดียนต์ (gradient clipping):]
วิธีแก้ปัญหาการระเบิดของเกรเดียนต์
ด้วยวิธีจำกัดขนาดของ \atom{เกรเดียนต์} ที่จะใช้คำนวณปรับค่าน้ำหนัก.
\index{english}{gradient clipping}
\index{thai}{การเล็มเกรเดียนต์}

\item[โครงข่ายประสาทเวียนกลับสองทาง (bidirectional recurrent neural network คำย่อ BRNN):]
โครงข่ายประสาทเวียนกลับ ที่ใช้นำค่าสถานะทั้งในลำดับก่อนหน้า (ทิศทางปกติ) และลำดับหลัง (ทิศทางย้อนกลับ) เวียนมาคำนวณผลลัพธ์ในลำดับปัจจุบัน.
\index{thai}{โครงข่ายประสาทเวียนกลับสองทาง}
\index{english}{bidirectional recurrent neural networks}

\item[แบบจำลองความจำระยะสั้นที่ยาว (long short-term memory model คำย่อ LSTM):]
แบบจำลอง โครงข่ายประสาทเวียนกลับ
ที่มีการใช้กลไกของประตูควบคุมการปรับเปลี่ยนค่าสถานะและค่าผลลัพธ์ของหน่วยคำนวณ
เพื่อช่วยเพิ่มประสิทธิผลในการรู้จำความสัมพันธ์ระยะยาวของข้อมูล.
\index{english}{lstm}
\index{thai}{แบบจำลองความระยะสั้นที่ยาว}

\item[ช่องแอบมอง (peephole connections):]
กลไกเพิ่มเติม สำหรับแบบจำลองความจำระยะสั้นที่ยาว
เพื่อช่วยให้การควบคุมเปลี่ยนค่าสถานะและค่าผลลัพธ์ของหน่วยคำนวณ
ทำได้แม่นยำมากขึ้น
โดยนำค่าสถานะเข้าไปเป็นส่วนหนึ่งในการพิจารณาการเปิดปิดของประตูด้วย.
\index{thai}{ช่องแอบมอง}
\index{english}{peephole}
\index{thai}{แบบจำลองความจำระยะสั้นที่ยาว!ช่องแอบมอง}
\index{english}{lstm!peephole}

\item[สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส (encoder-decoder architecture):]
โครงสร้างการต่อเชื่อมที่ใช้โครงข่ายประสาทเวียนกลับสองตัวต่อกัน
โดยให้ตัวหนึ่งประมวลผลชุดลำดับอินพุต และอีกตัวประมวลผลชุดลำดับเอาต์พุต
สำหรับภารกิจจำลองแบบชุดลำดับเป็นชุดลำดับ.
%บางครั้งอาจเรียก โครงสร้างแปลงชุดลำดับไปชุดลำดับ (sequence-to-sequence structure)
\index{thai}{สถาปัตยกรรมตัวเข้ารหัสตัวถอดรหัส}
\index{english}{encoder-decoder architecture}
\index{english}{sequence-to-sequence architecture}
	
\item[การประมวลผลภาษาธรรมชาติ (Natural Language Processing คำย่อ NLP):] 
ศาสตร์ที่ใช้วิธีการต่าง ๆ เพื่อให้คอมพิวเตอร์สามารถนำข้อความในภาษาธรรมชาติไปประมวลผล และให้ผลลัพธ์ตามจุดประสงค์ของภาระกิจที่ต้องการ.
\index{thai}{การประมวลผลภาษาธรรมชาติ}\index{english}{Natural Language Processing}\index{english}{NLP}

\item[โทเค็น (token):]
หน่วยพื้นฐานของภาษาที่มีความหมาย 
เช่น โทเค็น อาจหมายถึง คำ.

\item[ไวยากรณ์ (syntax):] 
กฎเกณฑ์ที่เกี่ยวกับโทเค็น และโครงสร้าง.
\index{english}{syntax}\index{thai}{ไวยากรณ์}

\item[การแจกส่วน (parsing):]
การวิเคราะห์โครงสร้างไวยากรณ์ของข้อความหรือประโยค.
\index{thai}{การแจกส่วน}\index{english}{parsing}

\item[การระบุหมวดคำ (Part-Of-Speech Tagging):]
\index{english}{Part-Of-Speech Tagging}
\index{thai}{การระบุหมวดคำ}
ภารกิจที่ระบุว่า คำต่าง ๆ ในข้อความ แต่ละคำอยู่ในหมวดคำใด จากหมวดคำ เช่น นาม สรรพนาม กริยา วิเศษณ์ คุณศัพท์ สันธาน บุพบท อุทาน.

\item[ภารกิจจำลองแบบชุดลำดับเป็นชุดลำดับ (sequence-to-sequence modeling task):]
ภารกิจที่ทั้งอินพุตและเอาต์พุตเป็นชุดลำดับ
แต่จำนวนลำดับของชุดอินพุต อาจไม่เท่ากับจำนวนลำดับในชุดเอาต์พุต
\index{thai}{ภารกิจจำลองแบบชุดลำดับเป็นชุดลำดับ}
\index{english}{sequence-to-sequence modeling}

\item[กลไกความใส่ใจ (attention mechanism):]
กลไกสำหรับภารกิจจำลองแบบชุดลำดับเป็นชุดลำดับ
เพื่อช่วยบรรเทาปัญหา เมื่อทำงานกับชุดลำดับที่ยาว.
\index{english}{attention mechanism}
\index{thai}{กลไกความใส่ใจ}
	
\end{description}



